{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0accfe3c",
   "metadata": {},
   "source": [
    "# Project: CO concentration Prediction using RNN AND LSTM\n",
    "\n",
    "## Project By : Deepthi D'Souza\n",
    "This project aims to analyze air quality data and build predictive models to forecast CO concentration. It involves preprocessing, visualizing, normalizing, and training Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) models on the dataset.\n",
    "\n",
    "### Dataset Information:\n",
    "* The dataset contains air quality measurements from an urban area.\n",
    "* Features include Date, Time, Temperature (T), Relative Humidity (RH), and various gas concentrations such as CO (CO(GT)), Benzene (C6H6(GT)), Nitrogen Monoxide (NOx(GT)), Nitrogen Dioxide (NO2(GT)), and more.\n",
    "* Some placeholder values (-200) indicate missing data, which are replaced with NaN during preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf9cda",
   "metadata": {
    "id": "1cbf9cda"
   },
   "source": [
    "### Step 1: Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d95a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220af52",
   "metadata": {},
   "source": [
    "### Step 2: Reading, Preprocessing and Printing Main Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4aaf571",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "a4aaf571",
    "outputId": "7aee36c6-d93a-43ab-8f83-9d1de64d421e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-58f349b9-5c13-45e7-ae40-e2ad1cab8843\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>18.00.00</td>\n",
       "      <td>2,6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11,9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13,6</td>\n",
       "      <td>48,9</td>\n",
       "      <td>0,7578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>19.00.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>9,4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13,3</td>\n",
       "      <td>47,7</td>\n",
       "      <td>0,7255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>20.00.00</td>\n",
       "      <td>2,2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9,0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11,9</td>\n",
       "      <td>54,0</td>\n",
       "      <td>0,7502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>21.00.00</td>\n",
       "      <td>2,2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9,2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11,0</td>\n",
       "      <td>60,0</td>\n",
       "      <td>0,7867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>22.00.00</td>\n",
       "      <td>1,6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6,5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11,2</td>\n",
       "      <td>59,6</td>\n",
       "      <td>0,7888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9468</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9471 rows Ã— 17 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58f349b9-5c13-45e7-ae40-e2ad1cab8843')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-58f349b9-5c13-45e7-ae40-e2ad1cab8843 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-58f349b9-5c13-45e7-ae40-e2ad1cab8843');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            Date      Time CO(GT)  PT08.S1(CO)  NMHC(GT) C6H6(GT)  \\\n",
       "0     10/03/2004  18.00.00    2,6       1360.0     150.0     11,9   \n",
       "1     10/03/2004  19.00.00      2       1292.0     112.0      9,4   \n",
       "2     10/03/2004  20.00.00    2,2       1402.0      88.0      9,0   \n",
       "3     10/03/2004  21.00.00    2,2       1376.0      80.0      9,2   \n",
       "4     10/03/2004  22.00.00    1,6       1272.0      51.0      6,5   \n",
       "...          ...       ...    ...          ...       ...      ...   \n",
       "9466         NaN       NaN    NaN          NaN       NaN      NaN   \n",
       "9467         NaN       NaN    NaN          NaN       NaN      NaN   \n",
       "9468         NaN       NaN    NaN          NaN       NaN      NaN   \n",
       "9469         NaN       NaN    NaN          NaN       NaN      NaN   \n",
       "9470         NaN       NaN    NaN          NaN       NaN      NaN   \n",
       "\n",
       "      PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  \\\n",
       "0            1046.0    166.0        1056.0    113.0        1692.0   \n",
       "1             955.0    103.0        1174.0     92.0        1559.0   \n",
       "2             939.0    131.0        1140.0    114.0        1555.0   \n",
       "3             948.0    172.0        1092.0    122.0        1584.0   \n",
       "4             836.0    131.0        1205.0    116.0        1490.0   \n",
       "...             ...      ...           ...      ...           ...   \n",
       "9466            NaN      NaN           NaN      NaN           NaN   \n",
       "9467            NaN      NaN           NaN      NaN           NaN   \n",
       "9468            NaN      NaN           NaN      NaN           NaN   \n",
       "9469            NaN      NaN           NaN      NaN           NaN   \n",
       "9470            NaN      NaN           NaN      NaN           NaN   \n",
       "\n",
       "      PT08.S5(O3)     T    RH      AH  Unnamed: 15  Unnamed: 16  \n",
       "0          1268.0  13,6  48,9  0,7578          NaN          NaN  \n",
       "1           972.0  13,3  47,7  0,7255          NaN          NaN  \n",
       "2          1074.0  11,9  54,0  0,7502          NaN          NaN  \n",
       "3          1203.0  11,0  60,0  0,7867          NaN          NaN  \n",
       "4          1110.0  11,2  59,6  0,7888          NaN          NaN  \n",
       "...           ...   ...   ...     ...          ...          ...  \n",
       "9466          NaN   NaN   NaN     NaN          NaN          NaN  \n",
       "9467          NaN   NaN   NaN     NaN          NaN          NaN  \n",
       "9468          NaN   NaN   NaN     NaN          NaN          NaN  \n",
       "9469          NaN   NaN   NaN     NaN          NaN          NaN  \n",
       "9470          NaN   NaN   NaN     NaN          NaN          NaN  \n",
       "\n",
       "[9471 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading and Preprocessing the Dataset\n",
    "df = pd.read_csv('AirQualityUCI.csv', sep=';')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50511e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "d50511e5",
    "outputId": "04347b87-739f-434f-9f79-26451444d85d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-28e57a85-3429-46d5-9f48-1050aa10192e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7674.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>914.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>7718.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>7715.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.152750</td>\n",
       "      <td>1099.833166</td>\n",
       "      <td>218.811816</td>\n",
       "      <td>10.083105</td>\n",
       "      <td>939.153376</td>\n",
       "      <td>246.896735</td>\n",
       "      <td>835.493605</td>\n",
       "      <td>113.091251</td>\n",
       "      <td>1456.264598</td>\n",
       "      <td>1022.906128</td>\n",
       "      <td>18.317829</td>\n",
       "      <td>49.234201</td>\n",
       "      <td>1.025530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.453252</td>\n",
       "      <td>217.080037</td>\n",
       "      <td>204.459921</td>\n",
       "      <td>7.449820</td>\n",
       "      <td>266.831429</td>\n",
       "      <td>212.979168</td>\n",
       "      <td>256.817320</td>\n",
       "      <td>48.370108</td>\n",
       "      <td>346.206794</td>\n",
       "      <td>398.484288</td>\n",
       "      <td>8.832116</td>\n",
       "      <td>17.316892</td>\n",
       "      <td>0.403813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>-1.900000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>734.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>658.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1227.000000</td>\n",
       "      <td>731.500000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>35.800000</td>\n",
       "      <td>0.736800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>963.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.900000</td>\n",
       "      <td>1231.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1116.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>969.500000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1674.000000</td>\n",
       "      <td>1273.500000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1.313700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.900000</td>\n",
       "      <td>2040.000000</td>\n",
       "      <td>1189.000000</td>\n",
       "      <td>63.700000</td>\n",
       "      <td>2214.000000</td>\n",
       "      <td>1479.000000</td>\n",
       "      <td>2683.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>2775.000000</td>\n",
       "      <td>2523.000000</td>\n",
       "      <td>44.600000</td>\n",
       "      <td>88.700000</td>\n",
       "      <td>2.231000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28e57a85-3429-46d5-9f48-1050aa10192e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-28e57a85-3429-46d5-9f48-1050aa10192e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-28e57a85-3429-46d5-9f48-1050aa10192e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            CO(GT)  PT08.S1(CO)     NMHC(GT)     C6H6(GT)  PT08.S2(NMHC)  \\\n",
       "count  7674.000000  8991.000000   914.000000  8991.000000    8991.000000   \n",
       "mean      2.152750  1099.833166   218.811816    10.083105     939.153376   \n",
       "std       1.453252   217.080037   204.459921     7.449820     266.831429   \n",
       "min       0.100000   647.000000     7.000000     0.100000     383.000000   \n",
       "25%       1.100000   937.000000    67.000000     4.400000     734.500000   \n",
       "50%       1.800000  1063.000000   150.000000     8.200000     909.000000   \n",
       "75%       2.900000  1231.000000   297.000000    14.000000    1116.000000   \n",
       "max      11.900000  2040.000000  1189.000000    63.700000    2214.000000   \n",
       "\n",
       "           NOx(GT)  PT08.S3(NOx)      NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
       "count  7718.000000   8991.000000  7715.000000   8991.000000  8991.000000   \n",
       "mean    246.896735    835.493605   113.091251   1456.264598  1022.906128   \n",
       "std     212.979168    256.817320    48.370108    346.206794   398.484288   \n",
       "min       2.000000    322.000000     2.000000    551.000000   221.000000   \n",
       "25%      98.000000    658.000000    78.000000   1227.000000   731.500000   \n",
       "50%     180.000000    806.000000   109.000000   1463.000000   963.000000   \n",
       "75%     326.000000    969.500000   142.000000   1674.000000  1273.500000   \n",
       "max    1479.000000   2683.000000   340.000000   2775.000000  2523.000000   \n",
       "\n",
       "                 T           RH           AH  Unnamed: 15  Unnamed: 16  \n",
       "count  8991.000000  8991.000000  8991.000000          0.0          0.0  \n",
       "mean     18.317829    49.234201     1.025530          NaN          NaN  \n",
       "std       8.832116    17.316892     0.403813          NaN          NaN  \n",
       "min      -1.900000     9.200000     0.184700          NaN          NaN  \n",
       "25%      11.800000    35.800000     0.736800          NaN          NaN  \n",
       "50%      17.800000    49.600000     0.995400          NaN          NaN  \n",
       "75%      24.400000    62.500000     1.313700          NaN          NaN  \n",
       "max      44.600000    88.700000     2.231000          NaN          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "df['CO(GT)'] = df['CO(GT)'].str.replace(',', '.').astype(float)  # Replace commas with dots and convert to float\n",
    "df['C6H6(GT)'] = df['C6H6(GT)'].str.replace(',', '.').astype(float)\n",
    "df['T'] = df['T'].str.replace(',', '.').astype(float)\n",
    "df['RH'] = df['RH'].str.replace(',', '.').astype(float)\n",
    "df['AH'] = df['AH'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Display basic statistics and check for any remaining NaN values\n",
    "df = df.replace(-200, np.nan)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8430836b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8430836b",
    "outputId": "2a454f54-d58e-49da-9969-99ac26ffbc6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              114\n",
       "Time              114\n",
       "CO(GT)           1797\n",
       "PT08.S1(CO)       480\n",
       "NMHC(GT)         8557\n",
       "C6H6(GT)          480\n",
       "PT08.S2(NMHC)     480\n",
       "NOx(GT)          1753\n",
       "PT08.S3(NOx)      480\n",
       "NO2(GT)          1756\n",
       "PT08.S4(NO2)      480\n",
       "PT08.S5(O3)       480\n",
       "T                 480\n",
       "RH                480\n",
       "AH                480\n",
       "Unnamed: 15      9471\n",
       "Unnamed: 16      9471\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f49fb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77f49fb7",
    "outputId": "26366ffc-ec25-4044-aeef-30c1a418d863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "Time             0\n",
       "CO(GT)           0\n",
       "PT08.S1(CO)      0\n",
       "C6H6(GT)         0\n",
       "PT08.S2(NMHC)    0\n",
       "NOx(GT)          0\n",
       "PT08.S3(NOx)     0\n",
       "NO2(GT)          0\n",
       "PT08.S4(NO2)     0\n",
       "PT08.S5(O3)      0\n",
       "T                0\n",
       "RH               0\n",
       "AH               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace(-200, np.nan)  # Replace placeholder values with NaN\n",
    "df = df.drop(columns=['NMHC(GT)', 'Unnamed: 15', 'Unnamed: 16'])  # Drop unnecessary columns\n",
    "df = df.dropna()  # Drop rows with NaN values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc26241",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbc26241",
    "outputId": "4650d35c-0dd6-4afc-edaf-0040d045a4fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6941, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a11155",
   "metadata": {
    "id": "e2a11155"
   },
   "source": [
    "### Step 3: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7269576",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "e7269576",
    "outputId": "fdd6ffda-a779-44af-9341-011ba7081650"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram of Temperature distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['T'], bins=30, kde=True)\n",
    "plt.title('Temperature Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6742fe0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "f6742fe0",
    "outputId": "90966b58-7f24-4732-a789-2c00fc79a5a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-eb924870e60a>:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  sns.heatmap(df.corr(), cmap='coolwarm', annot=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAMJCAYAAAB2vhXCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3QUVRvA4d+WbHolPZRQEnrvAgKC9KZ0pDdFQJqKiEqRoiJFpQlSFSkiTZBelN6RTgIkBAIJaZveNrvfH4ENSzY0ExL53uecPYe9e+/MfXfC7Ny5ZRQGg8GAEEIIIYQQQoh8p8zvCgghhBBCCCGEyCQNNCGEEEIIIYQoIKSBJoQQQgghhBAFhDTQhBBCCCGEEKKAkAaaEEIIIYQQQhQQ0kATQgghhBBCiAJCGmhCCCGEEEIIUUBIA00IIYQQQgghCghpoAkhhBBCCCFEASENNCGEEOIl6tu3L76+vrm6zeXLl6NQKAgODs7V7QohhHj5pIEmhBAFxMOLbHOvTz75JE/2eeTIESZOnIhWq82T7b8sBw4c4O2338bT0xONRoO7uztt27Zlw4YN+V21XDVt2jQ2bdqU39UQQgiRh9T5XQEhhBCmJk+eTPHixU3SKlSokCf7OnLkCJMmTaJv3744OTnlyT7y2oQJE5g8eTJ+fn68++67FCtWjKioKP788086duzIqlWr6NGjR35XM1dMmzaNTp060aFDB5P0Xr160a1bNywtLfOnYkIIIXKNNNCEEKKAadmyJTVq1MjvavwriYmJ2Nra5vl+1q9fz+TJk+nUqRO//vorFhYWxs8++ugjdu7cSXp6+r/ej06nQ6/Xo9Fosn32smJ9EpVKhUqlytc6CCGEyB0yxFEIIf5jtm/fToMGDbC1tcXe3p7WrVtz6dIlkzznz5+nb9++lChRAisrKzw9Penfvz9RUVHGPBMnTuSjjz4CoHjx4sbhlMHBwQQHB6NQKFi+fHm2/SsUCiZOnGiyHYVCweXLl+nRowfOzs7Ur1/f+Pkvv/xC9erVsba2xsXFhW7dunH79m2TbQYGBtKxY0c8PT2xsrKicOHCdOvWjdjY2Cd+F59//jkuLi4sXbrUpHH2UPPmzWnTpo3x/f379xkwYAAeHh5YWVlRuXJlVqxYYVLmYezffvstc+bMoWTJklhaWnL58uVcidWcb7/9ltdee41ChQphbW1N9erVWb9+vUkehUJBYmIiK1asMB6rvn37AjnPQZs/fz7ly5fH0tISb29vhg4dmm04a6NGjahQoQKXL1+mcePG2NjY4OPjwzfffPPUegshhMh90oMmhBAFTGxsLJGRkSZprq6uAPz888/06dOH5s2b8/XXX5OUlMSCBQuoX78+Z8+eNS4+sXv3bm7evEm/fv3w9PTk0qVLLFq0iEuXLnHs2DEUCgVvv/02AQEBrF69mtmzZxv34ebmRkRExHPXu3Pnzvj5+TFt2jQMBgMAU6dO5fPPP6dLly4MHDiQiIgIfvjhB15//XXOnj2Lk5MTaWlpNG/enNTUVIYPH46npyehoaFs3boVrVaLo6Oj2f0FBgZy9epV+vfvj729/VPrl5ycTKNGjbh+/TrDhg2jePHi/Pbbb/Tt2xetVsuIESNM8i9btoyUlBQGDx6MpaUlLi4u/zrWnHz33Xe0a9eOd955h7S0NNasWUPnzp3ZunUrrVu3BjKP/cCBA6lVqxaDBw8GoGTJkjluc+LEiUyaNImmTZsyZMgQrl27xoIFCzh58iSHDx82adDGxMTQokUL3n77bbp06cL69esZO3YsFStWpGXLlk/9boUQQuQigxBCiAJh2bJlBsDsy2AwGOLj4w1OTk6GQYMGmZQLCwszODo6mqQnJSVl2/7q1asNgOHvv/82ps2YMcMAGIKCgkzyBgUFGQDDsmXLsm0HMEyYMMH4fsKECQbA0L17d5N8wcHBBpVKZZg6dapJ+oULFwxqtdqYfvbsWQNg+O2333L+cszYvHmzATDMnj37mfLPmTPHABh++eUXY1paWpqhbt26Bjs7O0NcXJzBYMiK3cHBwXD//n2TbfzbWA0Gg6FPnz6GYsWKmeR7/HilpaUZKlSoYHjjjTdM0m1tbQ19+vTJFtvDv52Hx/H+/fsGjUZjaNasmSEjI8OYb+7cuQbAsHTpUmNaw4YNDYBh5cqVxrTU1FSDp6enoWPHjtn2JYQQIm/JEEchhChg5s2bx+7du01ekNkrptVq6d69O5GRkcaXSqWidu3a7N+/37gNa2tr479TUlKIjIykTp06AJw5cyZP6v3ee++ZvN+wYQN6vZ4uXbqY1NfT0xM/Pz9jfR/2kO3cuZOkpKRn3l9cXBzAM/WeAfz55594enrSvXt3Y5qFhQUffPABCQkJ/PXXXyb5O3bsiJubm9ltvWisOXn0eMXExBAbG0uDBg1e+Fjt2bOHtLQ0Ro4ciVKZ9VM/aNAgHBwc2LZtm0l+Ozs7evbsaXyv0WioVasWN2/efKH9CyGEeHEyxFEIIQqYWrVqmV0kJDAwEIA33njDbDkHBwfjv6Ojo5k0aRJr1qzh/v37JvmeNq/rRT2+8mRgYCAGgwE/Pz+z+R8OsStevDijR49m1qxZrFq1igYNGtCuXTt69uyZ4/BGyIo3Pj7+mep369Yt/Pz8TBosAGXLljV+/qR4nvTZs8aak61btzJlyhTOnTtHamqqMV2hUDyxXE4exlK6dGmTdI1GQ4kSJbLFWrhw4Wz7cnZ25vz58y+0fyGEEC9OGmhCCPEfodfrgcy5SJ6entk+V6uzTuldunThyJEjfPTRR1SpUgU7Ozv0ej0tWrQwbudJcmoYZGRk5Fjm0V6gh/VVKBRs377d7AqDdnZ2xn/PnDmTvn37snnzZnbt2sUHH3zA9OnTOXbsGIULFza7vzJlygBw4cKFp8bzIh6P50mfPU+sjzt48CDt2rXj9ddfZ/78+Xh5eWFhYcGyZcv49ddfXzyA55DTCpCGB/PrhBBCvDzSQBNCiP+IhwtCuLu707Rp0xzzxcTEsHfvXiZNmsQXX3xhTH/YA/eonBpizs7OANlW/Hu85+Vp9TUYDBQvXhx/f/+n5q9YsSIVK1bks88+48iRI9SrV4+FCxcyZcoUs/n9/f0pXbo0mzdv5rvvvntiIwigWLFinD9/Hr1eb9KLdvXqVePnL+p5Y33U77//jpWVFTt37jR5jtmyZcuy5X3WHrWHsVy7do0SJUoY09PS0ggKCnri348QQoj8JXPQhBDiP6J58+Y4ODgwbdo0s8/2erjy4sPekMd7P+bMmZOtzMPndz3eEHNwcMDV1ZW///7bJH3+/PnPXN+3334blUrFpEmTstXFYDAYl/yPi4tDp9OZfF6xYkWUSqXJcD9zJk2aRFRUFAMHDsy2DYBdu3axdetWAFq1akVYWBhr1641fq7T6fjhhx+ws7OjYcOGzxzb4541VnNUKhUKhcKkdzI4OJhNmzZly2tra5vtWJnTtGlTNBoN33//vUl9lixZQmxsrHFlSCGEEAWP9KAJIcR/hIODAwsWLKBXr15Uq1aNbt264ebmRkhICNu2baNevXrMnTsXBwcHXn/9db755hvS09Px8fFh165dBAUFZdtm9erVARg/fjzdunXDwsKCtm3bYmtry8CBA/nqq68YOHAgNWrU4O+//yYgIOCZ61uyZEmmTJnCuHHjCA4OpkOHDtjb2xMUFMTGjRsZPHgwH374Ifv27WPYsGF07twZf39/dDodP//8MyqVio4dOz5xH127duXChQtMnTqVs2fP0r17d4oVK0ZUVBQ7duxg7969xmGCgwcP5scff6Rv376cPn0aX19f1q9fz+HDh5kzZ84zLzbyb2I1p3Xr1syaNYsWLVrQo0cP7t+/z7x58yhVqlS2OWDVq1dnz549zJo1C29vb4oXL07t2rWzbdPNzY1x48YxadIkWrRoQbt27bh27Rrz58+nZs2aJguCCCGEKGDyafVIIYQQj3m4VPrJkyefmG///v2G5s2bGxwdHQ1WVlaGkiVLGvr27Ws4deqUMc+dO3cMb731lsHJycng6Oho6Ny5s+Hu3bvZlsg3GAyGL7/80uDj42NQKpUmS7UnJSUZBgwYYHB0dDTY29sbunTpYrh//36Oy+xHRESYre/vv/9uqF+/vsHW1tZga2trKFOmjGHo0KGGa9euGQwGg+HmzZuG/v37G0qWLGmwsrIyuLi4GBo3bmzYs2fPM393e/fuNbRv397g7u5uUKvVBjc3N0Pbtm0NmzdvNskXHh5u6Nevn8HV1dWg0WgMFStWzPYogYfL7M+YMSPbfv5trAaD+WX2lyxZYvDz8zNYWloaypQpY1i2bJlxX4+6evWq4fXXXzdYW1sbAOOS+48vs//Q3LlzDWXKlDFYWFgYPDw8DEOGDDHExMSY5GnYsKGhfPny2WIxV08hhBB5T2EwyAxgIYQQQgghhCgIZA6aEEIIIYQQQhQQ0kATQgghhBBCiAJCGmhCCCGEEEIIUUBIA00IIYQQQgjxyvv7779p27Yt3t7eKBQKs48zedyBAweoVq0alpaWlCpViuXLl+d5PaWBJoQQQgghhHjlJSYmUrlyZebNm/dM+YOCgmjdujWNGzfm3LlzjBw5koEDB7Jz5848raes4iiEEEIIIYT4v6JQKNi4cSMdOnTIMc/YsWPZtm0bFy9eNKZ169YNrVbLjh078qxu0oMmhBBCCCGE+E9KTU0lLi7O5JWampor2z569ChNmzY1SWvevDlHjx7Nle3nRJ2nWxf/CdssSud3FXKVZ33X/K5CrkqMSMrvKuQatdWrdcrxKOee31XIVR9ovs7vKuSa5Z4z87sKuUpta53fVchVIX9dfHqm/wjDD7/ndxVylZUydy5sCwoDivyuQq4pW9Inv6uQo/y8ljw5vjuTJk0ySZswYQITJ07819sOCwvDw8PDJM3Dw4O4uDiSk5Oxts6bc/OrdbUkhBBCCCGE+L8xbtw4Ro8ebZJmaWmZT7XJHdJAE0IIIYQQQrwwhUX+9VRaWlrmWYPM09OT8PBwk7Tw8HAcHBzyrPcMZA6aEEIIIYQQQmRTt25d9u7da5K2e/du6tatm6f7lR40IYQQQgghxAtTqv8bc/0SEhK4fv268X1QUBDnzp3DxcWFokWLMm7cOEJDQ1m5ciUA7733HnPnzuXjjz+mf//+7Nu3j3Xr1rFt27Y8raf0oAkhhBBCCCFeeadOnaJq1apUrVoVgNGjR1O1alW++OILAO7du0dISIgxf/Hixdm2bRu7d++mcuXKzJw5k59++onmzZvnaT2lB00IIYQQQgjxymvUqBFPegT08uXLzZY5e/ZsHtYqO2mgCSGEEEIIIV6YwkIG5eUm+TaFEEIIIYQQooCQHjQhhBBCCCHEC/uvLBLyXyE9aEIIIYQQQghRQEgDTQghhBBCCCEKCBniKIQQQgghhHhhCgsZ4pibpAdNCCGEEEIIIQoI6UETQgghhBBCvDBZJCR3SQ+aEEIIIYQQQhQQ0oMmhBBCCCGEeGEyBy13SQ+aEEIIIYQQQhQQ0kATQgghhBBCiAJCGmgv2ZIlS2jWrFmebLtOnTr8/vvvebJtIYQQQgghzFGqFfn2ehXJHLSnCAsLY+rUqWzbto3Q0FDc3d2pUqUKI0eOpEmTJgAcOXKEKVOmcPToUZKTk/Hz86Nfv36MGDEClUpl3FZKSgqff/45v/32m8k+4uLimDFjBhs2bODmzZvY2NhQokQJOnfuzKBBgxg1ahQrVqzIsY7FihUjODiYzz77jFGjRvHWW2+hVBaMtrdL/RqUGDMAx2oVsPJ251TH9wnfsje/q5WNW4e38ejaAwsXF5JvXCfk+9kkXb2SY373jl1wa/cWGg8PdLFaYv46QOjihRjS0zIzKJV49xmAy5vNsHApRHpkJJE7/yTs5+UvJR6vbl0o0q8PGtdCJFwL4Ma0r4m/eMlsXoVaTZGB/fFo3wZLd3eSgm8RNOs7Yg4fydpe1854de2Elbc3AEnXb3Jr4SJiDh1+KfF4du6Md6+eaAoVIjEwkKAZM0i4dNl8PCoVPv364d6mNRo3N5Jv3eLWD3PRHj1qzONQtSrevXphV7YMGjc3ro75kOi//nopsTg0aY1Tq46oHJ1Jux1E5M8LSb0ZYDav97jpWJetlC098dxJwmZNBEDl4IRL137YVKiK0saWlGuXiPx5Ienhd/MyjKfq2d6d5g2csbVRceV6EvN+ucvd+2k55l/6lT8erpps6Vv3RbHg13t5WVUTVrXfwKZ+S5R2jujCQkjYugpdaFCO+RVW1tg27YimfHWU1rZkaKNI/HM1aQHnAXAZMwOVs2u2csnH9pKw9Zc8iwPAstrrWNZ+E6WdAxn375C0ax0Z927lmF9haY1Vw3ZoSldBYWWDPi6apD3r0d3IPHdoqjbAstrrqBxdAMiIvEfyoT/R3TT/fzG3FWr7Fu6duqF2diH55g1C539HckDO52nXDp0p1KY9GjcPdHGxxB48wL1li4znaY+e/fDs2c+kTMrtW1wb1CvX675z6+/8sWE12phoihUvSb93R1GqdLkc8x89tI91v/xERHgYnt6FeafvEKrWrGv8/PiRv9izfRM3r18jIT6Or79fhm8JP5NtaGOi+GXpfM6fPUlKchJehYvydpfe1K7X6F/Hs+2PTWz6fR0xMdH4Fi/J4CHD8S9dJsf8hw/+xaqfl3E/PAxv78L07j+IGjVrm+S5HXKLFcsWc+nCeTIyMihStBifjJ+Am7sHAOPHjubihX9MyjRv2Yb3h4/61/H8+ccmNv6+Fu2DeAYNGY5/6bJPiOcAvz6Ix8sYTx3j59/N+pr9e3aalKlavSYTvvza+P7G9QBWLl1MYOBVVEoVdeo1oP+g97G2tv7X8Yj/PmmgPUFwcDD16tXDycmJGTNmULFiRdLT09m5cydDhw7l6tWrbNy4kS5dutCvXz/279+Pk5MTe/bs4eOPP+bo0aOsW7cOhSKzdb9+/XocHByoV6+ecR/R0dHUr1+fuLg4vvzyS6pXr46joyPXrl1j2bJl/Prrr3z33Xd89dVXxjJeXl4sW7aMFi1aABgbgS1btmTgwIFs376d1q1bv8RvKmcqWxvizl/j9vLfqbF+Xn5Xxyznxk0oPGQ4IbNnkHjlMu6duuD3zSwu9e6OTqvNnr/Jm/gMfo/gb6aTePEClkWK4jt2PGDgzvwfAPDs3hO39h0I+moKKUFB2JQug+/Y8WQkJhCxYX2exuPWohklPx5D4OSpxJ+/iE+vHlT4cT6n2nYgPTomW37f4e/j3qY1ARO/JDkoCOd6r1Huu5mc69mXxKvXAEgNCydo9g8k3wpBoQCP9m0p/8NsznTqRtKNm3kaT6E338R31EhuTv+K+IsX8erenXI//MDZjp1Ij8keT9H3h+DasiU3pk4lOfgWTnXqUHrGN1wcMIDEa5kNIaW1NYmBAdzfsoUy387I0/o/yrZ2A1x7DCJi+VxSblzDqXkHvD76ktsfDyYjPjZb/rDvp6JQWxjfK+3sKTJlLoknDhnTPEd+hkGXQdicL9EnJ+HY4i28xk7l9ifvYUhLfSlxPa5TC1faNinE7KV3CItMo1d7D74c5ct7nweSrjOYLTNyyg1Uyqw7ocV8LJk6pjiHTse9rGpjWaEWdi27Eb9lJbrbN7F+7U0c+44hes44DInx2QuoVDj2/Qh9Yhxxq+ehj4tB5eSKPiXJmCVmwWR4JC61R2Gc+n1E6qWTeRqLRdnqWDfpSNKO1ejuBmNV8w3sug4nbtFEDEkJ2QsoVdh1/wBDYjwJGxZjSNCidCiEITUrFkO8luQDm9BH3weFAk2FOth1eo+4pdPRR+ZtI9rp9TfwHjSUOz/MJOnaZdw6dKbE1G+5NvAddLHa7PkbNcWr/2Buz/qaxCsXsfQpQtEx4wADdxdl/RYlB9/k5rjRWTFmZOR63Y/8vZeVP81l4NAP8Stdjj83r2PaF6OZ/eNqHJ2cs+W/duUC338zie593qVardc4fGA3M6aO46s5SynqWwKA1JRkSperRJ36b7Doh6+zbQNg3qwpJCYk8PHnX2Hv6MihA7uZ/fUXTJ/9E8VL+r9wPAf/2s/SxQsZMmwk/mXK8MemDUz8fCzzFy3HyUw8Vy5f4tuvp9Cr70Bq1qrD3wf2Mf3LL5j1/UKK+RYH4N69u4z7aARNm7WkR88+WNvYEnIrGAuN6U2bZi1a06NnX+N7SyvLF47joUN/7Wfp4gUP4inLlk2/M+nzscxbtMJsPFcvX2Tmg3hq1KrL3wf28tWXXzDz+x+N8QBUq16L4aM+Nr63sMg6l0dHRTLh04+o/3ojBr8/nKSkJJb8OI/vZ33N2PET/3VM+UGhejV7svJLwehmKaDef/99FAoFJ06coGPHjvj7+1O+fHlGjx7NsWPHSExMZNCgQbRr145FixZRpUoVfH19GThwICtWrGD9+vWsW7fOuL01a9bQtm1bk318+umnhISEcOLECfr160elSpUoVqwYzZo1Y/Xq1bz//vs4Ojri6elpfAE4OTkZ37u5uQGZDbVWrVqxZs2al/clPUXEzr8JmDCH8M178rsqOfLo3JXIbX8QteNPUm4FEzJrBvqUVAq1bGM2v135iiRcvEDM3t2khYcRf+oEMft2Y1sm626bbfkKaA8fJO7YUdLCw9D+fYC4UyewLZPzHdPc4tO7J/fWbyB80xaSbt4kcPJU9CkpeL7VwWx+97ZtCFm8hJiDh0i5E8q9tb8RffAwhftm3UWO/uvvzM9DQki+FULw9/PISErCoXL23p3c5v1OD8I3beL+H3+QHBTEzenTyUhJwb1dO7P53Vq1InTZcrSHj5AaGkr477+jPXIE73d6GvNojxzh9oKFRB84kOf1f5RTi7eIO7CD+IN7SL97m4jlczGkpmDf0PywZ31iAhmxMcaXTYWqGNJSSThxEAALT2+sSpUlYsU8UoMCSQ8LJXLFPJQaDXZ1G77M0Ey0b1qItVvvc+xcPMF3Upm59A4uTmrqVnXIsUxcQgYxcTrjq2Yle+7eT+XCtcSXVm/res1IOfU3qWcOkRFxl4QtKzGkp2FVvYHZ/FbVGqC0sSVu1Q/oQq6j10aRHnyNjLDbxjyGpHgMCXHGl6Z0ZTKiwkkPupansVjVeoPUfw6TduEY+qgwknasBl0amkqvmc2vqfwaCisbEn5fSEboTfSx0ehuB5JxP9SYJ/36BXQ3LqGPiUAffZ+Uv7dgSEtF7V3c7DZzk+vbXYjesZWY3dtJDbnFnR9mYkhNwaW5+ZuRtuUqkHjpItoDe0gPDyPhzEliDuzF5vFekYwMdDHRxldGXPYbJf/Wtk1raNK8LY3fbE3hosUZOPQjNJZW7N+91Wz+7Vt+o0r12rTr2IPCRXzp2msQxUv6s3Nr1hSG199oQafu/ahYpUaO+7125SIt2nakVOlyeHj60LFbX2xt7bh5/d/97W3euJ5mLVrRtFkLihb1ZciwkVhaWrJn1w6z+f/YvIFq1WvydqeuFClajHd696NEST+2/bHJmOeXFUuoXqM2fQe8S4mSfnh5eVO7zmvZGkiWlpY4u7gYXzY2tv8qlsx4fqNZi1Y0adaSIkV9GTJsFJaWluzdtf0J8dTirU7dHsTTnxIl/fjzkXgA1BYWJnW1s7c3fnbyxDFUajWD3x+BT+Gi+PmX4b1hozh6+G/u3Q1FCGmg5SA6OpodO3YwdOhQbG2znwCcnJzYtWsXUVFRfPjhh9k+b9u2Lf7+/qxevdqYdujQIWrUyDqZ6vV61q5dS8+ePfF+MHTscQ97355VrVq1OHjw4HOV+X+mUKux8S9N3OlH7mYbDMSfOYVd+QpmyyRcuoCNf2lsHjTINF7eONSuS+zxY8Y8iZcuYl+tBpaFiwBgXbIUdhUqEXfimNlt5haFWo19ubJojx3PSjQY0B47jn0OjSmlxgJDmunQM31qCo5Vq5rfiVKJW8vmqKytiTt3PreqbpZCrcauTBlij5/ISjQYiD1xAvtKFc2XsbBA/1jPkT4lFfsqlfOyqk+nUmPpW4qkS+ey0gwGki+fw6pUzkODHmX/ejMSjv1t7Bl72LtmHFr7YJuG9HSs/MvnVs2fi6erBS5OFpy7ktWwSkrWc+1mMmVKPtvQHbVKQeM6Tuw+pM2jWpqhUqH29iXtxiNDgQ0G0m9cxqJIKbNFNGWqkh5yA7u2PSn0yRych3+JTcPWkNN5W6XCqnJdUs7k8TlaqULlWRSdSSPQQHrwVdQ+5htTGr+K6EKDsGnWDccPvsJh4GdY1W2ecywKBRZlq6Ow0KALzdtedIVajY2fP/FnT2UlGgzEnz2NTVnzf+eJly9i4+ePtf+D87SnFw4162Q7B2t8ClNu1QbKLFtD0Y8/x8LNPVfrrktP5+b1AJOGlFKppGKVGgReNT/sPODqRSo81vCqXK02AVcvPte+S5etwNGD+0iIj0Ov13P4rz2kp6VRvmIO5/ZnkJ6ezo3rAVSuUs2YplQqqVylGteumh/qeu3qZSpXrW6SVrV6DWN+vV7PqZPH8fYpzITPxtK7e0c+HDmUY0cOZdvWX/v30rPbWwwfMoCVy34iNSXlhWN5NJ5KVbLqlxlP9SfGU6lqNZO0qtVrcu2x43nxwjn6dH+b9wf1ZuHc2cQ90vhPT09DrVabTEextMzsDbx86cK/ikm8GmSIYw6uX7+OwWCgTJmcL5wCAjKHS5Uta36ccpkyZYx5tFotsbGxJg2xiIgItFotpUuXNilXvXp1rl3L/GFt27atSSPvaby9vbl9+zZ6vd7sPLTU1FRSU00vXtMNeiwU/59tdbWjEwqVGl1MtEl6ekw0VkWLmi0Ts3c3akdHSn+/AIVCgUKtJmLzRsJWrTTmCfv1Z1Q2NpRf8Svo9aBUcnfJIqL37MrTeCycnVGo1aRFmcaTFhWFY3Ffs2ViDh/Fp3dPtKfOkHL7Nk51auHa5A0Uj8yfBLDxK0XVVStQajRkJCVzacQYkm7m7YWZ2skpM57ox45PdDTWvr5my2iPHcO7xzvEnTlLyp07ONaqicsbjVHk87xMlb0DCpWKjDitSbouVou1V5Gnlrcs4Y9lEV8ilnxnTEu7d4f0yPsU6tyXiGVz0aem4NSiA+pCbqjNDM15GZwdM39WYuJ0JunaOB3OjhbmimRTp6o9djYq9hzOPoQ1ryht7FGoVOgTTIdU6hNisXD1NFtG5eKGyqksKeePErtyNioXD+za9QKlmqT9m7PltyxbDYWVDSln8nbupsLGDoVShT7JNBZDYjyqQh5myyidXFEXK0TapZMkrJuH0tkdm+ZdQaUi5dCfWfncvHHo/SGoLTJ7czcsQh8VlqfxqBwcM8/TWtO/B502Gssi5s/T2gN7UDs6UmrmXON5OnLrJu6vzZr3l3T1MrdnTif1TggWLoXweKcfpb6dy7X3+qBPTs6VusfFxaLXZ+Do5GKS7ujkwt075ucDamOis/UcOTo5E6uNNps/JyPHTmbO1xMY0L0VKpUKjaUVY8ZPw9O78PMF8YjMePQ4OZvWz8nJmTu3b5stYy4eJydnYh787sZqtaQkJ/P7b2t4p3c/+vQbxJnTJ/lq6kSmfDWTChUzb6693ugN3Nw9cHEpRHDwTVYuXUxo6G3GfTbpheOJzyEeRydn7twOeeZ4HJ2ciXlkyH216jWp+1p93D28CLt3l19WLOHLLz7hq5lzUalUVKpclWWLF7Bx/RratO9IakoKK5ctBiAm+vmOc0GhlCGOuUoaaDkwGMzPk3jRvMkPTvZWVlZPzbtx40bS0tIYO3assdyzsra2Rq/Xk5qaanai6fTp05k0yfRk1l3hwjuq7JPYhXl2lavi9U5vQubMJPHKJax8ClNk2Ag8e/U1LgLi3OgNXJo2I2jKRJKDg7Ap5UeRoSNIi4okeqf5YRP55cZXM/Cb+Dk1/9iQ2aNz+w7hm7bg8VZ7k3zJQcGc7tgNtb0drs2aUnrqZM73HZjnjbTnFfTtTEp+Np6q638Dg4GU0FDub/kD93Ztn164ALN/vRmpIUGmC4pkZBD2/VTcB4yg+MK1GDIySL50jsR/TqLg5fxYNqrtyLBeWTeeJn6f8yIUz6pZfWdOXYwnOlb39Mz5SaFAnxhHwqblYDCgu3sLpYMT1g1amm2gWVV/nbTAC+jjtS+9qk+lUGBIjCdp+yowGMgIu02KnSNWdd40aaDpo8KJWzodhaUVFqWrYdumN/G/zM7zRtrzsq1UBfeuPQmdN4ukq1fQePvg894HpPfozf1fM2+mxZ/KGmmQEnSTxKtXKLdyHU6vv0H0zm35VfVcs/aXn0hKjOezKXOwd3Dk5LGDzPn6CyZ9PY+iviXzu3pGeoMegNp1XqP9W50AKFGyFFevXGLHn38YG2jNH5l24Fu8BC7Ohfj80w+5d+8uXl7mRyHllwYN3zD+27d4CXyLl+C9AT25eOEfKlepRtFixflg9Ccs+2k+Py//CaVSRZv2b+Hk7IxSKQ0dIQ20HPn5+aFQKLh69WqOefz9MyfZXrlyhddeyz6u/8qVK5QrlznnqFChQigUCpM7LG5ubjg5ORl7yx4q+qDnxt7eHq2ZRSqeJDo6Gltb2xxXARo3bhyjR482SdvnUt1s3v8HulgthgwdamfTu5sWzi6k53AXy7v/IKJ27STqzz+AzB92pZUVxcaMJeyXFWAwUPi9oYSt/oWY/XuNeTQennj16JWnDbT0mBgMOh2aQqbxaAoVIi0yKscyl0eMRqHRYOHkSNr9CIqP+oCUO6bj4A06HSkP7pAmXL6Cffny+PTsTuDkqXkTDKDTajPjcXns+Li4kB5lPh6dVsu1Dz/KjMfRkbSICIoNH0ZqaP6uapgRH4chIwOVg5NJutrRiYzYJ/cUKTSW2NV5nZgN2Vf9Swu+zp3Ph6O0tgG1Gn18HD4TZpEaFJib1c/R8XPxXAu6YXxv8WDJY2cHNTGPNLCcHNTcvP30G05uLhZUKWfHtPnm717nFX1SPIaMDJR2pvPklHaO2XrVjGXitaDPgEdu0mVE3ENl7wQqFTyy4ITSqRAWJcsR9+vcvKi+CUNSAgZ9BkobBx5d8kJha59zLAlxmfV9NJaoMJR2jqBUZcYJoM9AHxOR+XnYbdRexbCq2ThzjlseyYiLzTxPP9ZroXZyyTb64SHP3gOI2beL6B2ZDa2U4MzzdJEPPuL+6p9N4nxIn5hAauhtNN4+uVZ3BwdHlEpVtt6vWG00Ts6FzJZxcnZB+1hvYaw2Jlsv3JOE3Qtl59bf+XbeSooUy1xYxLeEH1cv/cPOrRsYNOyj54wkU2Y8SrSPLdCk1cbg7GK+fubi0WpjcH7wu+vg4IhKpaJI0WImeYoUKcrlSzkP6/R/MMLp3t3QF26g2ecQT+xzxhOrjcHZOedRC55e3jg4OBJ2N9Q4PLRh4yY0bNwEbUw0llbWKBSwZeN6PDy9XiiW/KaQhmWu+v8c1/YMXFxcaN68OfPmzSMxMfskda1WS7NmzXBxcWHmzJnZPt+yZQuBgYF0794dAI1GQ7ly5bh8OWtMs1KppEuXLvzyyy/cvZs7F48XL16kak5zh8gc4+zg4GDy+n8d3giZjY6kgGs4VHtkvL9CgX216iTk8MOgtLKEB3f8jNvR641lAZSWVllpD+n1Oc/nyCUGnY74y1dwqv3I8sUKBU61axH/z5PnixnS0ki7H4FCrcb1zSZE7T/wxPwKpQKFJvuy6LnJoNORcPUqjrVqPrJjBY41axJ//snj9A1paaRFRKBQqXB5442Xtox+jjJ0pAZfx6Z8law0hQLrclVIuZ7zjSAAu1oNUKgtiD+yP8c8+uQk9PFxWHh4Y1m8FIln8na+40PJqXru3U8zvkLuphKtTady2ay5u9ZWSkqXsObqjac30N6s70xsnI4T582smpiXMjLQ3Q1GU+KRhXwUCixKlCX99nWzRXQh11G5eJj8v1a5epIRF2PSOAOwqlYffWIcaQH/PL6Z3KfPICMsBLXvo8PnFVgUK53jIwN0d26gdHaDR3peVS4eWY3QnCgUoMrbe70GnY6kwADsH5knhEKBXZVqJF0xP49LaWkF+scaYY+dp7OVsbJG4+WDLtr8zZ8XobawoEQpfy78c/qRaui5+M9p/MqYnz/nX6YCF8+dMkm7cPYk/mXMz4s2Jy01c27W40O7lUoVhsd+v56HhYUFJUv5c/6fs8Y0vV7P+XNnKZ3DIlily5Tj/LkzJmnnzp425rewsKCUf2lC75gOkQwNvYO7u/khuQBBNzJvDLnk0JB6FlnxZNUvM54zzxnPKUrncDwBIiMjiI+PM9voc3J2wdramkN/H8DCQkPlqjkv/CL+f/z/Xpk/g3nz5pGRkUGtWrX4/fffCQwM5MqVK3z//ffUrVsXW1tbfvzxRzZv3szgwYM5f/48wcHBLFmyhL59+9KpUye6dOli3F7z5s05dMh00uu0adPw8fGhVq1aLF26lPPnz3Pjxg02btzI0aNHTZ6j9iwOHjyYZw/CfhEqWxscKpfBoXLmnS6b4oVxqFwGqyIF5w5R+G9rcW3TFpfmLbEqWoyioz5EaWVF1IM7r77jPsN74HvG/LFHDuPW7i2cGzdB4+mFffWaePcfhPboYeMFgPboYbx69sGhTl00Hp441X8d985d0R76O8/jCV35C16d3sKjXVusSxTH7/NPUVpbE7Ypc8hV6Wlf4jtyuDG/fcUKFGr6BlaFfXCoVpUKC+eCQsntpcuNeXxHDsexejUsvb2w8SuV+b5mDe5v+/Px3ee6u6t+xaNDB9xat8ba15cS4z5BZW3N/T8yezBLTZpI0aFDjfntypfHpXFjLH18sK9ShbI//IBCoSR0ZdYcQaW1NTb+/tg86AW39PHGxt8fjUfOFwO5QbtjI/YNm2NfvwkW3kVw7TMUhaUV8X/vBsB98GhcOvfJVs6+4ZsknjmKPiF7o8W2Zn2sylRE7eaJTbU6eH08hcTTx0i+eDZb3pdl854ourV2p3Zle4r5WDJmQGGitTqOns3qvZk6xpc2jU0vVhQKeLOeE3uPann8/sbLkHx4F1Y1GmJZtR4qNy/s2vVGobEk5XTmedu+40Bs3+yUlf/EfhTWtti16oGqkAca/0rYNGxNyvF9phtWKLCqVp/Us1nniLyWcmIfllXqoalYG2UhT2xadAMLS9LOZz4P0KZNH6waZg1jTj1zEKW1DdZvdkbp4o66ZAWsXmtO6pmsc5ZVw/aoi5RC6eiC0s07830xP9Ly+JEBAJEb1uHSsg3OTVtgWaQYhYePQWllTfSuzHNQkQ8/xbPfYGP+uONHKNS6PU4N30Dj4YVd1Rp49h5A3PEjxmPgNfB9bCtWxsLDE5uyFfD9Ygpk6Ik5kLurDrfu0I19O//gr73buXM7mJ/mf0tqSjKNmmauQDl35pf8unyhMX/Ldp3558xx/tiwmtDbt/ht1RJuXL9K8zYdjXkS4uMIvhlIaEgwAHfvhBB8MxBtTGbj0rtwMTy9CrN47gyuX7tM2L1Q/tiwmgvnTlKzzuv/Kp72b3Vi145t7Nuzk9sht1g4bw4pqSk0fbM5ALO//YqVy34y5m/b/m3OnD7Jpg3ruHM7hNW/rOBGYACt23Yw5nmrY1cOHTzArh3buHc3lG1/bOLk8aO0bJO5Wu+9e3dZ++vPXA8MIDw8jOPHjjBn5leUr1AJ3+L/brhm+7c6s9tMPE3ezHyU0Zxvp/Pzg/lhD+M5axLPcm4EBtDqQTzJycksX7KQa1cvEx4exj/nzjBt8md4eflQtXrWzcZtf2zkxvUAQu/c5s8/NrFowff06jsQOzu7fxVPflGolPn2ehXJEMcnKFGiBGfOnGHq1KmMGTOGe/fu4ebmRvXq1VmwYAEAnTp1Yv/+/UydOpUGDRqQkpKCn58f48ePZ+TIkSarMA4YMIAaNWoQGxuLo6MjkDn08cSJE3z99dfMmDGDoKAglEolfn5+dO3alZEjRz5zfUNDQzly5Ai//JK3Dz99Ho7VK1B378/G9+W+/RSA2ys3cH7AuPyqlomY/XtROzrh3XfggwdVBxI4dgy6B0MeNO4eGB65E3vv58xhjN4DBqNxdUOnjUF79DB3f1pkzHP7+9l49x9E0REfYuHsnPmg6j82c2/lsjyPJ2LHLiycnSk2bEjmg6qvXuPie0NJf7BwiKWXp0nvntLSEt/hQ7Eu7ENGUhLRBw9zbdznZMRnPStJ4+JC6WlfonFzRRefQGJAIBfefR/t0ePZ9p/bonbvxsLZiaLvvYtFoUIkBgRwefgHxiGolp6eJnfKlZaWFB3yHlY+PmQkJxNz+DCBX3xBRkJWPHblylLhxx+N74s/GPZ7/4+tXJ/04hPOnybx+EGi7B1xfrsnakdnUkNucm/GF8aFQ9SF3LLNabXw9MG6dAXufj3e7DbVTs649hiIytEJnTaG+MN7idmUv4/aWL8jEitLJcN7e2Nro+JyYBKfzwk2eQaal5sGB3vTn6AqZe1wL6Rh16GXtzjIo1IvnkBha49tkw6ZD6q+F0LsilkYEjMblkqnQiZD4/Sx0cSumIldq+44D/sSfXwMyUd3k/S36Y0Li5LlUDm5knL65a2wm37lNMk2dlg1aIPSNvNB1Qnr5mJIymzkKx2cTUYCGOJjiF87F5smnbAcMB59vJbUk/tJOZa1sJHS1h6bNn1Q2jlgSE0h434oCWvmogt+cg9wbtD+vQ+VoxOevfo/eFD1dYI++9C4cIjG3cPk2IT/uhIMBjz7DMSikBu6WC1xx49wb3nWhbaFqxvFPpmAyt4BXayWxEsXCBz1HhmxubvU/muvNyEuVsu6X37KfBByiVKMmzwTpwdD/KIiwk0W9SpdtiLDP5rA2p8Xs2blIjy9C/PR+OnGZ6ABnDp+iAVzphnff/fNBAA6de9H53cGoFar+WTiDH5dsZBvvhxLSnIyHl4+vD9qvMkDr19Eg4aNiYuL5deflxMTE0PxEiWZMPkrYzyREfdN5lGVLVeeMR+P55eVS/l5+VK8fXwY9/lkk2eG1X2tPkOGjWT9utUsXjgXn8JF+GT8RMqVz1ytV61W88+5M/yx+XdSUlJwdXOnbr0GdOnek3+rfsPGxMZpWf3zskfi+doYT0TEfZOeyDLlKjD64/GsWrmUX5YvwdvHh08eiUepVBIcdJP9e3aRmJiAs0shqlSrwTu9+mFhkTXqJPDaVdb8soLk5GQKFynCkGGjaNyk4NxgF/lLYXie1TDEv9a5c2eqVavGuHG53zgZO3YsMTExLFq06OmZH7HNovTTM/2HeNZ/tRY8SYxIenqm/wi11at1T8ijXO4uyZ3fPtCYf+Dtf9Fyz+xDz//L1LbP9oiC/4qQv55vyfiCzPDD70/P9B9ipcyfB9znFcNLWjDpZShbMvfmR+a2IzVqPj1THnntVN734r9sr9bV0n/AjBkz+OPB0Kzc5u7unm0BECGEEEIIIfKSLLOfu6SB9pL5+voyfPjwp2d8AWPGjMmT7QohhBBCCCFeDmmgCSGEEEIIIV6YLLOfu17NpU+EEEIIIYQQ4j9IGmhCCCGEEEIIUUDIEEchhBBCCCHEC5NFQnKX9KAJIYQQQgghRAEhPWhCCCGEEEKIF6aQHrRcJT1oQgghhBBCCFFASA+aEEIIIYQQ4oUplNLnk5vk2xRCCCGEEEKIAkIaaEIIIYQQQghRQMgQRyGEEEIIIcQLUyhlkZDcJD1oQgghhBBCCFFASA+aEEIIIYQQ4oXJg6pzl/SgCSGEEEIIIUQBIQ00IYQQQgghhCggZIijEEIIIYQQ4oXJIiG5S3rQhBBCCCGEEKKAkB40IYQQQgghxAtTKKXPJzdJA03gWd81v6uQq8IOReZ3FXLVq3R8DHp9flchV9m4OeZ3FXJV4q34/K5CrrHws83vKuSq9PjE/K5CrkqJTc3vKuQaG8WrdV5zTH+1fkMzlK/Spa5PfldAvCSv0l+tEEIIIYQQ4iWTOWi5S/ojhRBCCCGEEKKAkAaaEEIIIYQQQhQQMsRRCCGEEEII8cKUKhnimJukB00IIYQQQgghCgjpQRNCCCGEEEK8MFkkJHdJD5oQQgghhBBCFBDSQBNCCCGEEEKIAkKGOAohhBBCCCFemEIpfT65Sb5NIYQQQgghhCggpAdNCCGEEEII8cJkkZDcJT1oQgghhBBCCFFASA+aEEIIIYQQ4oVJD1rukh40IYQQQgghxP+NefPm4evri5WVFbVr1+bEiRNPzD9nzhxKly6NtbU1RYoUYdSoUaSkpORZ/aSBJoQQQgghhPi/sHbtWkaPHs2ECRM4c+YMlStXpnnz5ty/f99s/l9//ZVPPvmECRMmcOXKFZYsWcLatWv59NNP86yOMsRRCCGEEEII8cLyc4hjamoqqampJmmWlpZYWlqazT9r1iwGDRpEv379AFi4cCHbtm1j6dKlfPLJJ9nyHzlyhHr16tGjRw8AfH196d69O8ePH8/lSLJID5oQQgghhBDiP2n69Ok4OjqavKZPn242b1paGqdPn6Zp06bGNKVSSdOmTTl69KjZMq+99hqnT582DoO8efMmf/75J61atcr9YB6QHjQhhBBCCCHEC8vPB1WPGzeO0aNHm6Tl1HsWGRlJRkYGHh4eJukeHh5cvXrVbJkePXoQGRlJ/fr1MRgM6HQ63nvvvTwd4ig9aEIIIYQQQoj/JEtLSxwcHExeOTXQXsSBAweYNm0a8+fP58yZM2zYsIFt27bx5Zdf5to+Hic9aC/J559/Tnh4OIsWLcqT7UdGRlKuXDnOnDlD4cKF82Qfj3Pr8DYeXXtg4eJC8o3rhHw/m6SrV3LM796xC27t3kLj4YEuVkvMXwcIXbwQQ3paZgalEu8+A3B5sxkWLoVIj4wkcuefhP28/KXE8yxc6tegxJgBOFargJW3O6c6vk/4lr35Xa1sXrVj49ahI57d3sHCxYWkG9e5/d0sEq9ezjmeTl1xb/8WGg/PzHgO7OfO4gUY0jLjUVrb4DNgME4NXsfC2YWkwABCfnjyd5RbbOq9iW2jNqjsHUm/G0LcxhWk375hNq/LkM+wLFUuW3rK5bPELJkBgFXFmtjUbYJF4eIobe2JmDkO3d1beRrD4/p1LUKbpu7Y2ai5eC2OWYuCCA3LeXWrNfOr4ululS19444wvvspyPi+nL8dA7sXpayfHXq9gevBSXw05Qppafo8iQNAU/V1rGo3QWHrQMb9UJL3/EbGvZy/T4WlNVavt8XCvzIKKxv0cTEk712P7mbm36emSn0sqzZA6egCQEZkGClHths/z0tWtd/Apn5LlHaO6MJCSNi6Cl1oUI75FVbW2DbtiKZ8dZTWtmRoo0j8czVpAecfZFBg80YHrKrURWnniD5eS8qZQyQd+CPPY8mJR8dOeL3zDhYuhUi6HkjwrJkkXjb/3SpUKrz79MW1ZSs0bm4kh4Rwe/5cYo8dy/N6bt+6gS2/r0EbE02x4iUZ8N4I/Epn/7/90JGD+1nzyxIiwsPw8vahZ7/3qFazLgA6nY7VKxdz9tQxwsPuYWNrS8UqNejZ911cCrkat3E39DYrl8zn2pWL6NLTKVa8JN16DqBC5Wq5Ht/GbTtZs+kPomO0lPItxgeD+1HWv5TZvEEht1n26zqu3Qgi/H4EQwf0pnO71iZ5ug4aRvj9iGxlO7Rsxsj3BuR6/R+3adt21m3YTHSMlpLFfRn+7gDK+PuZzRt8K4Tlq9YQcOMm4fcjeH9gPzq2b2OSZ8Wva1m5ep1JWhEfb5Yv/CHPYhDmubq6olKpCA8PN0kPDw/H09PTbJnPP/+cXr16MXDgQAAqVqxIYmIigwcPZvz48SjzoPewQPSg9e3bF4VCgUKhQKPRUKpUKSZPnkzPnj2N6eZevr6+ABgMBr744gu8vLywtramadOmBAYGmuwjICCA9u3b4+rqioODA/Xr12f//v1PrFdQUBA9evTA29sbKysrChcuTPv27U26QKdOncprr72GjY0NTk5OZrcTFhbGd999x/jx47OlDx8+nBIlSmBpaUmRIkVo27Yte/eaXvAfOXKEVq1a4ezsjJWVFRUrVmTWrFlkZGQY87i6utK7d28mTJjwtK87Vzg3bkLhIcO5t2IpVwb3J+nGdfy+mYU6h+/Aucmb+Ax+j7srl3KpTw+CZ3yFc+Mm+Ax615jHs3tP3Np3IOT7WVzq04M7i+bj2e0d3N7u9FJiehYqWxvizl/j4geT8rsqOXrVjo1z4yYUGfoBd1cs4fKgviTfCMTv29monZzN5ndp2ozCg4dwd8VSLvbuRvDX03B+owk+g94z5vH9eBwONWoSNHUyl/r1JO7kcfxnfo+Fq1uexmJVpQ4O7XqSsGsDkbPHo7sbgsvgT1DaOZjNH7N8NuEThxhfEd98hCEjg5TzWROTFRpL0oKuEbdtdZ7WPSfdO3jTsZUnsxbdZMinF0hO1TPj87JoLHKeMP7uJxd4e+Ap42vMpMwL6r+ORhnzlPO345vxZTn1j5Yhn1zgvU8usHF7GAa9Ic9isShTDes33iLl8Hbil39Nxv1QbLsMRWFjZ76AUoVt12EoHV1I3LSE+MVfkrTjV/TxscYs+ngtyX9tJn7FN8SvmIHuVgC2bw9G6Wr+QiC3WFaohV3LbiTu30zM/Inowm7j2HcMClt78wVUKhz7foTS2ZW41fOInjOOhE3LyYiLMWaxeb0V1rUak/DHL0R/9ykJO3/DukFLrOs0Nb/NPObSpClFPxjBnSVLuNi3D0mB1ykz+zvUzubPDYXffQ/3Dh0InjWT8z26cX/jBvy/+hobf/88refhv/eyYvE8Ovfoyzff/4Rv8VJM+fxDYrUxZvNfvXyBOd9Mpkmz1sz4/idq1m3AN1PGExJ8E4DU1BSCbgTSqXsfvvn+Jz4aP4W7d0L4avI4k+1MnzgWfUYGE6bN4ZvvFlOseEmmT/qEmOgoc7t9YfsOHmH+0pX07dqRxbO+omTxYnw0cRox2liz+VNTU/Hy8GBwr+64ODuZzfPjt9P4ffmPxte3kzKvnxrWq5OrdTdn/8HDLPxpOb27d2HhnBmULF6MsV98mWM8KalpeHl6MLBPzxzjAfAtWoTfVv5kfH339dQ8iiB/KFWKfHs9D41GQ/Xq1U2utfV6PXv37qVu3bpmyyQlJWVrhKlUKiCzDZIXCkQDDaBFixbcu3ePwMBAxowZw8SJE/Hz8+PevXvGF8CyZcuM70+ePAnAN998w/fff8/ChQs5fvw4tra2NG/e3OT5BG3atEGn07Fv3z5Onz5N5cqVadOmDWFhYWbrk56ezptvvklsbCwbNmzg2rVrrF27looVK6LVao350tLS6Ny5M0OGDMkxtp9++onXXnuNYsWKGdOCg4OpXr06+/btY8aMGVy4cIEdO3bQuHFjhg4dasy3ceNGGjZsSOHChdm/fz9Xr15lxIgRTJkyhW7dupn8YfTr149Vq1YRHR39fF/+C/Do3JXIbX8QteNPUm4FEzJrBvqUVAq1bGM2v135iiRcvEDM3t2khYcRf+oEMft2Y1umrDGPbfkKaA8fJO7YUdLCw9D+fYC4UyewLZPzXcaXLWLn3wRMmEP45j35XZUcvWrHxqNLdyK3biFq+zZSbgVza+Y36FNScW315Hii9+wiLSyMuFMniN6721hXhcYS59cbcWfhPBLOnyM19A53ly8hNfQObu3fytNYbF9vRdKx/SSf/AtdeCixvy/BkJ6Kda2GZvMbkhPRx8caXxr/ihjSU0n5J6uBlnz6EAm7N5IWcDFP656TTq29+Pn3Oxw+GcPNW0lM/+E6rs4a6tdyybFMbJyOaG268VW3ujOh91I4dynOmGdYX182bA/j1013Cb6TzO27KRw4GkW6Lu8aaJY13yDtnyOkXTiGPiqM5J1rID0NTUXzP9qaSnVRWNmQuGERGaE30cdFk3H7OvqIUGMe3Y2L6G5eRh8TgT7mPikH/8CQlorau3iexQFgXa8ZKaf+JvXMITIi7pKwZSWG9DSsqjcwm9+qWgOUNrbErfoBXch19Noo0oOvkRF225hHXaQUqVfPkhZwHr02irRLp0i/fgl14RJ5GktOvLp35/6WzURu20pycBBB33yFPjUFtzZtzeZ3bdGSuytWEHv0CKl373J/4wa0R47i1b1Hntbzj43raNqiDW+82YoiRX0ZPGwMllZW7Nu1zWz+P7esp0r1WrTv2J3CRX3p3msgxUv6s33rBgBsbe34YuosXmvwBj6Fi+JfpjwDh4zk5vVrRNzP7BWIi9Vy7+4dOnR+B9/iJfHyKULPvu+RmprC7Vs596K+iN82b6N1sya0bNoY36KFGT1kIFaWGv7cY/4meBm/Ugzp15Mmr9fDwsLCbB4nRwcKOTsZX0dPncHb04MqFfL+N2f9pj9o1bwpLZq+gW/RIox8/10sLS3Zsdv8aJky/qV4t38f3ni9fo7xQOYFvYuzs/Hl6Gj+xpzIe6NHj2bx4sWsWLGCK1euMGTIEBITE42rOvbu3Ztx47JueLRt25YFCxawZs0agoKC2L17N59//jlt27Y1NtRyW4EZ4mhpaWnsWhwyZAgbN25kx44d2XqEnJycTLogDQYDc+bM4bPPPqN9+/YArFy5Eg8PDzZt2kS3bt2IjIwkMDCQJUuWUKlSJQC++uor5s+fz8WLF812aV66dIkbN26wd+9eY8OqWLFi1KtXzyTfpEmZPSnLly/PMbY1a9Zka8C9//77KBQKTpw4ga2trTG9fPny9O/fH4DExEQGDRpEu3btTIZGDhw4EA8PD9q1a8e6devo2rWrsay3tzcbN25kwIC8GwKgUKux8S/NvVU/ZyUaDMSfOYVd+QqEmymTcOkCLm82w6ZMWZKuXkHj5Y1D7bpE795pzJN46SKubdphWbgIqXduY12yFHYVKnFngQwBeFav2rFRqNXY+pcmbNVKk3jiTp/EtnwFs2Uy42mObZlyJF69jMbLG8c6rxG1a0fmNlUqFGo1+gfDHR/Sp6ZiX7FynsWCSoVF4eIk7NuSlWYwkBpwEU0xPxKfYRM2tRuRcvYYhrTUp2d+CbzcLSnkrOH0+aw7y4lJGVwOTKCcvz37Dj/9Tr1areDN111Zt/WeMc3JQU05f3t2H4xk7tQKeHtYEhKawpLVIVy4Gp8nsaBUofIsQuqxXY8kGtAFX0PtUxxz37hFqYpk3A3C+s2uWPhVxJCUQNrlU6Qe3w3m7qoqFFiUqYbCQvPEoYb/mkqF2tuXpL8faQAYDKTfuIxFkVIkmymiKVOV9JAb2LXtiWXZqugT40k9f4ykv/80xqK7fR2rGo1QFfIgIyoclWcRLIr5kbB9Td7FkgOFWo1t6TLcXbkiK9FgIPbkSewrVOSeuTIaDfrH/u/oU1Owr5x3/+/T09O5eT2At7v0NKYplUoqVqnOtauXzJYJuHqJNh26mKRVqVaLE8cO5rifpMREFAoFtnaZvb32Do54Fy7KX/t2UqKUPxYWFuzavhlHJ2dKlCqdC5FlSk/Xce3GTXp06mBMUyqVVK9ckcvXAnMu+Jz72H3gEF3at0ahyNul3NPT0wm4foPunbJu1imVSqpVqcTlawH/atuhd+/Rpc9ANBYWlCtTmgG938HDPW9HbbxM+bnM/vPq2rUrERERfPHFF4SFhVGlShV27NhhXDgkJCTEpMfss88+Q6FQ8NlnnxEaGoqbmxtt27Zl6tS86wUtMA20x1lbWxMV9fQf96CgIMLCwkyWy3R0dKR27docPXqUbt26UahQIUqXLs3KlSupVq0alpaW/Pjjj7i7u1O9enWz23Vzc0OpVLJ+/XpGjhz5wi3k6OhoLl++TI0aNUzSduzYwdSpU00aZw89HCq5a9cuoqKi+PDDD7Pladu2Lf7+/qxevdrYQAOoVasWBw8ezNMGmtrRCYVKjS7GtKcuPSYaq6JFzZaJ2bsbtaMjpb9fkDlEVa0mYvNGkwvvsF9/RmVjQ/kVv4JeD0old5csInrPLrPbFNm9asdG7eiEQq0m/bF4dDHRWBUtZrZM9J5dmfHMXQgKBUq1mvubNxD2S+aFnD45iYSLF/Du3Y+bt4JJj4nGpcmb2JWvQGronTyLRWlrj0KlMhn+BqBPiEXt7v3U8hZFSmLhVZTYtYvzqorPzcU5825xtDbdJD0mNg0Xp5zvJD+qfk0X7GzV7Nif9YBQb4/M+Wl9uxRmwcpbXA9OpHlDN2ZOKEe/Uf88cX7bi1LY2KFQqtAnmjYA9UlxqAt5mC2jdCqE0tGftMsnSfxtAUpnN6ybdQWVitTD27PyuXpj32sMqNWQlkrixsXoo8yP3sgNSpsHf2sJcSbp+oRYLHIYWqlycUPlVJaU80eJXTkblYsHdu16gVJN0v7NACT9/ScKS2ucR0wDgx4UShL3bCD1n7yfw/U4tdODc8NjI0bSo6OxLmb+3BB7/Bie3XoQdzaz59yhRk2cGzXO09Xn4uNi0eszcHxsSLaTkwuht0PMltHGROPkZNoD7ejkjDbG/OiYtLRUflm2kHoNm2Bjk3lNoVAomDB1Fl9/OZ5enVqgUChxdHJi/OQZ2NnnMMz1BcTGxaHX63FxcjRJd3ZyJOTO3VzZx6HjJ0lITKTFG+ZHGuSm2Lh49Ho9zo8NVXR2cuT2nVDzhZ5BGX8/Ph45jMI+3kTHxLBy9W+M/OQzlsydg42N9b+stXgRw4YNY9iwYWY/O3DggMl7tVrNhAkTXto0IiiADTSDwcDevXvZuXMnw4cPf2r+h0MUzS2X+fAzhULBnj176NChA/b29iiVStzd3dmxYwfOOYxV9/Hx4fvvv+fjjz9m0qRJ1KhRg8aNG/POO+9QosSzD+cICQnBYDDg7Z11AXb9+nUMBgNlypR5YtmAgMy7NWXLljX7eZkyZYx5HvL29ubs2bM5btPcw/zS9Ho0ebw8ql3lqni905uQOTNJvHIJK5/CFBk2As9efY0LTTg3egOXps0ImjKR5OAgbEr5UWToCNKiIoneuf3JOxAv7FU7NvZVquL1Th9CZs8g8cplLH0KU2T4SLx69+PeymUABE2dhO/Y8VTe8AcGnY6kwACi9+7GpvST/0/mJ+vajUi/G5LjgiIvQ9MGrowZnHX++2S6+SWJn0erJu4cPxtDVExWI0/x4HT0x+5wduzPXCjgetAtqlV0pNUb7iz+1fyF7UunUGJIiid5x2owGMgIv43S3gnLWk1MGmj66HDil01HYWmNRemq2LTuRcKv3+VpI+25KRToE+NI2LQcDAZ0d2+hdHDCukFLYwPNskJNLCvXJf63H9Hdv4vaqwh2rXqgj9eSevZw/tb/GdyaPYvin3xK5TVrwWAgJTSUyG1bcWtjfrj0f4FOp2PW9AkYMDB46BhjusFgYPH82Tg6OfHlN3PRaDTs3bmNryaN4+s5P+Ls4vqErRYsf+7eR+3qVXAtlPOw6YKudo2shVlKFvelrL8/PQa8x4FDh2nVLH/mcIqCrcA00LZu3YqdnR3p6eno9Xp69OjBxIkTc2XbBoOBoUOH4u7uzsGDB7G2tuann36ibdu2nDx5Ei8vL7Plhg4dSu/evTlw4ADHjh3jt99+Y9q0aWzZsoU333zzmfadnJw5mMTKKmvFsuedUPg8+a2trUlKSsrx8+nTpxuHZT40qFhh3i1uvnfFHF2sFkOGDrWz6cnSwtkl293Mh7z7DyJq106i/sxc7Ssl6CZKKyuKjRmb2bNhMFD4vaGErf6FmP17jXk0Hp549ehV4BoBBdWrdmx0sVoMOh0Wj8WjdnYhPYeJ7t4DBhO1aweR2zLjSb55IzOeDz/h3s/LM4cV3g3l2oj3UVpZobKxJT06ihITviT17ovfIX0afWI8howMlPamd5ofrob3JAqNJdZV6hK/c32e1e9ZHD4ZzZXABON7C3XmkBYXJwuTXjRnRw3Xg58+aNPDVUP1io588e01k/SHjbVbt00H4926k4y7m+aF6/8khqQEDPoMlLb2ZDySrrRxwJAYZ75MQmbvyKPDGTOiwlDaOYJSBfoHW9JnoNdGZn4efhuVV1EsazTKnOOWB/RJD/7WHlt8RmnnmK1XzVgmXptZ30djibiHyt4JVCrIyMC2RVeS/t5G6oUTD2K5g8rJFZvXW7/0BppO++Dc4PLYuc7FhfQo8+c6nVZL4Ccfo9BoUDs6kh4RQZH3h5ISmjs9PebYOziiVKqyLQii1Ubj5Gy+weHk7IJWaxpDrDYmW36dTsesryYQERHOxGlzjL1nABf+OcOZk0dZvnabMb1EqdL8c+4kB/bs4K1Hhlz+G44ODiiVSqIfW0AjRhv7xAUznlXY/QhOn7/A5E/GPD1zLnB0yLyJHxOjNUnPrXgesrOzpbC3F3fvFaCbNP9Sfj4H7VVUYL7Nxo0bc+7cOQIDA0lOTmbFihVmh/897uH8sSctl7lv3z62bt3KmjVrqFevHtWqVWP+/PlYW1uzYsWKbNt8lL29vXGc6T///EODBg2YMmXKM8fl6pp5lyomJuvk7Ofnh0KhyPGBeA/5P1hZ6soV80t/X7lyxZjnoejoaNzcch7TPG7cOGJjY01e/Yo937L8Bp2OpIBrOFTLGraJQoF9teokXDK/UIHSyjJzSMyj29HrjWUBlJZWWWkP6fXGz8XTvWrHxqDTkRhwDfvqpvE4VKtBYk7xWFphMJip64OyJskpKaRHR6Gys8ehZm20h3Oe4/GvZWSQficIS7/yWWkKBZZ+5Um79eS5GlaVa6NQq0k+fSjv6vcMklP0hIalGF/Bd5KJikmjWsWsRqeNtYpyfnZcDnj6XLGWb7ijjUvn2GnTi9ew+6lERKVRxMd06E8Rb2vCI/Jo/p0+g4yw26iLPTo/R4Ha1z/H+WK60JuonN2ArL8rpbN75jBWfYbZMpmbVaBQ5eH90YwMdHeD0ZR4ZEEFhQKLEmVJv33dbBFdyHVULh4m/0dUrp6Zqzg+WDFYYaHJNrfOkE/naINOR+K1qzjUqJmVqFDgWKMm8RcvPLlsWhrpEREoVCpcGjcm5uDfeVZPCwsLSpTy58K508Y0vV7PhXNnKF2mvNky/mXKc+GfMyZp/5w9if8j+R82zu7dvcMXU2dj72B64yctNXMY8ONztpQKJfpcXHXOwkJN6ZIlOHM+6zvX6/WcPn+RcqXNL0v/PLbvPYCToyN1HumByksWFhb4lyrJ2cfiOfvPecqVzr3VPpOTk7kbFo5LDqO4hCgwDTRbW1tKlSpF0aJFUauf/YerePHieHp6miyXGRcXx/Hjx43LZT7sUXp8iUylUon+8YvOJ1AoFJQpU4bExGeZzp+pZMmSODg4cPmR57K4uLjQvHlz5s2bZ3ZbD1eJbNasGS4uLsycOTNbni1bthAYGEj37t1N0i9evEjVqlVzrI+5h/m9yPDG8N/W4tqmLS7NW2JVtBhFR32I0sqKqB2Zk9J9x32G98CsZc1jjxzGrd1bODdugsbTC/vqNfHuPwjt0cPGi2ft0cN49eyDQ526aDw8car/Ou6du6I9lHc/ns9LZWuDQ+UyOFTOHApnU7wwDpXLYFXEfC9sfnjVjk34utW4tW5HoeatsCpWjGKjP0ZpbUXk9q2Z8Xz6BT6DshbhiT1yCPf2b+P8RlM0nl441KiJd//BxB45ZIzHoWZtHGrVMX5ees5cUkJuEfXn1jyNJfHvP7Gp3RjrGg1Qu3vj0LE/Co0VySf+AsCx+xDsW3XNVs6mViNSLp7GkJSQ7TOFtS1q72KoPTJvtKjdvVB7F8vWU5dX1m+7R6+OhXmthjPFi9rw6fBSRMakcehEVg/AzAnleKuF6dwnhQJaNHZn54EIMsychtduCeXtlp40rOOCj6cV/bsVoai3NX/uvZ89cy5JPbkPTeXXsKhQG2UhD6ybdwULS9IuZM6xsmndC6vX22XlP3sQhZUN1k07oXR2R12iPFZ1m5F6Nuv/hdXr7VAVLonSwQWlqzdWr7dDXdSPtMun8iwOgOTDu7Cq0RDLqvVQuXlh1643Co0lKQ8a+fYdB2L7ZtZjMpJP7EdhbYtdqx6oCnmg8a+ETcPWpBzfZ8yTdvUcNg3boPGvhNKpEJqy1bCp15y0y2ey7f9luLd6Ne7t2uPaqhVWxXzx/XgsSisrIrZm/j8u8cUEigx535jftlx5nBs2wtLbG/vKVSg95ztQKLn3y8857SJXtH2rC3t2buXAnu3cCQlm8byZpKYk0/jNVgB8P3Mqq5b/aMzfql0nzp0+zpYNawi9fYu1q5Zy8/o1WrZ5G8hsnH077XNuBF5lxIefo8/IICY6ipjoKNLTM3uf/cuUx9bOnrmzphF887rxmWj3w+9Rvab5VUlfVOf2rdm6ax879v3Frdt3mL3wJ1JSUmnZtBEA02bPZdHKX43509N1BN4MJvBmMLp0HZFRMQTeDObOY71Jer2eHXsP0LxxQ9R5tFKeOZ06tGXbzj3s3LufW7fvMGf+IlJSUmne9A0Avpr1PT+t+OWReNK5fjOI6zeD0Ol0REZFcf1mEKF3s5aqWbhkBf9cuERY+H0uXbnKF9O+QalU8kbD+i8trrymUCry7fUqKjBDHF+UQqFg5MiRTJkyBT8/P4oXL87nn3+Ot7c3HTp0AKBu3bo4OzvTp08fvvjiC6ytrVm8eDFBQUG0bp31cMQyZcowffp03nrrLc6dO8eECRPo1asX5cqVQ6PR8Ndff7F06VLGjh1rLBMSEkJ0dDQhISFkZGRw7tw5AEqVKoWdnR1KpZKmTZty6NAhY30A5s2bR7169ahVqxaTJ0+mUqVK6HQ6du/ezYIFC7hy5Qq2trb8+OOPdOvWjcGDBzNs2DAcHBzYu3cvH330EZ06daJLl6yVnpKSkjh9+jTTpk3L0+8cIGb/XtSOTnj3HfjgYciBBI4dg+5BT6HG3cPkeUX3fs4cKuc9YDAaVzd02hi0Rw9z96es1Slvfz8b7/6DKDriQyycnTMfhvzHZuO8oYLAsXoF6u7N+jEv9+2nANxeuYHzA8blVOyletWOTcz+vaidnPHuP9D4MNrAj0YZ47F098jqIQPu/rwcg8GAz4B30bi5ka6NIfbIYUJ/WmjMo7Kzw2fQe2jc3NHFx6H96wChPy3EkPGEXo9ckHLuGHG2Dtg174TKwYn00FtEL/7KOOxM5VQoW2+mys0LTYkyRP1o/v+1VYXqOHXLanA79/oAgPidv5Ow6/c8iiTL6k13sbJU8eG7JbCzVXPhahwfT7lCWnrW35iPhyWODqY/N9UrOeLpZsmf+8w3uNZvC0NjoWRoX1/s7dTcuJXEh19e5m543q1gmX71DMk2dljXb43C1p6M+6EkrpuHISmzN1Dp4GLSg2SI15Kwbj7WTd7Gvv+4zLlYpw5kruL4gMLWDts2vVHYOmBITSEjIpTEdfPRBf/7+XtPknrxBApbe2ybdMh8UPW9EGJXzDIO11Q6FTKJRR8bTeyKmdi16o7zsC/Rx8eQfHR35iqODyRsXYVN07ewa9cLpa1D5jPeTh4wzlF72aL37sHC2YnCAwdjUagQSYEBXB010rhIkqWH6blBaamhyLvvYentTUZyMtqjR7gxaSIZCdlvfOSmeq83IS5Wy5pflqKNica3RCnGT/7WOGQxMiIc5SM9XWXKVWTER1+w5uef+HXFYrx8CvPxZ1Mp6ps5/zM6KoJTxzOHlH44vL/JviZO/44Klari4Ji5IMjqlYuZ+OlIMnQ6ihQrzsefT8O3hPkHSL+oNxq8hjYujmW/rst8UHVxX76ZMA6XBwuehUdGmQx/i4yOZtCorOuotZv+YO2mP6hcoRzfTc1ahOH0PxcIj4ik1YOG3svSuEE9YmNjWb5qDTExWkqWKM5Xkz4zDnG8HxFp0jMZFR3DuyOyFnNbt3EL6zZuoXKF8syaPhmAiKgopn47m7i4eBwdHahQrixzv52Ok+PLuZEm/nsUhrx6wtpz6Nu3L1qtlk2bNj0xn0KhYOPGjSYNHcicozVhwgQWLVqEVqulfv36zJ8/32T436lTpxg/fjynTp0iPT2d8uXL88UXX9CyZUuT7S9btoy+ffsSGRnJl19+yb59+wgODjY+GLtPnz6MGjXK2BvXt29fs8Mk9+/fT6NGjQDYvn07gwYNyrZs571795g6dSpbt27l3r17uLm5Ub16dUaNGmUsC3Dw4EGmTp3K0aNHSUlJwc/Pj379+mVbXXL16tVMmjTpqUMnH3e6cb2nZ/oPCTsUmd9VyFWe9f87k7mfJtswyf84nxp5+yyrl637racvzPRfsanmqvyuQq5Kj3/2kRv/BTf3Xn56pv8Im5+3PD3Tf4irztxDCv67MpT/+b4Io8L+5h8vUxDcGtwh3/ZdbNGmfNt3XikQDbRXncFgoHbt2owaNSrbkMTcVKdOHT744AN69Hi+h25KA61gkwZawSUNtIJLGmgFmzTQCi5poBVc0kAz71VsoBWYOWivMoVCwaJFi9DpdHm2j8jISN5+++08bQAKIYQQQggh8tarc1uhgKtSpQpVqlTJs+27urry8ccf59n2hRBCCCGEMEeW2c9d8m0KIYQQQgghRAEhPWhCCCGEEEKIF/aqLnefX6QHTQghhBBCCCEKCGmgCSGEEEIIIUQBIUMchRBCCCGEEC9MFgnJXfJtCiGEEEIIIUQBIT1oQgghhBBCiBenkEVCcpP0oAkhhBBCCCFEASE9aEIIIYQQQogXJsvs5y7pQRNCCCGEEEKIAkIaaEIIIYQQQghRQMgQRyGEEEIIIcQLk2X2c5d8m0IIIYQQQghRQEgPmhBCCCGEEOKFySIhuUt60IQQQgghhBCigJAGmhBCCCGEEEIUEDLEUQghhBBCCPHCZJGQ3CUNNEFiRFJ+VyFXedZ3ze8q5KqwQ5H5XYVco7J+tU7g3tUM+V2FXKWxtsrvKuQapcWr9fOWdD8mv6uQqyztNfldhVzjkXY7v6uQq26pSuV3FXJV0Ywb+V0FIZ7bq/ULJoQQQgghhHipZJGQ3PVq3c4WQgghhBBCiP8w6UETQgghhBBCvDDpQctd0oMmhBBCCCGEEAWENNCEEEIIIYQQooCQIY5CCCGEEEKIFyfL7Ocq+TaFEEIIIYQQooCQHjQhhBBCCCHEC1MoZJGQ3CQ9aEIIIYQQQghRQEgDTQghhBBCCCEKCBniKIQQQgghhHhhClkkJFfJtymEEEIIIYQQBYT0oAkhhBBCCCFemEIpi4TkJulBE0IIIYQQQogCQnrQhBBCCCGEEC9O5qDlKvk2hRBCCCGEEKKAkAaaEEIIIYQQQhQQMsRRCCGEEEII8cJkkZDcJT1oQgghhBBCCFFASAPtJbt27Rqenp7Ex8fn+rY/+eQThg8fnuvbFUIIIYQQIicKhTLfXq+iV3qIY1hYGFOnTmXbtm2Ehobi7u5OlSpVGDlyJE2aNAHg6NGjjB8/nuPHj6NSqahSpQo7d+7E2toaAIVCwcaNG+nQoYPJtvv27YtWq2XTpk3GtNDQUMaOHcv27dtJSkqiVKlSLFu2jBo1ahjzjBs3juHDh2Nvb29MMxgM/PTTTyxdupRLly6h1+spVqwYTZs2Zfjw4Rw6dIh+/fo9MdagoCA+/PBDSpQowahRoyhRosS//PaezqtbF4r064PGtRAJ1wK4Me1r4i9eMptXoVZTZGB/PNq3wdLdnaTgWwTN+o6Yw0eytte1M15dO2Hl7Q1A0vWb3Fq4iJhDh/M8FrcOb+PRtQcWLi4k37hOyPezSbp6Jcf87h274NbuLTQeHuhitcT8dYDQxQsxpKdlZlAq8e4zAJc3m2HhUoj0yEgid/5J2M/L8zyW5+FSvwYlxgzAsVoFrLzdOdXxfcK37M3vamVTdFAPin/QH42HK/EXr3Llo6nEnr5gNq9CrabEmMH49GiPpZcHiYFBBEyYSeSeQ1mZlEpKfToM7y5tsfRwJTXsPqGrNnHjmwUvJR6bem9i90ZbVPaOpN8NIXbDctJDbpjNW2jo51iWKpctPeXyWaIXfwNKFfatumBVtgqqQu4YUpJJDbhA3NY16ONi8joUo95ve9KysSt2NiouBSTy/fLb3A1PzTH/ylnl8HSzzJa+ZU8Ec1fcAWBEvyJULW9PIWcLklMyuByYyJK1d7l9L+ft5jaLyvWxrPEGClt79BF3Sd7/O/qwkJwLWFpjVa8V6lKVUFjZoo+PJvXARnRBOZ9P8opdo5Y4NnsLlaMTaXeCiV69mLTgQLN5PcZMwap0hWzpSRdOEfHDFON7tWdhnDv2xsq/PChVpN+7TcTCr8mIjsyzOB5ybfsW7p27o3ZxIfnmDULnzSHpWs7fq9tbnSnUpgMadw90cVq0B//i3pIfs87TgEUhV7wGDsGhZm2Ullak3r1DyLfTSQ68lufxbPhzF6s3bSNaG0tJ36KMHNiHcv4lzeYNCrnDktXruXYjiLCISIb370mXti1N8iQlJ/PTr+v5+/hJYmLj8C/uywcDelHWz/w2/w2DwcDvvy5i/67NJCUm4F+2Ev2GfIynd9Enltu97Te2bVxFbEwURYv70XvwGEr6lwcgIT6W339dzIVzx4mKCMfBwYnqdRrS6Z13sbG1A+BWUAB/rF9JwJV/iI+Lxc3dizdavEWLdt1yNb4Nf+5izcatxmMzYlAfyvmXMps3KOQOS379jYAHx2ZY/150aWfm2Kz6jYPHTxETG4tfcV8+GNg7T46NeDW8sg204OBg6tWrh5OTEzNmzKBixYqkp6ezc+dOhg4dytWrVzl69CgtWrRg3Lhx/PDDD6jVav755x+UL7BUaExMDPXq1aNx48Zs374dNzc3AgMDcXZ2NuYJCQlh69at/PDDD8Y0g8FAjx492LRpE59++imzZ8/G29ubu3fvsnHjRqZMmcKCBQto0aKFsczbb79NhQoVmDx5sjHNzc0NlUpF8+bNWbBgATNmzHjBb+7ZuLVoRsmPxxA4eSrx5y/i06sHFX6cz6m2HUiPzn5R6Dv8fdzbtCZg4pckBwXhXO81yn03k3M9+5J4NfOHMDUsnKDZP5B8KwSFAjzat6X8D7M506kbSTdu5lkszo2bUHjIcEJmzyDxymXcO3XB75tZXOrdHZ1Wmz1/kzfxGfwewd9MJ/HiBSyLFMV37HjAwJ35mcfWs3tP3Np3IOirKaQEBWFTugy+Y8eTkZhAxIb1eRbL81LZ2hB3/hq3l/9OjfXz8rs6Znm+3ZIy08ZyaeREtKfO4/t+b2psWMzB6q1Ii4zOlt/v8xF4d23LxQ++IDHgJq5N6lN11Q8ce7MH8eczL+ZKjBpI0QHduPDeOBKuBOJQtQIV509DFxfPrYW/5Gk8VlXq4NihF9rflpB+6zq2DVtS6N1PuD99DPqEuGz5o5fNQqHKOlUrbe1x+/Arks8dA0Ch0aApXJz43RtJD72F0sYWx7f64DLwQyJnjc/TWB7q0tqdDs3cmLEohLCIVPp09GL6xyUZ+MkV0tMNZssMnxBgsiqzb2Frvv6kFH8f1xrTAoOT2HckmvtR6djbquj1thfTPy5F79GX0JvfbK5S+1fFqmEHUvauI+PeLTTVGmL79nskLJuGITkhewGlCtuOQ9AnxZO8dTn6hFiUDs4YUpLzvrKPsalRD5fO/YlatYC0oADsm7TDfcQE7n4xFH18bLb8EQu+AnXW35nK1h6vL+aQdCrrJprazRPPj6eRcHgv2i2rMaQkY+FdBEN6ep7H49TwDbzfHcad72eSePUybm93psS0mVwd0MPsedqpcVO8BrxLyMyvSLp8EcvCRSj64adgMHD3x7mZMdrZ4Td7PvH/nOXm+I/QxWqx9ClMRkLuj3B53N5DR5m7bBVj3utPOf+S/PbHDsZM/opf536Ls5Njtvwpqal4ebjT6LXa/LDM/Dnq63mLuRlyh89GDMHVxZldfx1m1MTp/Pz9N7gVcsnV+m/d8DO7tq7j3RFf4ObhzfpVP/L1hBF8PW8NGk32Gy8Axw7uZtWS7+j3/lhK+Zdnx5Y1fD1hBDMWrMPRyYWY6Ei00RH06PcBPkWKE3k/jGULviImOoIRn3wFQPD1qzg4OTNk9CQKuXoQcOU8S+dNR6lU0axN51yJbe+ho8xb+gtjhvSnnH8pftuynQ8nfcWqeTNzPDbenu40rlebH5bmcGzmLiYo5DbjRz44NgcOMXrCNFb+MCPXj414NbyyDbT3338fhULBiRMnsLW1NaaXL1+e/v37AzBq1Cg++OADPvnkE+PnpUuXfqH9ff311xQpUoRly5YZ04oXL26SZ926dVSuXBkfHx9j2tq1a1mzZg2bN2+mXbt2xvSiRYtSp04dDAYDCoXC2KMHoNFosLGxwdPTM1s92rZty/jx4/O8gebTuyf31m8gfNMWAAInT8Xl9QZ4vtWB20uWZcvv3rYNIYt+IuZgZi/GvbW/4VSnNoX79uLaJ58BEP3X3yZlgr+fh1fXzjhUrpSnDTSPzl2J3PYHUTv+BCBk1gwca79GoZZtCF+d/WRrV74iCRcvELN3NwBp4WHE7NuNbdmsXg7b8hXQHj5I3LGjxjxxTd7Etkw5IvIskucXsfNvInb+/fSM+ch3WB9ur/iN0FUbAbg0ciJuzRvi0+ttgmb/lC2/d7d23Pz2RyJ3ZcZ1e8kaCjWqS/HhfTk/aCwATrWrcn/bPiJ2/gVAcshdvDq1xrF6xTyPx65Ra5KO7iP5ROa+Y39bglXZqtjUbkTC3i3Z8huSEnm0LWJd9TUM6amk/HM88/OUZKIWTjN+ngHE/r4Mt9FTUTkVIkMblZfhAPBWC3d+3RLO0TOZF/7f/HiLdXMrUq+6IweOac2WiY3Xmbzv2saB0PBUzl/Navj8uT+r7uGRsHz9XX6cVhYPNw337qeR1yyrNyL94lHSL50AIGXPb6hLlMOiQm3STmbvabaoUBuFlQ3Ja+aAXg9ARlz2mwgvg8Ob7Yk/tIvEI/sAiF61AOuK1bGr14S4HRuy5dcnmTY4bWs2wJCWStLprBEMTh3eIfniGbS/rzCm6SLC8igCU24duxK1/Q+id2Wep+989y0Oteri0rw199euypbftlwFEi9dRLt/D/DgPL1/DzZlss7T7l3eIS3iPrdnTjempYXdy+NIMq3dsp22bzamdZOGAHz4Xn+Onj7Htr1/0bNju2z5y/qVNPa2/Pjzmmyfp6am8dfRk0wbN5oq5csC0L9bRw6fPMOmHXsY9E6XXKu7wWBgx5Y1tO/Sj+p1Muv/3qiJDO3dktPH/qLu683Mltu+eTWNm7WnYdO2APR7/xPOnTrCX3v+oF2nPhQpVpIR47425vfwKkznnkNYMGsCGRk6VCo1Dd80/W7cPX24fu0Cp47uz7UG2rrNf9KmWWNaNWkEwJghA5792Kw0f2z+PnqCaZ+OyTo23TtxJA+OTb6SRUJy1Ss5cDM6OpodO3YwdOhQk8bZQ05OTty/f5/jx4/j7u7Oa6+9hoeHBw0bNuTQoUNmtvh0W7ZsoUaNGnTu3Bl3d3eqVq3K4sWLTfIcPHjQZLgjwOrVqyldurRJ4+xRCsXz/cHXqlWLO3fuEBwc/FzlnodCrca+XFm0x45nJRoMaI8dx75yJbNllBoLDGmmF1T61BQcq1Y1vxOlEreWzVFZWxN37nxuVT0bhVqNjX9p4k6fzEo0GIg/cwq78tmH+wAkXLqAjX9pbMpknmg1Xt441K5L7PFjxjyJly5iX60GloWLAGBdshR2FSoRd+KY2W0K8xQWFjhUKU/U/qNZiQYDUQeO4lSritkySksNGSmmQ+D0KSk416lufK89fpZCDetgU8oXAPsKpXGuW42I3QdzOwRTKhUWhYuTGnAxK81gIDXwIhbF/J5pEza1G5F89iiGtJyH+SmsbTDo9eiTk/5tjZ/K001DIScLzlzM6nVIStZz9WYiZUtlP/+ao1YpaFLPhZ1/5dyYtLJU0vz1Qty7n0pEVN732KBUofQojO5WwCOJBnS3AlB5+Zotoi5ZAd29YKze6ITdu19i23ssmlpN4TnP4/+aSo2maElSrjxy7jQYSLnyD5Ylnu0mpF39piSePJT1d6ZQYF2xBrrwu7iPmEDhb5fjOe4brKvUzoMATCnUamz8/Ek4ezor0WAg4ewpbMuWN1sm8fJFbPz8sSn94Dzt6YVDrTom52DHuvVJCryG72eTKb9uC/7zl+DSsm2exgKQnq4j4EYQ1Stn/cYolUpqVKrApWvmh6A+TYY+gwy9Ho3GwiTdUqPh/JWAHEq9mIjwu8TGRFGhci1jmo2tHSX9yxN4zfzQc116OkHXr1K+SlYZpVJJ+co1uX7VfBmApKQErG1sUaly7k9ISkzE1t7hBSLJ7uGxqVHJ9NhUr5wLx8bisWNjqeHC5bwfSiv+m17JHrTr169jMBgoU6ZMjnlu3szskZk4cSLffvstVapUYeXKlTRp0oSLFy/i55d1sdS9e3dUKpVJ+dTUVFq3bm2yvQULFjB69Gg+/fRTTp48yQcffIBGo6FPnz4A3Lp1K1sDLSAgIFuv3ciRI/npp8yeAScnJ+7cufPMsXs/mL9169YtfH19s32emppKaqrphV2aXo/mOYZ1Wjg7o1CrSYsyvTOcFhWFY/Hs+wSIOXwUn9490Z46Q8rt2zjVqYVrkzdQPPa92viVouqqFSg1GjKSkrk0YgxJN/Ou90zt6IRCpUYXYxpLekw0VkXNj6WP2bsbtaMjpb9fgEKhQKFWE7F5I2GrVhrzhP36MyobG8qv+DXzTrpSyd0li4jesyvPYnkVaQo5oVSrSYswvXBPvR+FrX9xs2Ui9x7Cd1hfYo6cIulmCIUa1cWj7Zsmf2s3Zy1GbW9Hg1PbMGRkoFCpCJw8h3vrtuZpPEpbBxQqFRmPDTHTx8eicfd+anmLoiWx8C6Kdu2inDOpLXBo053ks0cwpOb90DoXp8yLDm2saaMpJlaHs6OFuSLZvFbdETsbFbsOZm+gtW3iysBu3lhbqbh9N4VPvr6OLiPvxzcqrG1RKFUYkkyHuxmS4lG5eJgto3QshLKIH+lXT5O08UeUTm5YNekEShVpx3bmeZ0fUtnZZ/6dxWlN0jPiY7HwKvzU8hpfPzQ+xYhaMdeYprR3RGlljUOLt9FuXkXM7yuxrlAVt/fGEj7rc1IDzM8/zg0qB0cUKjXp2c7TMVgWKWa2jHb/HtSOjpSaNc94no78YxP31/xszKPx8sK1TXsifl9H+OqfsSldhsLvj8CgSydm9448iyc2Pp4MvR4XR9Phcs5ODtwKvftC27SxtqZCaT9WrNuEb2EfnB0d2XPwCJcCAvExM9rm39DGZP4/dXAyHZrn4ORCbIz5HuP4OC16fQaOj5VxdHLhXuitHMtsWruUxs075FiXgCvnOX5oNx9+Mes5IsjZw2Pz+FBGF0dHQu68+LEpX9qPFes2UqxI5rHZe/AIl67l/rHJT4oXmB4kcvZKNtAMhqf/eOsfDD959913jQtwVK1alb1797J06VKmT88a8jB79myaNm1qUn7s2LFkZGSYbK9GjRpMmzbNuK2LFy+ycOFCYwMtOTkZKyurp9Zt/PjxDBs2jA0bNhi396weDoVMSjJ/53z69OlMmjTJJK2vmwf93L2eaz/P68ZXM/Cb+Dk1/9gABgPJt+8QvmkLHm+1N8mXHBTM6Y7dUNvb4dqsKaWnTuZ834F52kh7XnaVq+L1Tm9C5swk8colrHwKU2TYCDx79TUuAuLc6A1cmjYjaMpEkoODsCnlR5GhI0iLiiR65/b8DeAVd+XjaVT4YXJm48tgIDnoNndWbaRwz7eNeTzfbolXlzb8M+CjzDlolcpS5qtxpITd5+6vm/Ox9k9mU7sR6XdDclxQBKUKlz4jQKEg9releVKHN15zZkS/Isb3n8389/83WzQsxMnzcURrddk+23skmtMX4ynkpKZTKw8+G1ackV8G5Di3LV8pFBiSEkjZvRYMBvT375Bm54imRuOX2kD7t+zqNyXtTrDJgiIPR3MknztB/J4/AEi/E4RlyTLYv948TxtoL8KuUhU8uvXizg+zSLp6GUsfH3yGjMAjug/hqx4M0VQoSQ64yr1lmTc8km8EYuVbAtfW7fO0gZZXPhsxhOlzF/HWgGGolEr8S/jSpP5rBNwI+lfbPXxgB0vnf2V8n1uNoSdJSkrg28mj8SlSnLe7DzKb5/atG8ye+hFvdRtIxap18rxO/8ZnI9/nq7k/8nb/oaiUSvxK+tKkwWtc+5fHRry6XskGmp+fHwqFgqtXr+aYx8srs0FSrpzp6mhly5YlJMR0hS5PT09KlTJdvcfe3h7tIxOTvby8zG7r999/N753dXUlJsZ0AQ0/Pz+uXTPt4nZzc8PNzQ13d/cc65+T6Oho4zbMGTduHKNHjzZJO1GnwXPtIz0mBoNOh+axia2aQoVIizQ/RCk9JobLI0aj0GiwcHIk7X4ExUd9QMqdUJN8Bp2OlNu3AUi4fAX78uXx6dmdwMlTn6uOz0oXq8WQoUPtbBqLhbML6dHm7wR69x9E1K6dRP2ZeZGSEnQTpZUVxcaMJeyXFWAwUPi9oYSt/oWY/XuNeTQennj16CUNtOeQFqVFr9OhcStkkm7pXojUcPOrxqVHxXC2x3CUlhosXJxIvXcf/0ljSArO6oku/eWHBM3+ibDfM+ezJFwOxKqINyVGD87TBpo+MQ5DRkbm6o2PpCvtHbP1djxOobHEuuprxO/4zXwGpQrnPiNQObsSOX9KnvWeHT0Ty9Xricb3FhaZd02dHC2Ijs1qYDk7qrlx6+l1cC9kQdUK9kz+zvyFSlKynqTkVO6Gp3LlehAbfqxIvepOHDiWtytUGpITMegzUNjYm6QrbOzRJ2ZfzAXA8OD48shNQn10OEo7R1CqQJ9htlxuy0iIz/w7c3AySVfZO5IR++TvTaGxxLZmfbSbV5vZpo70e7dN0tPv3cGyVNlcqXdOMuJiMWTosMh2nnZGF23+N8ezz0Bi9u4iekdmr3hK8E2UVtYUGfER4b+uBIMBXXQUKSGmvTcpIbdwrN8wbwJ5wNHeHpVSSXSsaU96jDaOQmYWoXhWPl4ezJ36OckpKSQmJePq4syEb7/Hy/P5ryUeVa1WA+NKiwA6XebZK04bjbOLqzE9ThtN0RLmh2rbOzihVKqI1Zr+rsZqo7P1qiUnJTJj4kisrG0Y+enXqNXZL1VDQ24y/bOhNG7egQ5d+79wbI97eGxitKbHJjo2Fhdnpxfero+XBz9M/cL02Mz4Hm+Pf3dsChJ5UHXueiX7I11cXGjevDnz5s0jMTEx2+darRZfX1+8vb2zNY4CAgIoVsz8kIknqVev3lO3VbVqVS5fvmySp3v37ly7do3Nm3PnovDixYtYWFhQvrz5cfmWlpY4ODiYvJ5neCNkNqLiL1/BqfYjcw8UCpxq1yL+nyfPFzOkpZF2PwKFWo3rm02I2n/gifkVSgUKjea56vc8DDodSQHXcKj2yNBThQL7atVJuHTRbBmllSUY9KbbedAj+3CuidLSKivtIb3+5c9F+Y8zpKcTd+4ShRo9cndUoaBQwzpoT5x7Yll9ahqp9+6jUKvxaP8m97dlLeqgsrHOfnwyMvJ+iEZGBul3gtD4PzK/UaHA0q886beePL/BqnJtFGo1SafMzJN90DhTu3kStWAqhiQzKwzmkuQUPXfvpxlft0JTiNKmU7V8VkPGxkpJmRK2XLme/fz7uOavF0Ibp+P4uewrCz4u87+PAguLl/D/SJ+BPvwO6qKPXnAqUBf1J+NesNkiGaFBKJ3cgKz6KZ3d0CfEvrTGWWZFdKSF3MCqzCNzghUKrMpWIvXmk+e82FSvh0JtQeLxv7JtMzX4OmpPH5NkCw9vMqLydukjg05HUmAAdlWy5pGiUGBXpTqJV8z33Cmtsp+DDQ9HvTw4DydeumCcJ/yQZeEipIfn7cInFhZq/EsW5/T5rLrr9XpOX7hI+dLPNhf1SaytrHB1cSY+IZETZy/QoFb1pxd60vZsbPH0LmJ8+RQpjqNzIS79kzV3OykpgRsBl/ArbX6hJbWFBcVLlTEpo9fruXT+JKXKZJVJSkrg6wkfoFJbMPqzb82uCHkn5CZTx79Pgzda06XXkH8V2+NyOjZnzl/K5WOTwMmz56n/L4+NeHW9kj1oAPPmzaNevXrUqlWLyZMnU6lSJXQ6Hbt372bBggVcuXKFjz76iAkTJlC5cmWqVKnCihUruHr1KuvXP/8y6KNGjeK1115j2rRpdOnShRMnTrBo0SIWLcqaK9K8eXMGDhxIRkaGcU5bt27d2LBhA926dWPcuHE0b94cDw8Pbt26xdq1a7PNfXuagwcP0qBBA5NVH/NC6MpfKD11MgmXLhN38SKFe/ZAaW1N2KbMhmbpaV+Sev8+wXMyl523r1gBjYc7iVevoXF3p9j774JCye2ly43b9B05nJiDh0m5dw+VrS3urVviWLMGIe++n6exhP+2Ft9PxpMYcJWkB8vsK62siNqxLbNe4z4jLSKSuz8tBCD2yGE8OncjKTCAxCuXsfQpjHf/QWiPHjau3KY9ehivnn1Iux+eucy+nz/unbsStX1bnsbyvFS2NtiWypprZ1O8MA6Vy5AWHUvK7ZezmtnTBM9dQcWF04k9e5HYUxfwfb83KhtrQn/JXNWx4o9fkXo3nIBJswFwrFEJKy8P4i5cwcrLg1LjhqJQKAn6bolxmxHb91Pyw3dJuXOPhCuB2Fcqh++wvtz5Ofvqdrkt4cA2nHsMIf32TeMy+wqNJUkPLoidegwhIzaG+G2mq4HZ1GlMyoVT2RtfShXOfUeiKVycqJ++AaUSpX3mXXh9UgJk5H3DYOOO+/Ro70FoWAphEWn07eRFlDadw6ezGl1ff1KKw6e0bNmT1fOpUECz1wux+2A0j7eXPd00NKrjzOkLcWjjdbi5aOjaxoO0ND0n/zHfg5XbUk8fwLpFDzLCb5MRFoKmWkMUFhrSL2UukGTV4h0MCbGkHsrspUn75zCaKg2wavwWaWcPonR2Q1PrTdLOvvyVUuN2b8a13wjSbl0nNSgQh6ZtUWisSDiceaOiUL8RZGij0G40XanWrn5Tks4dR5+Yfan5uJ0bcRv8IakBl0i5dgHrCtWwrlST8Jmf5Xk8Eb+vpehHn5IUeJWkq1dwe7szSitrondm9oIX/Wg86VGR3Fv6Y2Zdjx3G7e2uJN8IJOnqZTTePnj1GUjssazz9P0N6/CfswD3br3Q/r0Pm9JlKdSqLXfm5O0qyABd27Vk2vc/UqZkccr6leS3rTtITkml1YNVHad8twBXF2fe65X5fK/0dB3BD+ajp+t0RETFEBgUjLWVFYW9MucxHT97HgwGivh4EXovnPkrfqVoYS9avfF6rtZdoVDQol03Nq1bhod3EdwfLLPv5OJqXNURYNpnQ6lRp5FxdcWW7bvz45zJFC9VlpL+5dixZQ2pKSk0bNIGeNA4++ID0lJTGTJ6EslJiSQnZd7kcXBwQqlScfvWDaZ/NpSKVWvTskMP43w4pVKJg6MzuaFL+1ZM/24hpUuVyDw2f2wnOSXFeGymzpmPayEX3n302NzOOjaR0dEE3gzG2jrr2Jw4+w8GA8Zjs2D5rxQt7G3cphCPe2UbaCVKlODMmTNMnTqVMWPGcO/ePdzc3KhevToLFmQ+jHbkyJGkpKQwatQooqOjqVy5Mrt376Zkyed/cGDNmjXZuHEj48aNY/LkyRQvXpw5c+bwzjvvGPO0bNkStVrNnj17aN68OZB5olu7di2LFy9m2bJlfPPNN6Snp1O4cGGaNGnCrFnPN9Z7zZo1TJw48bnr/7widuzCwtmZYsOGZD6o+uo1Lr43lPQHC4dYenma3L1UWlriO3wo1oV9yEhKIvrgYa6N+5yM+KyLTY2LC6WnfYnGzRVdfAKJAYFcePd9tEePZ9t/borZvxe1oxPefQc+eFB1IIFjx6B7MBxV4+6B4ZGHLt37OXMYo/eAwWhc3dBpY9AePczdn7Ia47e/n413/0EUHfEhFs7OmQ+q/mMz91ZmfwRBfnKsXoG6e7MmzZf79lMAbq/cwPkB4/KrWibCNmxH4+qM36cfYOnhStyFK5zqONi4cIh1YS947G/N7/MPsPYtQkZiEhG7/ub84LHoYrMuOC9/NAW/z0ZQbuYXaNxcSA27z+1l67j+1fw8jyfl3DFi7Rywb9EJlYMT6aG3iPrxq8xeFkDl7GoyRA5A5eaFZYkyRC3IPidV5eiMdcXMHmD3j742+Sxy7mTSbuT9A5LXbbuPlaWSkf2LYmej4mJAIp/OuGEyT8zLXYOjvelPTrXy9ni4atj5d/ZhamnpeiqUtuWt5m7Y2arQxuq4cC2BkZMD0MZln6uWF3QBZ0mxscXytZYobBzQR4SStOFHYyNZae+M/pFjZUjQkrRhIZaNOmDb+2MMCbGknf3L7JL8eS3p1GFi7B1xatcdlYMzaXeCuP/9JOMz0NQubtn+ztQe3lj5lSN89gSz20w+d5yoVQtxbNER524D0YXfJWLh16Rez/u/Me1f+1A7OuHVewBqZxeSb17n5vgP0WmzztOPxhO2aiUGgwGvPgOxcHVDF6sl9thhwpZlra6cHHCVoEnj8eo/GM+efUgLu0fogh+I2bc7z+NpUr8u2rh4lqxZT3RMLKWKF+PbL8bi8mCIY3hElMkqzpExMfQfnfVcwzWbt7Fm8zaqlC/LD1MyG8iJSUn8+PNaIqKisbe3o1Gdmgx6p4vZIYL/Vpu3e5GakszSedMzH1RdrjIfT/zOpMfrflgo8Y8M3a7T4E3iYrX8/usiYmOiKFbCn48nzsHROXMIe/CNa9x4MJdxzLsdTfY3e/FG3Dy8OXF4H3GxMRw+sIPDB7LmCbq6ezHnp025EluT+nXRxsaxdPV6omO0mcdmwiePHZus0RaR0TEMGP2p8f2aTdtYsynz2Hw/9XMAEhKTWfTzGuOxaVi3JoPe6ZonxybfKF7JQXn5RmF4lhU1RK6ZN28eW7ZsYefO3J8wvn37dsaMGcP58+ef6z/93xVyWOr+P8rWzSa/q5Crwg6Zn2v1X6SyfrVO4JUGVMnvKuSqvpEf53cVcs36qiuenuk/JOZqcH5XIVfFBOf98/leFu85s/O7CrnqlqrU0zP9hxTV57Co0n+QR9mCOyQy9tsR+bZvxw+/e+4y8+bNY8aMGYSFhVG5cmV++OEHatWqlWN+rVbL+PHj2bBhA9HR0RQrVow5c+bQqlWrf1P1HL1CTff/hnfffRetVkt8fDz29vZPL/AcEhMTWbZs2at1R0YIIYQQQhRo/6VFQtauXcvo0aNZuHAhtWvXZs6cOTRv3pxr166ZXaAvLS2NN998E3d3d9avX4+Pjw+3bt3Cyckpz+ooV/IvmVqtZvz48U/P+AI6deqUJ9sVQgghhBDiVTBr1iwGDRpkfMzWwoUL2bZtG0uXLuWTTz7Jln/p0qVER0dz5MgRLB48cNzcs4Zz06s13kgIIYQQQgjxfyM1NZW4uDiTV2pqqtm8aWlpnD592uT5xkqlkqZNm3L06FGzZbZs2ULdunUZOnQoHh4eVKhQgWnTppk8Dzm3SQNNCCGEEEII8eKUynx7TZ8+HUdHR5PX9OnTzVYzMjKSjIwMPDw8TNI9PDwICzP/iI2bN2+yfv16MjIy+PPPP/n888+ZOXMmU6ZMyfWv8SEZ4iiEEEIIIYT4Txo3bhyjR482SbO0zP4MvRel1+txd3dn0aJFqFQqqlevTmhoKDNmzGDCBPOr3v5b0kATQgghhBBCvLBHHwvxsllaWj5zg8zV1RWVSkV4eLhJenh4OJ6enmbLeHl5YWFhYfJs4rJlyxIWFkZaWhoajebFK58DGeIohBBCCCGEeOVpNBqqV6/O3r1Zz6fU6/Xs3buXunXrmi1Tr149rl+/jv6RZ64GBATg5eWVJ40zkAaaEEIIIYQQ4t/Ixzloz2v06NEsXryYFStWcOXKFYYMGUJiYqJxVcfevXszbtw4Y/4hQ4YQHR3NiBEjCAgIYNu2bUybNo2hQ4fm2tf3OBniKIQQQgghhPi/0LVrVyIiIvjiiy8ICwujSpUq7Nixw7hwSEhICMpHGn5FihRh586djBo1ikqVKuHj48OIESMYO3ZsntVRGmhCCCGEEEKI/xvDhg1j2LBhZj87cOBAtrS6dety7NixPK5VFmmgCSGEEEIIIV6YQpl/i4S8imQOmhBCCCGEEEIUENKDJoQQQgghhHhxCunzyU3ybQohhBBCCCFEASENNCGEEEIIIYQoIGSIoxBCCCGEEOLFySIhuUp60IQQQgghhBCigJAeNCGEEEIIIcQLU8giIblKvk0hhBBCCCGEKCCkgSaEEEIIIYQQBYQMcRSorV6tPwODXp/fVchVKutX5z5KRvKrdWyUalV+VyFXJccl5HcVck1Galp+VyFX6XWv1v+djPRXJx51Rkp+VyFXpSterfOaUq/L7yr8f5BFQnLVq3PlJ4QQQgghhBD/ca9W14kQQgghhBDipVIopc8nN8m3KYQQQgghhBAFhPSgCSGEEEIIIV6cQuag5SbpQRNCCCGEEEKIAkIaaEIIIYQQQghRQMgQRyGEEEIIIcSLk0VCcpV8m0IIIYQQQghRQEgPmhBCCCGEEOLFySIhuUp60IQQQgghhBCigJAGmhBCCCGEEEIUEDLEUQghhBBCCPHCFLJISK6Sb1MIIYQQQgghCgjpQRNCCCGEEEK8OIX0+eQm+TaFEEIIIYQQooCQHjQhhBBCCCHEi1PKMvu5SXrQhBBCCCGEEKKAkAaaEEIIIYQQQhQQ0kB7TK9evZg2bVp+V+O57dixgypVqqDX6/O7KkIIIYQQ4v+IQqHMt9er6LnmoPXt25cVK1YAYGFhQdGiRenduzcBAQGsWrUqx3LFihUjODgYg8HAhAkTWLx4MVqtlnr16rFgwQL8/PyMeQMCAvjoo484fPgwaWlpVKpUiS+//JLGjRvnuP2goCDGjx/PgQMHiI6OxtXVlerVq/P1119TpkwZgoOD+fLLL9m3bx9hYWF4e3vTs2dPxo8fj0ajMW7nn3/+4c8//2TBggXGtEaNGvHXX3+xevVqunXrZkyfM2cOc+bMITg4GIDly5fTr18/ypQpw5UrV0zq99tvv9GlSxfj9/Aw/8iRI9FqtdniUSgUbNy4kQ4dOhjT9u/f/z/27ju6qeoB4Pg3SZvRle6WlkJZZe89ZYMCshQFZAiIKLhYijJkyBIQQRmyUZbsvTfIlj3LaKGT7pkmaZLfH5WU0BQKpBT93c8575zm5t6be99LX959dzx+/PFHTp06hUajITAwkDfffJPBgwfj7+9P69atGTVqFCtWrKBHjx657itb8n33Xfx6fIDcw4O04GDu/fgjqVevWY0rkcnw//BDvNu2Qe7lhSY0lNDZv5B44oQ5jkvVqvj16IFT2TLIvby4MWQo8YcPv5K6eHXojO/73bF3dyf9zm0e/DyDtBvW6wLg/c57eLfviNzHl8ykRBIOHSRswVxMOh0AUpUD/n3749qwEfZu7qQH3+L+7J9Iv3E91zxtqchH3Sj2eR/kPp6kXLnB9WE/kHTustW4Ejs7ig/pj3+39igK+ZAWfI9bY6YTu+9YdiSplJLfDsKvSzsUPp5oox4SvmITd6bOtZpnQXBvUIPiQ/qirlYBpZ83Zzt/SvSW/QVdrBwc6jXH8Y02SJ3V6CPvk7JpOfoHd63GdR/wHfISZXOEZ1y/QOLiaQAoKtTAoW4z7P0DkTo6E/vTt2RG3M/XOjypb7eitGvhi5OjjMs3kpk+9zZhkRm5xv/zt5oU8lHmCN+wI4Kf5t8BwN3Vnk97F6NGFTccVDIehGtYvvY+h0/E5Vs9ABTVGqGo3QKpkwuGh2Gk7/kTQ2RorvElChXKN95GXroKEqUDxuR40vetI/POVQDkVRuiqNYImdodAENsJJpjO8i8m/v5xZacm76FunVHZGo3dA/uEbfiN3T3gq3G9R3+A6oyFXOEp188Q/TP4wEotniL1bTxfy4haddG2xXcCq8OnfB5rxv27u5o7tzm/qynn1O9O3fB6+2OyH18ss7Thw8RvmAeJn3WeRqpFL9efXFv0RJ7dw/0sbHE7t5B1O9L87Uej6zbeYAVW3YRn5hEyaIBDO7bjfKliluNu3nvYXYePsHdB+EAlC5elAHdOlnEX7hmM3uPn+ZhXDz2dnZZcbp2onyQ9TxfhslkYtOq+Rzeu5H0tFRKlalMjwHf4OtX5Knp9u/4k50bfycpMY4igaXo/tEwigdVsJr/T+O/4PLff/HZN9OoVqexxfvH9m9l95YVREXcR+XgSM16zenx8dc2q9/6nftYtWkH8YlJlAgM4Kt+PShXqoTVuFv2HmTXoePcvR8GQOkSgXzc/V2L+IdPnmHT7oPcvHOP5NQ0lkwfT6liRW1WXuG/57mbna1btyYyMpLg4GCGDBnC999/T6lSpYiMjDRvAEuWLDG/PnPmDABTp05l1qxZzJs3j1OnTuHo6EirVq3IyMj+IW/bti2ZmZkcOHCAc+fOUblyZdq2bUtUVJTV8uj1elq0aEFSUhIbNmzg5s2brFmzhooVK5obPzdu3MBoNDJ//nyuXr3KTz/9xLx58/j2228t8po9ezbvvvsuTk5OFuFKpZKRI0ei1+ufum8cHR15+PAhJx5rdAAsWrSIIkWeftJ6mvnz59O8eXN8fX1Zv349165dY968eSQlJTF9+nRzvN69ezNr1qwX/pzn4dGiBYFffUnYgoVc/KAHabeCKTd7NvZublbjF/n0E3w6deTujz9yvst7RK3fQOkfp+JYOsgcR6pSkRZ8i7tTpr6SOjzi1qQZAQM/J2LZIq591BvNnWBKTfsJO1frdXFv3pLC/T8hYtlirvR8n5ApE3Fr2gz/jwaY4wQOH4FLjZrc+2EcVz/8gOQzpwiaPgt7T698r49vpzcpM/Frbk/+lb8adibl8k1qbFiA3NPdavxSo74g4MMuXBv2A8dqteXB4jVUXTEb50rZDYPiX/WjSN/3uT5sAsdqtuHm6OkU+6IvRQd8kO/1ySuZowPJl25y5fOxBV2UXCkr18a5XXdS924kduZIMiPu49bva6SOLlbjJyybycNxA81b7LSvMRkMaC+dMseRyBXo7t0kZceaV1UNC906FaZzGz+mzQ3m42EX0GQYmf59BeT2uU8Y7z/0Au17nTRvX47Ounlw8HisOc53X5YmwN+BET9cpdfnf3P4RCxjh5WlVDHHfKuLfdnqqJp1JuPYdpIXT8IQHY7Te58hcXCynkAqw6nr58jUHqRuWEDyb2NJ37ESU0qiOYopJRHNoU0kL5lM8tIp6ENu4fTOAKSehfKtHo841myAx3t9SdyymoixX6F7EILv4LFIndVW4z/8dRL3v+xp3sJGDsRkMJB29rg5zuPv3/+yJzGLf8ZkNJJ27q98rYtbk2YU/uQzIpct5nr/PqTfuU2pqTOwc3W1Hr9ZC/z7DyBi+WKu9upGyI+TcWvSDP+PPjbH8e36AV7tO3B/1gyu9upG2G9z8H2/O16d3snXugDsO36aWcvW0Pfdt1k6dQylAgP4asJPxCclW43/99WbtGhQi1++H8ZvE7/Fx9OdL8fP4GFcgjlOgJ8PQ/p1548Z45g34RsKeXvyxYQZJCSl2Lz8OzYuY++21fQcMIJRU5ciVyqZMfYz9DptrmlOHdvD6sU/0f79j/h+xh8EBAYxfexnJCfG54i7Z+vKXPPZvfkP1q+Yw1udevPDrD8ZNnYOFarWsUm9APYfO8kvS1byYZcOLJo2jpKBRRg87kcSEq0fm/NXbtC8QR1mjxvB/Emj8fHwYPDYH4mJy66XJkNHpbJBfNLjPZuV87UjlRTc9h/03A00hUKBr68vRYsW5ZNPPqF58+bs2rULX19f8wbg6upqfu3l5YXJZGLmzJmMHDmS9u3bU6lSJZYvX05ERASbNm0CIDY2luDgYL755hsqVapEqVKlmDx5Munp6Vy5csVqea5evcqdO3eYM2cOderUoWjRotSvX58JEyZQp07WP2zr1q1ZsmQJLVu2pHjx4rz99tsMHTqUDRs2mPMxGAysW7eOdu3a5fiMrl27kpiYyIIFC566b+zs7OjWrRuLFy82h4WFhXHo0CG6dev2XPv58fSff/45n3/+OYsXL6Zx48YEBgbSqFEjFi5cyOjRo81x27Vrx9mzZ7lz584Lfdbz8OvejehNm3i4dSuae/e4O2kShowMvN9+22p8r7feInzJUhKP/4U2PJzo9etJ/Osv/LpnX+An/vUXD+bOI/7QoXwv/+N8unQldtsW4nZuJyM0hNDpUzFmaPF8q63V+E7lK5J65TLx+/agi4oi+exp4vfvxbFMOSDrgtmtUWPC5v1K6qULaMPDiFi6CG14GF7tO+Z7fQIH9eLBsrWEr9hI2s07XP3yewyaDPx7dLIa3+/9t7k7/Tdi9xxBExLGg0WridlzhGKf9TbHca1dlYfbDxCz+zCa+xFEb95D7IHjqKvnvNteUGJ2H+HWmJlEb95X0EXJlUOjN0k/dRDN2SMYHkaQvGEJJr0WVa03rMY3adIwpiSZN3mpCpj0OjIunjbHyfj7OGn7NqELtn6OzG9d2vmzfO19jp2O505oOj/MvImHu4KGdTxzTZOYrCc+MXurV8OdsEgNF64kmeNUKOPChu0RXA9OJTI6g+VrH5Calknpkrk0lmxAWasp2ovH0V0+iTEuivRdqyBTh7xSPavx5ZXrIVE6kLp+HobwuxiT4sl8EIzhYbg5jv72ZTLvXMWYEIMx/iEZR7Zg0mmx8yuWb/V4xKVVe1KO7CH12H70EQ+IWz4Hk06Lc8PmVuMb01IxJCeaN1X5qph0WtLOZDfQHn/fkJyIQ5XaZNy4TGZMdL7Wxefd94jdvpW4XTvICA3h/owfMWZo8Xjz6efphP170UVHkXL2NAkH9uJYJvvGk2P5CiQeP0ryyRPooqNIPHKI5LOnzefy/LRq6x7ebt6Itk0bUCzAj+H9e6BQyNl24JjV+GO/7E/n1k0JKlaEQP9CjBjQG6PJxNnL2T2IrRrWoValcvj7eFE8wJ8ver1HWrqG26EPbFp2k8nE3q2raNelL9VqNyYgsBQffTGOhPgY/j51KNd0ezavoFHLDjRs9jb+AcXp+ckI5AolR/db9srev3uT3ZtX0Pez0TnySEtNZsOKuXz0xVjqvtEa70KFCQgsRdVczqEvYvXWXbRr0Zg2zRpRLMCfYR/3RqlQsO2A9RE9Y776hE5vNqdUsaIULezH15/2xWgycvZSdi9568b1+bBLB2pULm+zcgr/bS89cFOlUqH7Z1jX09y7d4+oqCiaN8/+YVCr1dSuXdvc4+Th4UHp0qVZvnw5aWlpZGZmMn/+fLy9valevbrVfL28vJBKpaxbtw6DwZDnciclJeHunt2jcOnSJZKSkqhRo0aOuC4uLnz33XeMGzeOtLS0p+bbp08f/vzzT9LT04GsoYytW7fGx8cnz2V73Nq1a9HpdAwfPtzq+66P3T0sUqQIPj4+HD169IU+K68kdnY4lSlD0qnsi0RMJpJOn8a5kvULdom9PcYn7qwZM7Q4V6mcn0V9JomdHY5BpUk+dyY70GQi+dwZHMvnHHYBkHr1Mg5Bpc0/4vJCfqjr1CPpVNb3WCKTIbGzw/jE/4VRq8W5Yv7WV2Jvj0uV8sQdfKwX12Qi7tAJXGtVsZpGqpBjyHjy2GTgVif7fy7x1Hk83qiDQ8lAAJwrlMatbjVi9ubvd+0/RSbD3r8YuuCr2WEmE7rgq9gXLZmnLFS1GpNx4QQmfe53qV+lQj5KPNzlnL2YaA5LSzdw/VYK5Us75ykPOzsJLRt7s2Of5QX+lRvJNG3gibOTHRIJNGvohVwu5fzlpFxyeklSGTLfImTeu/lYoAl9yA3s/K03puSlKpIZfg+Hlu+j/nwyLv1GoqzbCiS53NGVSLAvWx2JvZzMcOvDWm1GZoeiaEk01y5kh5lMaK5dRFGiTJ6ycG7YnNTTRzHl0isidXHFoVINUo7utUGBcyexs8PBynk65e+zOD3jPO3wT4NMXsgPl9p1STp10hwn7eoVnKvVQFE4AABViZI4VahE8umTVvO0Fb0+k5t3Q6n52CgFqVRKzYrluHIzbzdYM3RaMg0GXJys9yjr9Zls2nsYJwcVpQIDbFLuR2Kiw0lKiKN8pVrmMAdHJ0oEVeD2TetD6TP1ekLu3KB8pdrmMKlUSrnKtbh985I5TKvNYP6MkXzQfzhqt5w3ea5eOIXRZCIh/iHfDnqHwX3fYs7Ub4iLsT7K6nnp9ZncuhNCjUrZDSmpVEqNSuW4evN2nvLQPjo2zvnX2y/8973wc9BMJhP79+9n9+7dfPbZZ8+M/2iI4pMNFR8fH/N7EomEffv20aFDB5ydnZFKpXh7e7Nr1y7cchk65+/vz6xZsxg+fDhjx46lRo0aNGnShO7du1O8uPVx17dv32b27NlMmzbNHBYaGopMJsPb29tqmk8//ZSff/6ZGTNmMGrUqFzrWbVqVYoXL866devo0aMHS5cuZcaMGdy9m/PHOCkpKcdwyicFBwfj4uJCoUJ5Gw7j5+dHaGju8yW0Wi1areWPrc5oRC7Ne1vdztUViZ0dunjLYQn6+HhUgYFW0ySePIlft+4k/32ejLAw1LVq4t60CZLn+Nz8YKfOqos+wbIumQnxKItYHx8ev28Pdmo1pX+ZBxIJUjs7Hm7eQNQfWfMzjZp0Uq9cxq/nh9wNDUGfEI97sxY4la+ANjwsX+sj93BFameHLsZyno72YRyOQdYvMmP3HyNwUG8S/jpL+t37eDSui0+7FkhkMnOcuzMWYOfsRMOz2zEZDEhkMoLHzSTyz235Wp//EqmjMxKZDGOqZQPDkJqE3PvZ/9/2AcWxLxRA8tqn9+S/Sh5u9gAkJFrejIhP1OHuJreWJIeGtT1wcrRjxwHLBtqYH68zdlhZdqyoS2amkQytke8mXSM8Kve5bS9D4uCERCrDmG45jMmUloLMw/oNNqmrJ3ZFPdBdPUPqn78idfPGodV7IJORcWxHdjwvP1x6DgU7e0w6LakbfsMYZ5sLytzInF2QyGQYkhMtwg3JidgX8n9menmxUsgLBxKzZHaucZzrNcWYoSH93Ilc49iCndoVicyOzCfO0/qEeJS5TB9I2L836zw9ay4SiQSJnR0xmzcStWK5OU7Uyt+ROThQftlKMBpBKiVi0W/E79uTr/VJTEnBYDTirrYc2uzu6kJoeGSe8pjzxzq83FypWcmyt+/Y2YuMnjmfDK0ODzc1P48egqtL3m6W5FVSYtbvi4urh0W4i9qdpATrc0RTUhIxGg24uFoOtVer3YkKCzG/XrVoOiXKVKJa7cZW84mJDsdkMrJt3RK69RuKg4MTG1bMZdr3Axk/czV29vYvXjEg6dGxcX3y2KjzfmyWr8HTzc2ikfd/4T+6WEdBee69uW3bNpycnFAqlbz55pu89957fP/99zYpjMlkYuDAgXh7e3P06FFOnz5Nhw4daNeunXlumzUDBw4kKiqKFStWULduXdauXUv58uXZuzfnXb3w8HBat27Nu+++y0cffWQO12g0KBQKJLnc+VQoFIwbN45p06YRGxtrNc4jffr0YcmSJRw+fJi0tDTeeustq/GcnZ25cOFCju3JfZJbmaxRqVTm3jtrJk2ahFqttth+j8rbSedl3Js2Hc2D+1Rdt5a6J/6i+PDhPNyyFdO/cNVJ5ypVKdS9F/d/+pHrH/Xm9shvUNepR6GeH5rj3PthLEgkVN6wlep7D+PTuQvx+/diMpkKsOTWXR8+kfQ7ITQ8u52WcZcoN20kYSs2Whwb305vUqhLWy72HcZfDTtzecAIAj/vg1+39gVY8v8vqlqN0Ufez3VBkVehxRte7F5dz7zZyV7+B7ltC19OnYsnLt6ykdevWyBOjjK+HHWZfkMusGZzOGOHlaV4UYeX/kybkUgwpaWQvnMFhqgH6K+fI+P4LhRVG1pEM8ZFk7x4EinLpqL9+yiObXsi9fAtoELnjXPDFugehOS6oAiAU8PmpJ48jCnz6fOzC4JT5aoU6t6T+zOnc63/h9wZNQJ1nbr49uhtjuPWuCnuzVtyb8L3XOv/ISGTJ+DTpSvurd4suILnwfKNO9h7/DSThw1EIbdskFSvUIZlP47htx9GUKdKBUbOmJfrvLa8OnF4JwPeb2jeDJmZL5Vfbs6fPsz1y2fp1ndIrnFMRiOGzEy69xtGxap1KVG6Ih8P+YHoyAdcv3I2X8r1PH7fsJX9x08x8evPUcjzdpNKEKx57h60Jk2aMHfuXORyOX5+ftjZ5S2LR3PToqOjLXqDoqOjqVKlCgAHDhxg27ZtJCQk4OKSdfdizpw57N27l2XLlvHNN9/kmr+zszPt2rWjXbt2TJgwgVatWjFhwgRatGhhjhMREUGTJk2oV68ev/32m0V6T09P0tPT0el0Fis7Pu6DDz5g2rRpTJgwgcBceooAunfvzvDhw/n+++/p0aNHrvtIKpVSsuTThzYFBQWRlJREZGRknnrR4uPj8fLKfSGKESNGMHjwYIuwvxvnvkKmNZmJiZgyM5G7W94Js3d3Rx9n/e5ZZmIiN4cOQyKXY69Wo4uJoehng9CGRzzXZ9taZlJWXezdLOti5+aOPt56Xfz69iduzy5it28FQHP3DlKlkqJDvyHy96VgMqGNCOfmF58iVSqROTiij4+j+JjxaCPCreZpK7q4RIyZmci9LO9sKrw90EZbv7Ggj0vgfLfPkCrk2Lu7oo18SNDYIaSHZPf2lR4/lHs/LSRqfVavQOq1YJQBfhQf3J+IlZvzr0L/Ica0FEwGA1InywUaZE5qjClPH7YnsVegrFyH1D3r87OIz3TsdDzXbv5tfm1vn9VAc3OVE5eQfZHu7ion+F7qM/Pz8VJQvZIrIydbrmjo56ukc1s/egw6R8iDrBtOd0LSqFzehY5v+TF9bt6GGj0PU3oqJqMBqYMLjw+Wlzg6Y0y1foFrTE0GgwEeu/FiiIvKOsZSGRj/yclowJgQk/V+1APsChVFWbNJ1hy3fGJIScZkMCBzcbUIl7m4YkhKfGpaiVyBU62GJGzKfaEGRalyyAsVJmZe/i/qlJmUiMmQid0T52l7N3f08TkXmADw6/MRcXt2E7cj6zydce9u1nl6yNdZox1MJgoPGEjUqj9IOLjfHEfu40uhbj2I370z3+rj6uyMTCrN0XCKT0zGw9X6Ai6PrNi8i9837mDW6KGUtDJ0UaVUEFDIh4BCPlQIKsG7g0awdf9RenVq88LlrVKrkcVKi5n/rIKZnBiHq3v2MMTkpHgCigXlSA/g7OyKVCrLsSBIUlI8Lm5Zv1fXL50lJiqMgd0tr0l+mTqcoLJV+OaH31D/83l+AdkjQlzUbjg7uxJvg2GO6kfHJvHJY5P0zGOzctMOVmzYzszvh1My8MUXhvvXeo7OBOHZnvv2p6OjIyVLlqRIkSJ5bpwBFCtWDF9fX/bvz172Ojk5mVOnTlG3bl0Ac8+P9Ilhb1Kp9Lme7yWRSChTpozFfLHw8HAaN25M9erVWbJkSY7PeNRIvHYt96WPpVIpkyZNYu7cuebl8q1xd3fn7bff5vDhw/Tp0yfP5bbmnXfeQS6XM3Wq9R/Bx5fpz8jI4M6dO1StWjXX/BQKBS4uLhbb8wxvBDBlZpJ64wbqWjWzAyUS1DVrknLJ+vhzc1qdDl1MDBKZDPemTV/ZMvq5liczk7RbN3Gu/tjcQ4kEl2o1SLtqfdEFqUKJyfTE9/HR9/OJE5QxIwN9fBwyJ2dcatYm8Xj+ztky6fUkX7iKR+PHVrSSSPB4ow6Jpy88Na1Rq0Mb+RCJnR0+7VvwcHv2/6rMQZWzt9NgKPAhqv8qBgP68HvISz427EUiQV6yPPrQpzc4lJVrIbGzQ/P38afGy28ajYHwqAzzFvIgnbh4HdUruZrjOKhklA1y5urNZ68c91YzHxKT9Jw4a3nRplRkfa+e7HE2GvNxwS6jAUPUfewCSz8WKMG+aGkyw+9ZTZIZdgepmxeQXSiZuw/GlMTsxpk1EgnIXniGQd4YMtGG3kZZ9rF5rxIJqrKV0N658dSkjjXrg709qScO5RrHuWELtCHB6B6E2Ka8T2HKzCT91k1cqlmep52rVSc1t/O0UgFPnKdNT5ynpQplzvOa0ZjvF5r29llL4D++wIfRaOTs5etUKG19KXeAPzbtZMn6bfw08ivK/jMf+FlMJhN6/cv1eKlUjvgUCjBvfgHFUbt5cO1S9pxATXoqd25doWRp6/PQ7eztCSxRhmuXsueuG41Grl86Q8nSlQBo07kX42auYuxPK8wbQNc+g+n7+RgASpXJ+j5HhWdP5UhNSSIlJREPr5dfGdXe3o6gEoGcu5Q9V9hoNHLu0jXKl879hvqKjdtZtm4z00YNpUxJ2z/WQPj/k8+/ENkkEglffvklEyZMoFSpUhQrVoxRo0bh5+dnft5X3bp1cXNzo1evXowePRqVSsWCBQu4d+8ebdpk3/0pU6YMkyZNomPHjly4cIExY8bQo0cPypUrh1wu5/DhwyxevJivv856JsajxlnRokWZNm0aMTEx5rwe9ex5eXlRrVo1jh07Zm6sWdOmTRtq167N/Pnzn7rwx9KlS5kzZw4eHh65xsmLgIAAfvrpJwYNGkRycjI9e/YkMDCQsLAwli9fjpOTk3mp/ZMnT6JQKMwN3vwUsWIlpb4fQ+q166RevUqhbl2RqVQ83Jp1t7Lk2O/RPYzh/q+/AuBUvjxyb2/Sbt1C7uVFQP/+SCRSwpdnzweQqlQoA7LvCCr8/XAICiIzKQlddP6tEBb95yqKjRhF+o0bpN24is877yNVKYndmTW/KvDb0ehjYghfkPXMr6S/juHTpSvpwbdIu3YVZeHC+PXpT9Jfx8wNNZeatUEiIeN+KMrChSk8YBAZ90OJ25H/c7ZCfllGxXmTSDp/haSzlwn8tCcyBxXhf2Q9o6ji/MloI6K5NfYnANQ1KqEs5EPy5esoC/lQcsRAJBIp935eZM4zZudBSgz9mIywSFKvB+NcqRyBg3oT9vsGq2UoCDJHBxxLZt+1dChWGJfKZdDFJ5HxIP+H8eZF+pGdqN/7GH3YPfQP7uDYsDUSuQLNmawbFer3P8aQlEDqzj8t0qlqNibj6jlM6Tl7pSQqR2RuHkhdsubp2v1zkfJo5cf89ufWcHp1CSAsUkNkdAb9uhUlLl7L0ZPZPbYzx1XkyMlYNuzIPg4SSVYDbefBaAxPXCOHhml4EKFh6KelmLPkLkkpmTSs7UGNyq58PeEq+SXj9AEc2/bEEBVKZkQoyppNwF6B7lLWHCuHtr0wpiSScTir11j791GU1d9A1eJdtOcOIXXzRlmvFdqzh8x5Kt9oT+bdqxiT40GuRF6uJnZFS5G6+pd8q8cjybs349nvS3Qht9Heu4VLi7eRKJSkHMu6+eLZ70sMCfEkrF9ukc65YQvS/z6JMc16I1uiVOFYsz7xaxZbfT8/RK9dQ+A335F26wbp16/h/U4XpEolcbu2AxA4YiS6mFgiFs4DIOmv4/i8+37Wefr6NRT+hfHr8xGJJ46bz9OJJ45T6INe6B5Gk3HvHg6lgvB+9z3idm7P9/p0bdeS8b8sokyJQMqXLMbq7fvI0Gpp26Q+AGNnLcTLw41Pu3cG4PeNO1iwZjNjv/yIQl6exCVk/W+rlAocVEo0GVqWrt9Gw5pV8HBTk5ScyrpdB4iJT6BpvZyLn70MiURCi3Zd2bp2ET5+AXh6+7Nx5Vzc3L0s5o5NHfUJ1eo0pnmbrKXlW7bvzsKfvyewZDmKlyrPnq0r0WZoaNAsa/VstZun1YVBPDx98fLJmjfp61+UqrXeYOWiafT69DtUKkfW/f4rhfwDKVPRNvV8v11rfpi9gDIli1G2VHH+3LoHjVZLm6aNABj/83y8PNwY8EEXAP7YsI1Fqzcw5qtPKOTtSVxCIgAqpRIHVdbzHpNTUomOjSM2Puu9+//MZ3N3VePh5mqTchc4ccPWpl5ZAw1g+PDhpKWl0b9/fxITE2nQoAG7du1Cqcz6Ant6erJr1y6+++47mjZtil6vp3z58mzevJnKlbPvAt68eZOkpKyTU+HChQkMDGTs2LGEhIQgkUjMr7/66isA9u7dy+3bt7l9+zaFCxe2KNPjd2j79evH8uXLGTRo0FPrMWXKFOrVs77s8iMqlQqVSpX3nfMUn376KUFBQUybNo2OHTuaH1Tdtm1bi+GKq1atonv37jg45P8cjbi9e7F3c6XIgI+x9/Ag7dYtrn32uXm4icLXF4zZ+1aqUFDkkwEo/f0xaDQkHD9O8OjRGFKzLzidypWlwvz55tfF/qnbw63buD02/55tlXBwP3aubvj16Ye9uwfpt4MJHvYVmQlZz5dRePtk95ABEb8vxWQy4d/3Y+ReXugTE0j66zjh/1wYAMicnPD/aAByL28yU5JJPHyI8IXzMD3HSqMvKmrDTuSebpT69nMUPp4kX77O2c79zQuHqAoXsqiPVKGg1KjPUQUGYEhLJ2bPES71/5rMx56dc23YBEqN/IJy00cj93JHG/WQB0v+5PbkOflen7xSV69A3f2/m1+Xm5b1nMMHyzdwqe+IgiqWhYyLp5A6uuDcqnPWg6ojQklYONU8hE7m6mkxXA5A5lUIefHSxP822WqeyvLVUL+X/Wwn1w+yFm1K3bOB1L3534BeuSEMlVLGsE9L4eRox+XrSQwdexWdPrsefr5K1C6Wc2VqVHbF11uZY/VGAIPBxPBxV/i4ZzEmjyyPSikjPFLDxJ9vcfJcQo74tqK/fg6NgxPKhm2ROmY9qDr1z18wpWf9L0hd3Cx6ZUwpCaSs+QWHZu+g6PsdxpREtGcOknEye5EJqaMzDm17IXVywaTNwPAwnNTVv5AZ8vReLFtIO3MMqbMatw7dkKnd0D64S/RP32P8Z+EQO3cvi/M0gL2vP8qg8kROy7nE+SNOtRsBElJPHcnH0ltKOLgfO7Urfr37/fOg6mCCvx5iPk/LvX0wPVaXyN+zhjH69e2P3NOLzMQEEk8cJ2Jh9vSGB7N+wq/PRxT5Yij2bm5ZD6reupnI5UvyvT7N69ciITmFhas3EZeYTKnAAH767ivc/xlGFx0bj/Sx7uINew6hz8zk22lzLfLp++7b9HuvPVKplNDwKHYcnkNScipqZ0fKlijG3PHfUDzg2YvCPK+3OvZCl5HB0jkTSU9LIahsFQaPnoW9XGGO8zAqjNTHFqmp3aAlKUkJbFo1j6SEOIoUC2LwmNmoXZ/vRvZHX45l1aIZzBz/JRKplNLlqzF49KznGtX1NM0a1CExOYWFqzZkPUS8WBGmjxr22LGJszg2m3YfQJ+ZycgfLRfU+bBLB/q+n/V4m2NnzjPxl+wFnsbMmJMjjiA8TmJ6HVctKCAajYbSpUuzZs2aV9ILZUuxsbGULl2as2fPUqzY8z1f568aNZ8d6V9E7vhK7zvku9jziQVdBJsxaP59i8I8TfUv/1v/O+/e+vjZkf4lttTJv/ldBSExOH9XgH3V4u9Zn+P7b1Rs1pSCLoJN3bAr2Mff2FopY+5TV/5tvMrXfnakApKx/qcC+2xl568K7LPzy3/rSvYlqVQqli9f/sxVGl9HISEhzJkz57kbZ4IgCIIgCILwUsQy+zYlGmhPaNy4cUEX4YXUqFHD6kO2BUEQBEEQBEH49xANNEEQBEEQBEEQXly+LbH7/0n0RwqCIAiCIAiCILwmRANNEARBEARBEAThNSGGOAqCIAiCIAiC8OLEIiE2JfamIAiCIAiCIAjCa0L0oAmCIAiCIAiC8OIkYpEQWxI9aIIgCIIgCIIgCK8J0YMmCIIgCIIgCMKLk4o+H1sSe1MQBEEQBEEQBOE1IRpogiAIgiAIgiAIrwkxxFEQBEEQBEEQhBcnFgmxKdGDJgiCIAiCIAiC8JoQPWiCIAiCIAiCILw48aBqmxJ7UxAEQRAEQRAE4TUhGmiCIAiCIAiCIAivCdFAEwRBEARBEAThxUmlBbe9gF9//ZXAwECUSiW1a9fm9OnTeUq3evVqJBIJHTp0eKHPzSvRQBMEQRAEQRAE4f/CmjVrGDx4MGPGjOHvv/+mcuXKtGrViocPHz41XUhICEOHDqVhw4b5XkbRQBMEQRAEQRAE4cVJJAW3PacZM2bw0Ucf8eGHH1KuXDnmzZuHg4MDixcvzjWNwWCge/fujB07luLFi7/MnsoTsYqjgE8574Iugk05eKkLugg25VfNVNBFsBmpnaygi2BT52aeKegi2JTnx2MKugg2k3wvoqCLYFPqL4cVdBFsyjhtckEXwWbWRzco6CLYVGfvowVdBJv6K7NuQRfBZtoWdAFeU1qtFq1WaxGmUChQKBQ54up0Os6dO8eIESPMYVKplObNm3PixIlcP2PcuHF4e3vTt29fjh7N//8R0YMmCIIgCIIgCMKLk0gLbJs0aRJqtdpimzRpktVixsbGYjAY8PHxsQj38fEhKirKappjx46xaNEiFixYYPPdlhvRgyYIgiAIgiAIwr/SiBEjGDx4sEWYtd6zF5GSkkKPHj1YsGABnp6eNskzL0QDTRAEQRAEQRCEf6XchjNa4+npiUwmIzo62iI8OjoaX1/fHPHv3LlDSEgI7dq1M4cZjUYA7OzsuHnzJiVKlHiJ0lsnhjgKgiAIgiAIgvDi/iWLhMjlcqpXr87+/fvNYUajkf3791O3bs75imXKlOHy5ctcuHDBvL399ts0adKECxcuEBAQ8NK7zhrRgyYIgiAIgiAIwv+FwYMH06tXL2rUqEGtWrWYOXMmaWlpfPjhhwD07NkTf39/Jk2ahFKppEKFChbpXV1dAXKE25JooAmCIAiCIAiC8OJe8IHRBeG9994jJiaG0aNHExUVRZUqVdi1a5d54ZD79+8jLeD6iAaaIAiCIAiCIAj/NwYNGsSgQYOsvnfo0KGnpl26dKntC/SEf09zVxAEQRAEQRAE4T9O9KAJgiAIgiAIgvDCTM+5WIfwdKIHTRAEQRAEQRAE4TUhetAEQRAEQRAEQXhxEtHnY0tibwqCIAiCIAiCILwmRA+aIAiCIAiCIAgvTvSg2ZTYm4IgCIIgCIIgCK8J0UATBEEQBEEQBEF4TYghjoIgCIIgCIIgvDCxzL5tiR40QRAEQRAEQRCE14ToQRMEQRAEQRAE4cWJRUJsSuxNQRAEQRAEQRCE14ToQXvFRo0aRXR0NL/99ptN89XpdAQFBbFu3Tpq1Khh07xz49KsDa5vdUamdkP34B6xv89De/eW1bh+IyahKlspR3jahTNEzfgeAJmLK+7vfYhDhapIHRzJuHmV2N/noY+OyM9qAOBQvwWOjdsic1ajj7hP8sZl6B/csRrX/ZORKEqWyxGece08CYt+BEBZsSYOdZthX7gYUkdnYqaPIDMiNF/r8DiH+i1watrOXJ+kDUvR37deH4+Bo3KtT/yCqSCV4fxWF5RlqyDz8MaUoUF76zLJ21ZjTE7I76oA4FCvOY5vtEHqrEYfeZ+UTcvRP7hrNa77gO+QlyibIzzj+gUSF08DQFGhRtbx8Q9E6uhM7E/fkhlxP1/r8LzcG9Sg+JC+qKtVQOnnzdnOnxK9ZX9BF8uqrm3caV5PjaNKyo27Gcxf85DIGP1T07irZfRs70m18o7I7SVExeqZ/Uc0d+5rzXEK+9jTo4Mn5UuqkEklPIjSMXVhJLEJmflWF6fGb6Ju2RGZ2hVdWAjxqxagCwm2GtdnyASUpSvkCE+/fJaY2RPMr+18C+PWuSfKoPIglaGPfEDMvCkY4mPzrR6PrNt1gBVbdhOfmETJogEM7tOV8qWKW427ed8Rdh4+wd0H4QCULl6UAV07WsRf+Odm9h4/w8O4eOzt7KzGyS//pd8cAJPJxPHts7h8fC1aTTJ+xavR4v3vcfMOzDXNqd3zuXVhD/HRd7GzV+JfvCqNOgzF3Sd7/6clxXB441RCbvyFTpuGu08x6rQaQFDVVvlan//Sd81kMrF73S+cPLAOTVoKxUpXpXOf0XgVKpprmjvXz3Jo22LC7l4jOTGG3oNnUbFms1zjr1s4lhP7/6R9j69p9FbP/KiG8C/2f9tA6927N8uWLWPSpEl888035vBNmzbRsWNHTCYTAAaDgVmzZrF48WKCg4NRqVTUqVOHkSNHUr9+/ef6zKioKH7++WcuX76cI3zSpEls376dsLAw1Go1JUuW5IMPPqBXr1689dZbHD58ONd833jjDQ4dOsTQoUP5+uuv2b8//y/iHGs3xLPbR8Qs/YWMOzdxbdWBQsPG82B4fwwpSTniR836AYmdvfm11MmZgAm/kHb6mDnM98uRmDINRM0cj1GTjrp1Rwp9/QMPvhmASafNkaetKKvUweXtD0hatxj9/ds4NnwT9/7fEDNlCMbU5BzxE5b+hMQu+19H6uCE55DJZFw6ZQ6TyBXo7t1Ec/Ekrl3651vZrVFWqYO6Qw8S1y5CH3obxzfexOPjb3g4yXp94pfMQCJ7rD6OzngNnYzmwkkAJHI58sLFSNm7EX14KFIHR9Qde+HebyixM77L//pUro1zu+4kr1+C7v5tHBu2xq3f18ROHYYxzcrxWTYzx/Hx+GoiWivHJ+PiKdTv9sv3OrwImaMDyZdu8mDpemqs+7Wgi5Orjs3daPOGK7N+jyY6Tk+3th6MHujP5xNC0WearKZxVEmZNDiAy8Eaxs8JJynVQCEvOWnpRnMcX097Jg4OYN9fSazeHo8mw0hAITl6vfU8bcGhRn3c3+1D3Iq56O7dwrnZ23h/MYaI0QMxWjmvxcydDI9912SOzhQaPZP0s3+Zw+y8fPEdPpHU4/tJ3LIKU4YGe78ATPqnN2BtYd/x08xa9ifD+39A+ZLFWbN9H1/9MJPVP0/AXe2SI/7fV2/SokEtKgaVQC63549NO/lywk+smDEObw83AAIK+TKkbzf8fbzQ6nSs3raXL8b/xNrZE3FTO+dbXf5LvzmPnN67gPOHfufNHpNRexbm2NafWfdLXz4ctQM7e4XVNA+CT1O1UXd8i1bEaDRwdMsM1s7uy4ejtiNXOACwY/nXaDXJdBwwF5WTG9fPbGXroi/54Ov1+ATkvBlnC/+l7xrAwa2LOLprBV0/mYi7lz+71s7mt8n9Gf7jFuzl1o+NTqvBr0hpajXuxNIZXzw1/8tn9hF6+yIubt75UfyCIRYJsan/6yGOSqWSKVOmkJBgvRfAZDLx/vvvM27cOL744guuX7/OoUOHCAgIoHHjxmzatOm5Pm/hwoXUq1ePokWz78DcvXuXqlWrsmfPHiZOnMj58+c5ceIEw4cPZ9u2bezbt48NGzYQGRlJZGQkp0+fBmDfvn3msA0bNgDQvXt3jh07xtWrV19shzwH19YdST60i5Sj+9BHPCBm6S+YtBk4v9HSanxjWiqGpATz5lChKiadltTTRwGw9/VDWbIsMct+RXsvGH1UOLHLfkUql+NU9418rYtjo7dIP3kQzZnDZEaHk7R+ESa9FlUt659r0qRhTEkyb/Kgipj0WjIuZjcANOeOkbp3I7pbV/K17NY4NW5D+okDaE7/U5+1izDpdDjUbmw1vindsj6KJ+pjytAQN28iGRdOYoiJRB96m6T1S5AHFEfm6pHv9XFo9Cbppw6iOXsEw8MIkjcseb7jU6oCJr2OjIunzXEy/j5O2r5N6IJf/fHJq5jdR7g1ZibRm/cVdFGeqm0TV9bujuf05TRCI3T8vDwad7WM2pUdc03TqYUbsQmZ/PJHNMGhWh7GZXLxRjpRsdmNlm7tPDh3NY3lm+O4F6YlKlbPmctpJKUa8q0uLi3ak3JsD2l/HUAfGUb8irmYdFqc6lu/C25MT8WYnGjelOWqYNJpST933BzHtUN3NFf+JnH9MvQP7pEZE4Xm4hmrDT5bW7VtL283a0jbJg0oFuDH8P4foJDL2XbgmNX4Y7/4iM6tmhBUrAiB/oUYMaA3RpOJs1eum+O0alibWpXK4e/jRfEAf77o9R5pGg2374fla13+S785kHV98ffB5dRp/QklKzfHy78Mb/WaSmrSQ25fzP1//p1Bi6hQtxOefqXwLlyGN3tMJiUhguj72b/7EXfPU/WNDygUWAlXzwDqvvkpCgcXizi29l/6rplMJo7s/J3mHT+mQo2m+BUtTddPJ5Gc8JArZ3O/AV62SkPefO8LKtZs/tT8k+Kj2bh0It0HTkUm+7/tJxGe4f+6gda8eXN8fX2ZNGmS1ff//PNP1q1bx/Lly+nXrx/FihWjcuXK/Pbbb7z99tv069ePtLQ0TCYTzZs3p1WrVuaet/j4eAoXLszo0aPN+a1evZp27dpZfMann36KnZ0dZ8+epUuXLpQtW5bixYvTvn17tm/fTrt27XB3d8fX1xdfX1+8vLwA8PDwMIe5u7sD4ObmRv369Vm9enV+7K5sMjsUgSVJv3ohO8xkQnPtAsqSZfKUhXOjlqSePGK+S/noTqdJr7PI06TXZw0Lyi8yGfaFi6F9/ELdZEJ76wryoqXylIVD7cZknD/5Su64PtOj+tx6oj7BV7B/jvpozp94an0kKgdMRiNGTfrLlvjpZDLs/YuhC37swsJkQhd8FfuiJfOUhapWYzIunMCkfw2Oz3+Mj4cd7mo7Lt7I/h6kZxgJDsmgdKAy13Q1Kzpy+34Gw/r4snRSMaZ/HUCLetl32SUSqFHekYiHekYP9GPppGJMGRpArUq5N/pemswOeZESZFy/lB1mMpFx/SKK4qXzlIVTg+aknTmW/b8jkaCqWIPM6Ai8vxhD4WlL8R0xFVWV2vlQAUt6fSY374ZSs1J2j4lUKqVmpbJcuWV9ePCTMnQ6MjMNuDhZ3+96fSab9h3ByUFFqaKFbVJuq/5Lvzn/SIoLIy05hqKl65nDFCpnCgVWJuLe+Tzno9WkAKB0VJvD/IpX5ebfO9GkJWIyGrlxdjuZei0BpWrZrgKP+U9914D4h2GkJMYSVKGOOUzl4EyREpUIDb74UnkbjUZW/voNjdt+iG9A3n7D/jWk0oLb/oP+m7XKI5lMxsSJE5k9ezZhYTnvyKxcuZKgoKAcjSqAIUOGEBcXx969e5FIJCxbtowzZ84wa9YsAAYMGIC/v7+5gRYfH8+1a9cs5ofFxcWxZ88eBg4ciKOj9ZOS5Dm7jGvVqsXRo0efK83zkjm7IJHJMCQnWoRnJiUiU7s9M72ieBCKgECSD+82h+kiw9DHPsTj3d5IHZxAZodrm3ew8/DCzvXZeb4oqaMzEpksx91sY2oSUmfXZ6a3DyiBfaEipJ86mE8lfD5Sx3+OzZP1SUlC5uL6zPT2RUpg71eE9JNPqY+dPS5tu6I5/xcmreYlS/x05uOTalkfQ2oSUmd1Lqmy2QcUx75QAJrTh/KphP/fXF2y7v4mpVj2aiWmGMzvWePjaU/rhmoiYvSM/TWCXceS6PuOF01qZw1bUjvJUCmldGrhxvlr6Xz/SzinLqbydb9ClC+pype6yJycrZ7XDClJeTqvyQNLIfcvSuqxveYwqbMaqVKFS+tOaK7+TfTMsaSfP4nXgK9R5HMjIDElFYPRmGN4mbvahbjEvPXezfljHV7urtSsaDks7ti5izT9YCBvdP+E1dv28vOowbi65N+Qs//Sb84jackxADi4WI5CcHD2IC05b3MTTUYjB9dPxL94Nbz8gszh7frOxGjI5Nfhtfnpi4rsWTWaDv1/wc079/lTL+O/9F0DSE7K2v/Oak+LcGe1B8mJLzdv9OCWRUhldjRs/cFL5SP89/3f96127NiRKlWqMGbMGBYtWmTx3q1btyhbNudiA4A5/NatrAnK/v7+zJ8/n549exIVFcWOHTs4f/48dv/MT7h//z4mkwk/Pz9zHrdv38ZkMlG6tOXdWU9PTzIyMgAYOHAgU6ZMyXN9/Pz8CA3NfTEKrVaLVmvZk6A1GFDIZHn+jJfl3Kgl2vv3LCd3GwxEzfoB775fUGzeGkwGA5qrF0i7eAYJr++4ZlXtxugj7ue6oMi/jcOj+uSyoAhSGe69vgCJhKS1i19t4V6AqlZj9JH3c11QRHg+jWo4M6Br9pyJH+a+2GIKEomEO/czWLE1DoB7YVqKFFLQqoGag6dSzKs1n76cxtaDiQCEhOsoXVxJqwZqrt7O3xsDL8KpQXN0YSEWC4o8usGmuXCalH1bAdCH3UNRogzOjVqhvZX/w9Ff1PKNO9h7/DRzxg5DIbe3eK96+TIs+3E0SSmpbN53lJEz5rNw0rdW5xq9Dl6H35xrp7ewd9UY8+tOn85/6Tz3rRlLbEQwXQevtAg/vu1nMtKTefezpaic3Lh9cR9bF33J+1+twMs/b73Br1JBf9fOHdvGuoXfm1/3Gz7XZnk/7sHdqxzd9TtfTVz33Dffhf8///cNNIApU6bQtGlThg4dmuO9R0MW8+Ldd99l48aNTJ48mblz51KqVPaQMo0m64JCqcx92M8jp0+fxmg00r179xyNqWdRqVSkp+c+7GzSpEmMHTvWIuyzSiX5onJQLilyMqQkYzIYcvTI2KldMSQ9fVU/iVyBU51GJGz4I8d7upDbhI36DKnKAezsMKYk4z9mBtp71ldQswVjWgomgyFHb4zUSY0xJfGpaSVyBaoqdUnZvS7fyve8jGn/HBtnNY8vQSB1Vue4+/wkiVyBqmo9UnattR5BKsOt1xfI3DyJnTMh33vP4LHj42R5fGRO6mfO4ZHYK1BWrkPqnvX5WcT/K6cvp3IrJMP82t4u6yJD7SwjITm7F83VWca9sNzPXQnJmTyI0lmEhUXpqFvFCYCUVAOZBhMPIrU54pQtnj89aIbUFKvnNZmzOk/nNceaDUjcvMpKnpnoIx9YhOsjw1CUtH7zz1ZcnZ2QSaXEJ1kupBOflIyH69N7n1ds2c3vm3Yya/QQShYNyPG+SqkgoJAPAYV8qBBUgnc/+5atB47Rq+NbNq3DI/+F35ySlZpSKLCy+bUhM+v7n54ch5M6+6ZHekoc3oWfPWxz35px3L1yiPe++gNnN19zeGLMfc4f/oPe323D0y/rGsS7cBnC7pzlwpEVtOg6zlZVMvu3f9fKV29C0ZIVza8z/1nAJyUpFhc3L3N4SlIc/oF5G1Jrzb0b50hNjmfCZ9lz1IxGA1v++JEjO39n5Oy9T0n9+jOJRqdN/V8PcXykUaNGtGrVihEjRliEBwUFcf36datpHoUHBWU3bNLT0zl37hwymYzgYMsTvKdnVlf54wuSlCxZEolEws2bNy3iFi9enJIlS6JSPf+FSHx8vHmemjUjRowgKSnJYhtQocTzfYghE23IbRzKV8kOk0hQlatCxu0bT03qVKshEjt7Uv7KfQidUZOOMSUZex8/FMVKkvb3yecr3/MwGLLuaJd6bLiRRIKiVHl0oU//kVZWro3Ezg7NOeuToAvEP/WRBz229Pc/9dHnsT7pZ63U55/GmZ2XL3Fzf8CUnmrjgufCYEAffg95ScvjIy9ZHn3o7acmVVaulXV8/j7+1HhC3mVoTUTF6s3bgygd8UmZVCrtYI6jUkopFajk5mMNuSfduJuBv7fcIszP256Y+KwLo0wD3A7NwN/nyThyYvJriX1DJrr7d1CWeWxpdokEZdlKaO/ezD0d4FC9PhI7e9JOPbHa7j/nSjtff4tgex8/DHExtiq5Vfb2WcuSn72c/RtmNBo5e/kGFYJyX6b8j807WbJuGz999yVlSwTm6bNMJhP6/FyV8j/wmyNXOuHmXdS8eRQqiaOLF6E3T5jjaDWpRIZcxK9Y1VzzMZlM7FszjtsX99Lli2W4elo2avS6rBtnkifm5Uilsue64fw8/u3fNaXKEU/foubNp3AJnF09Cb6SvfBXRnoq9+9comipyk/J6emqN3ybIVM2MnjyevPm4uZN43Yf0n+EbR+9JPz7iQbaPyZPnszWrVs5cSL7ZPn+++8THBzM1q1bc8SfPn06Hh4etGjRwhw2ZMgQpFIpO3fuZNasWRw4cMD8XokSJXBxceHatWvmsEfpf/nlF9LS0mxSjytXrlC1au4nd4VCgYuLi8X2IsMbE3dtxPmNVjg3aIa9XwCevQYiUShJOZJ1B8i7/2Dc3+2VI53zGy1I+/sExtSUHO851myAskxF7Lx8cahWh0LDJ5B27iSaK3mfMP0i0o7swKF2E1Q1GmLn7YdL5z5I5Eo0p7MuttRdP8H5rfdypHOo1ZiMK+esNlYkKkfs/Ipi55M1mdnOuxB2fkXzNG/qZaUe2o5jnSaoajbCztsP9Tt9kMgVpP9z8eja7ROc27yfI51DnSZkXD6bsz5SGW69v0QeUJyEP34BqTRrbo2zGl7B0Nj0IztxqN0YZfWGyLz9cOn0IRK5As2Zf47P+x/j9GaXHOlUNRuTcfVpx6cIMp+sC2c7r0LY+RV5Jccnr2SODrhULoNL5aw7tg7FCuNSuQzKgEIFXDJL2w4m8m5rd2pWdKSIn5wvevgQn2Tg1MXsc9rYz/x5s1H2vt16IIGgYko6t3TD19OehjWcaVlfzc4j2b2im/YlUL+aMy3queDrac+bjdTUrODIriOJ+VaX5L2bcW7YAse6TbDzLYx79wFI5EpSj2et3Obx4Re4dsw5d8SpQXPSL5zCmJbzvJa8eyOONerj1KAFdl6+ODd5C1WlmqQc3plv9Xika9sWbNl/hO2HjhMSFsHUBX+QodXStknWI2LGzl7EnBXZPcy/b9rJb6s3892nvSnk5UlcQhJxCUmka7Ia25oMLXNXbuDKrTtExsRx404IE+YsISY+gaZ18/f5m/+l3xzIGv5arUlPTu6ay+1L+4kJv8nO5cNxUntTsnJ2D8ufP/fi70PZvX/71ozl+pkttPlwOnKFI2lJMaQlxaDXZR0jd9/iuHoVZe/K0USGXCIx5j5n9i0m5MZxSlZ6+uqCL+O/9F2TSCQ0erMH+zbN58rZA0Tev8XKuSNwcfOmQo3sFV3nTujDsd0rzK+1GWmEh1wnPCSroRofE0Z4yHUSYrOGgjs6u1IooJTFJpPZ4aL2xNuvWL7W6ZWQSAtu+w8SQxz/UbFiRbp3725e5AOyGmhr166lV69e/PjjjzRr1ozk5GR+/fVXtmzZwtq1a82Le2zfvp3Fixdz4sQJqlWrxrBhw+jVqxeXLl3Czc0NqVRK8+bNOXbsGB06dDB/xpw5c6hfvz41atTg+++/p1KlSkilUs6cOcONGzeoXr36c9Xj6NGjjB8/3ib75GnSTh0lzlmNW6cPsFO7ob1/l8gfR5uH0dl5eOW4W2fv64+qdAUiplh/dpadqxue3fohU7uSmZhAyvH9JGzK5xUpgYwLJ0l2dMGp1TvIXFzRh4cSv2Cy+ZlhMlcPMBkt0si8CiEvXoa4+ROt5qmsUB3X9weYX7v1+ByAlN3r833IXcaFkyQ5ueDcOrs+cfMnmxfakLl5whPHRuZVCEXxMsTNzVkfmdoNVcWsH0TvYZbzIWN/GYfujvVeZlvJuHgKqaMLzq06Zz2oOiKUhIVTHzs+1usjL16a+N8mW81TWb4a6vc+Nr92/eAzAFL3bCB174Z8qsnzUVevQN39v5tfl5v2LQAPlm/gUt8RuSV75TbuS0CpkPBJV28cVVKu38lg/Jxwi2eg+Xra4+KU3Zi/fV/LlAWRfPC2B13edOdhXCaL18dw5Gz2RfSpS2nMX/2QTi3d6PuOFxEP9UxdGMn1u7n3zL2s9LPHSXBW4/p2V2QubujC7vFw1ljzcFo7d68c3zU7Hz+UpcoR/dMYa1miuXCKuBXzULfujNv7/ciMjiBm3hS0t/P3/wagef1aJCSnsnDNZuISkykVGMBP332J+z/DzqJj45A+Nixpw55D6DMz+Xa65Rycvu+2o1+X9kilUkLDI9lx6C+SUlJROztStkQx5o77muIBlr2EtvZf+s15pFaLj9DrNOxZORqtJhn/EtXpPHChxTPQEmMfoEnLHnlz8WjWMNo1M3tY5NX6g0lUqNsJmcyezp/+xpHN09k4bwA6bTpuXkV4s8dkilfIv8cH/Je+awBN2vVFp9WwbuH3aNJTKFa6Gv2/mW/xDLS46AekPTYV4sHdq8wd/6H59ZbfpwJQo1F7un5i/VpBEHIjMeVXn/drrnfv3iQmJlo8yywkJITSpUuj0+nMJ/rMzExmzpzJ0qVLCQ4ORqlUUrduXUaNGmV+UHVMTAwVK1bkiy++MA+T1Ov11K1blxIlSrBmzRoAdu7cyUcffcT9+/eRPjb8IDIykokTJ5ofVK1QKChXrhzvvvsun376KQ4ODhZlLFasGOfPn6dKlSoWdTpx4gRvvfUWERERzzU88k7PNs+17153Dl6vTy+ILZiM/51/Uandq1uM5lU4N/NMQRfBphZ+vK2gi2AzM3XDCroINuU8aEhBF8GmEqZZv3nyb3Sg5/aCLoJNdfbO35WgX7W/MusWdBFspm2117dfJfXklgL7bKc6bxfYZ+eX1/dI57OlS5fmCAsMDMyxKIednR1Dhw61uoDII15eXkRFRVmE2dvbc/bsWYuw1q1b4+fnx5o1a+jatas5vFChQsyePZvZs2c/s9yBgYG5jiOfOXMmw4YNe6G5a4IgCIIgCIIgFLz/5sDN15REIuG3334jM9P2k9x1Oh0VK1bkq6++snnegiAIgiAIgiC8Gv+3PWgFpUqVKjmGJtqCXC5n5MiRNs9XEARBEARBEJ5KLLNvU6IHTRAEQRAEQRAE4TUhetAEQRAEQRAEQXhhpv/ocvcFRexNQRAEQRAEQRCE14RooAmCIAiCIAiCILwmxBBHQRAEQRAEQRBenFgkxKZED5ogCIIgCIIgCMJrQvSgCYIgCIIgCILw4sQiITYl9qYgCIIgCIIgCMJrQvSgCYIgCIIgCILwwkxiDppNiR40QRAEQRAEQRCE14RooAmCIAiCIAiCILwmxBBHQRAEQRAEQRBenFgkxKbE3hQEQRAEQRAEQXhNiB40QRAEQRAEQRBemAmxSIgtiR40QRAEQRAEQRCE14RooAmCIAiCIAiCILwmxBBHQRAEQRAEQRBemEksEmJTooEm8Ll8SkEXwabSQlMKugg2JVcpC7oINqNJTi3oItiU58djCroINtVvftuCLoLNDB20q6CLYFO+BzwKugg2dT5heEEXwWbG+mQUdBFs6qyxdkEXwaaS02UFXQRBeG6igSYIgiAIgiAIwosTPWg2JfamIAiCIAiCIAjCa0L0oAmCIAiCIAiC8MJMErHMvi2JHjRBEARBEARBEITXhGigCYIgCIIgCIIgvCbEEEdBEARBEARBEF6YWGbftsTeFARBEARBEARBeE2IHjRBEARBEARBEF6cWCTEpkQPmiAIgiAIgiAIwmtCNNAEQRAEQRAEQRBeE2KIoyAIgiAIgiAIL0wsEmJbYm8KgiAIgiAIgiC8JkQPmiAIgiAIgiAIL8yEWCTElkQPmiAIgiAIgiAIwmtCNNAEQRAEQRAEQXhhJom0wLYX8euvvxIYGIhSqaR27dqcPn0617gLFiygYcOGuLm54ebmRvPmzZ8a3xZEA00QBEEQBEEQhP8La9asYfDgwYwZM4a///6bypUr06pVKx4+fGg1/qFDh+jatSsHDx7kxIkTBAQE0LJlS8LDw/OtjKKBJgiCIAiCIAjCv5JWqyU5Odli02q1ucafMWMGH330ER9++CHlypVj3rx5ODg4sHjxYqvxV6xYwaeffkqVKlUoU6YMCxcuxGg0sn///vyqkmigCYIgCIIgCILwEiSSAtsmTZqEWq222CZNmmS1mDqdjnPnztG8eXNzmFQqpXnz5pw4cSJPVU1PT0ev1+Pu7m6TXWeNWMVREARBEARBEIR/pREjRjB48GCLMIVCYTVubGwsBoMBHx8fi3AfHx9u3LiRp8/7+uuv8fPzs2jk2ZroQbOBHj16MHHixAItw65du6hSpQpGo7FAyyEIgiAIgiD8fzEhLbBNoVDg4uJiseXWQHtZkydPZvXq1WzcuBGlUpkvnwGvoAetd+/eLFu2DAB7e3uKFClCz549uXXrFitWrMg1XdGiRQkJCcFkMjFmzBgWLFhAYmIi9evXZ+7cuZQqVcoc99atWwwbNozjx4+j0+moVKkS48ePp0mTJrnmf+/ePb777jsOHTpEfHw8np6eVK9enSlTplCmTBkA3n77bS5cuMDDhw/Nq7ZMmTIFPz8/cz4XL15kx44dzJ071xzWuHFjDh8+zKpVq3j//ffN4TNnzmTmzJmEhISYwzQaDZMnT2bVqlWEhobi7OxMkyZN+P777ylfvnye93Pr1q0ZNWoUK1asoEePHnlOlx8+aO9Nq4ZuODrIuH47nV//iCDioS7X+IsnB+HjKc8Rvu1AHHNXRuZnUXP48L0A2jb3xsnBjis3k5nx2z3CozJyjb96TlV8vXP+g27cFcXPC++ZX5cLcqJf1yKULeWE0Wjidkg6wyZcR6fL3wZ1z06+vNnEEycHGVdvpTFr6QMionMfl718Rjl8vXKe1Lbsi+GXZWEAfPFhAFXLO+PhZo8mw8C14DQWrYngQWTu+dpC325FadfCFydHGZdvJDN97m3CInM/Nn/+VpNCPjmPzYYdEfw0/w4A7q72fNq7GDWquOGgkvEgXMPytfc5fCIu3+rxSNc27jSvp8ZRJeXG3Qzmr3lIZIz+qWnc1TJ6tvekWnlH5PYSomL1zP4jmjv3s/d9YR97enTwpHxJFTKphAdROqYujCQ2ITO/q/RU7g1qUHxIX9TVKqD08+Zs50+J3pJ/4/dfxnutXWlW1xlHpZQbIVoWrI0lKvbp+89dLaN7W3eqllWhsJcQFZvJr6tjuPsg57nvo3c9aFnPhSUb49hxJDm/qmH2Vh0F9Srao1JIuBdhYM2BDGIScz/3vFlHwVt1LM8D0fEGJixPM7+uV8GeGmXsKewlQ6WQMHxuMpr8PQUA0Ld7IO1a+uLsaMfl68lMmxNMWKQm1/hrF9a2fh7YHs6MebcB8PNVMqhPCSqWc0FuL+XU3/H8NP82CYlP/398XiaTiW1r5nB83wY06SkUL12Frv2/w7tQ0aemO7xzNXu3LCM5MZbCRYPo0vcbAktVNL8fE/WADcunc+fGBTL1OspVqU+Xvt/g4uphjnP/7nU2/TGT0NtXkUqlVKnTnM69hqJUOdi0fjv+/JW/9q9Hk5ZCsTJVeK/fqKfW7/a1s+zfspT7966RnBBDv6EzqVyrmfl9Q6aebatnc/X8UeIehqN0cKJ0xTq07/Ylandvm5U9t/oc2jybv4+sJSM9mYCS1WjTYwwePoG5pjm6fT43/t5LbORd7ORKAkpUpfm7Q/D0LQ6AJjWRg5tnc/fqcZLiI3FwdqdM1WY06fAFSgfnfK2PkMXT0xOZTEZ0dLRFeHR0NL6+vk9NO23aNCZPnsy+ffuoVKlSfhbz1fSgtW7dmsjISIKDgxkyZAjff/89pUqVIjIy0rwBLFmyxPz6zJkzAEydOpVZs2Yxb948Tp06haOjI61atSIjI/vCrG3btmRmZnLgwAHOnTtH5cqVadu2LVFRUVbLo9fradGiBUlJSWzYsIGbN2+yZs0aKlasSGJiojlekyZN+PPPP7l58ybr16/nzp07vPPOOxZ5zZ49m3fffRcnJyeLcKVSyciRI9Hrcz/Ba7VamjdvzuLFi5kwYQK3bt1ix44dZGZmUrt2bU6ePPlc+7l3797MmjXrudLY2jutPWnXzINf/4hg8MQ7ZGiNjP8qEHu73B9g+OWEO3ww+IZ5+256VsPm2Ln8v3B5XNcOfnR+y5cZv93lk28vo9Ea+XFUWeT2uZf9428u06nfWfM2ZOw1AIsL/HJBTkz9rixnLybyyTeXGfDNZTbujMJkNOVrfbq08aZDSy9mLXnA59/fJENrYNLwEtg/pT6fjbnFe4Mum7evJ2ddwBw5lWiOExySzvQFofT7+jrfTr2DRCJh0vCSSPPxGZXdOhWmcxs/ps0N5uNhF9BkGJn+fYWnHpv+Qy/QvtdJ8/bl6MsAHDwea47z3ZelCfB3YMQPV+n1+d8cPhHL2GFlKVXMMf8qA3Rs7kabN1yZv/ohX097gFZnZPRA/6f+nziqpEwaHECmEcbPCefzH0JZsiGWtPTsC21fT3smDg4gPErHqJ/D+WrSfdbuikevz9/vWl7IHB1IvnSTK5+PLeiiPFX7pmrebOTCb2vjGDEzAq3WyMgBvs88NuM/L4TBYGLib1F8NSWcZVviLY7NI7UqOhBUVEF84qtpMDevIeeNqnLW7M9g+uo0tHoTn3Z0wE729HQRsQa+/S3FvP30Z7rF+3J7CddDMtl75hW0yv7RvXMA77T1Z9qcYPoPPY8mw8CMcRWfeh74aPDfvN3jL/P25ciLABw8FgOAUiHlp3GVMJlMfPHdJT4ZfgE7OylTRlVAYuNz2t5NSzi0YxVd+49k2MQ/UChUzB7/CXpd7vvw7PFdrF82jTbvfsyIqavxDyzN7AmfkJKU9RujzUhn9vgBgIQvxixgyIRlZGbqmTv5M/OImsT4h8wa1x8v3wCGT/qDgSPnEPngDr//Osqm9du3eTGHd67kvY9GMWTiChQKFXN++Pip9dNqNfgHBtGl73dW39fpMnhw7zqtO3/M8Clr6DfkJx5GhDB/6mc2Lbs1x3cu5NS+32nT43v6ffcncoWKP2b0I1Ofe31Cb52hZpNu9P1uDT2GLMZoyOSP6f3QabP+f1ISH5Ka+JAWXYbzybitdOgzidtXjrJlqfX6C7Ynl8upXr26xQIfjxb8qFu3bq7ppk6dyvjx49m1axc1atTI93K+kgaaQqHA19eXokWL8sknn9C8eXN27dqFr6+veQNwdXU1v/by8sJkMjFz5kxGjhxJ+/btqVSpEsuXLyciIoJNmzYBWWNJg4OD+eabb6hUqRKlSpVi8uTJpKenc+XKFavluXr1Knfu3GHOnDnUqVOHokWLUr9+fSZMmECdOnXM8b766ivz+/Xq1eObb77h5MmT5kaXwWBg3bp1tGvXLsdndO3alcTERBYsWJDrfpk5cyYnTpxg27ZtdOnShaJFi1KrVi3Wr19P2bJl6du3LyaTiYyMDMqXL0///v3Nae/cuYOzs7PFijPt2rXj7Nmz3LlzJ+8Hx8baN/dgzbaHnLyQQkiYlumLw3B3taNuVZdc0ySnGkhIzjRvNSs5E/FQy+WbabmmyQ/vtCnE7+vDOH4mgbuh6UyafRtPNzkNauU+CTQpOZP4RL15q1vdjfDIDC5czW5cDuodyIadUazcFEFImIYHERkcOhGHPjN/L5o7tvZm5ZZoTvydxL0HGUydH4qHqz31q6tzr09KJglJ2VvtKi6ER2u5dCPVHGfHwTgu30wjOlbH7VANS9dF4O0px8crZy+orXRp58/ytfc5djqeO6Hp/DDzJh7uChrW8cw1TWKy3uLY1KvhTlikhgtXksxxKpRxYcP2CK4HpxIZncHytQ9ITcukdEmnXPO1hbZNXFm7O57Tl9MIjdDx8/Jo3NUyalfOvWHYqYUbsQmZ/PJHNMGhWh7GZXLxRjpRsdk3gbq18+Dc1TSWb47jXpiWqFg9Zy6nkZRqyNf65EXM7iPcGjOT6M37CrooT9XmDRfW70nk7JV07kfq+WVlDG4uMmpWzL2noUMzNXGJBuasjuX2fR0P4zO5dFNDdJxlI8xdLaNPJw9+/iOGzHy+QfNI46pydp/ScvluJhGxRn7frUHtKKFSiacPoDGaICXdZN7SMizLe+i8jr1nddyLenXfrXff9mf5n6EcOxXHnZA0Jvx04/nPAzU9CIvQcP6f80DFcmp8vZX8MPMmd0PTuBuaxg8/3aBMSWeqV3K1WdlNJhMHtq+gdeePqFyrCYUDg+j12QSSEmK4ePpArukObP2d+s07UbdpBwoFlKBr/5HIFUr+OrAJgDs3LhAXE0HPQePxL1oK/6Kl6DVoPPfvXOPWlaznNF05dwSZzI73+n2Lj38ggSUr0LX/SM6f3MfDyPs2q9+hHX/QqlN/KtVsin/R0vQYNJGkhBguncm9fuWrNqTt+59b9Jo9TuXgzKBRC6hWrzU+fsUoFlSZd/t8y4O714iPzb8RNiaTiVP7ltOo7QDKVG2GT0BpOvSdQkriQ278nfs57IOvFlKlQSe8/UvhG1CG9n0nkRQfQWTIVQC8CwfRZeBsSldpirt3EYqVrUPTjl9x6+JBjIaCHeXwskwSSYFtz2vw4MEsWLCAZcuWcf36dT755BPS0tL48MMPAejZsycjRowwx58yZQqjRo1i8eLFBAYGEhUVRVRUFKmpqbl9xEsrkDloKpUKnS73IW+P3Lt3j6ioKItJeGq1mtq1a5tXWvHw8KB06dIsX76ctLQ0MjMzmT9/Pt7e3lSvXt1qvl5eXkilUtatW4fBkLcfl/j4eFasWEG9evWwt7cH4NKlSyQlJVltSbu4uPDdd98xbtw40tKsNzRWrlxJixYtqFy5skW4VCrlq6++4tq1a1y8eBGlUsmKFStYtmwZmzdvxmAw8MEHH9CiRQv69OljTlekSBF8fHw4evRonupka76e9ri72nPhenZ90zVGbt7VUKaEKk952MkkNKnjyt5jiflUSusKeSvwcJNz7lL2xXtauoFrwamUC8rbsAM7OwktGnmy42D2czRcXewoF+RMQpKeX36owIaF1Zk5tjwVy+TvUAZfLzkervb8fSXFHJauMXLjbhplS+atd8hOJqFZfXd2H859uJ9SIaVVIw8iH2qJibPtcKBHCvko8XCXc/ZiojksLd3A9VsplC+d92PTsrE3O/ZZDmm4ciOZpg08cXayQyKBZg29kMulnL+clEtOL8/Hww53tR0Xb2T3SKRnGAkOyaB0YO7j2WtWdOT2/QyG9fFl6aRiTP86gBb1sm98SCRQo7wjEQ/1jB7ox9JJxZgyNIBalfK3N/C/xNvDDjcXOy7fyh6hkZ5h4naoltKBuc9nqFHegTsPtAzu5c3CcUWYOsSPZnUsv5sSCXzW3YstB5MIi8qf/5UnebhIUDtKufkg+8IvQwchUQaKFXp6F5qXq5QJ/ZwY86ETPVurcHPOxy7yPPDzUeLpruDMhQRzWFq6gWu3kqlQJvcbgI+zs5PQsokP2/dlj66R20kxAXp9dm+nTmfEaIJK5XK/mfW84h6Gk5wYS5lKtc1hKkdnAktV5O6tS1bTZOr13L97ndKVsm8cS6VSylSsw72bWWkyM3VIkGBnn32DzE6uQCKRcvv6eQD0eh0yO3uk0uxLPnt51vf5zo3zNqpfGMmJsRZlVTk4E1iyIvduXbTJZzyiSU9BIpGgyschgYmxYaQmxVC8XD1zmNLBmcLFK/HgzoU856NNz/oNVjnm/l3SalJQKJ2QysS6fa/Ke++9x7Rp0xg9ejRVqlThwoUL7Nq1y7xwyP37982j+wDmzp2LTqfjnXfeoVChQuZt2rRp+VbGV9pAM5lM7Nu3j927d9O0adNnxn80RNHaSiuP3pNIJOzbt4/z58/j7OyMUqlkxowZ7Nq1Czc3N6v5+vv7M2vWLEaPHo2bmxtNmzZl/Pjx3L17N0fcr7/+GkdHRzw8PLh//z6bN282vxcaGopMJsPb2/o46E8//dRcHmtu3bpF2bJlrb73KPzWrVsAVKlShQkTJtCvXz++/PJLQkNDrfbO+fn5ERoaajVPsP6sCIPh2Y3lvHBTZ51cEpIt7wIlJmfiprbPUx51qjrj5CBj3/GEZ0e2IXe3rPLFPzHnICFJh7tr3sreoKY7To527Hqsgeb3z9yH3l0Ks21fNMN/uE7wvVSmjymHv2/+TS59VObEpCfrk/djUa+6GicHGXuO5mygtWvmyeYFldiysDI1K7nwzZTbZBryp0fA459jk5Bo+T2NT9Th7pa3XruGtT1wcrRjxwHLBtqYH69jZydlx4q6HFhXn6GflOS7SdeeOu/wZbm6ZP2fJKVY3hxKTDGY37PGx9Oe1g3VRMToGftrBLuOJdH3HS+a1M66SFE7yVAppXRq4cb5a+l8/0s4py6m8nW/QpQvmbcbJP/vXJ2zGi2JT/Q4JqYazO9Z4+1hR8t6zkTG6JkwP4o9f6XQp6M7b9TM7olt31SNwcgrmXP2iItj1k98Sprl/2ZKusn8njWhUQb+2KNhzqZ01hzQ4OEi4ct3HVHk7dSRLx79rz85LyzhOc4Djep4Zp0H9mc30K7eTCYjw8AnvYujUEhRKqQM7FMCO5kED3fbjQpISsgaWv34vDAAF7UHyYmx1pKQmpKA0WjARW2Zxtk1O02xUpWQK1Vs+mMmOq0GbUY6G5ZPx2g0kJyYNYyzdMVaJCfGsXfzUjL1etJTk9m84meLcr2s5MSs3wnnJ8v6lPq9CL1Oy5YVP1G9/puoHPJvpENqUta+c3SxrI+jiydpyXmrj8loZNfqiQSUrIZ34SCrcdJTEjiydS7V3ujycgV+DZgk0gLbXsSgQYMIDQ1Fq9Vy6tQpatfOvnly6NAhli5dan79aE2MJ7fvv//+Jfda7l5Jc33btm04OTmh1+sxGo1069bNZpUymUwMHDgQb29vjh49ikqlYuHChbRr144zZ85QqFAhq+kGDhxIz549OXToECdPnmTt2rVMnDiRLVu20KJFC3O8YcOG0bdvX0JDQxk7diw9e/Zk27ZtSCQSNBoNCoUCSS7dqwqFgnHjxvHZZ5/xySef5Fr+vBoyZAibNm3il19+YefOnXh4eOSIo1KpSE9Pt5I6y6RJkxg71nIOSMmqnxBU7dM8l+ORxrXVDOqRvWDK97NybxjmVcsGbpy9kkJ8Uv529Tdv6MmQ/sXNr7+ZlLelVZ/mrWbenDqfQFxC9gXEo/PG1r3R7DqYdcK/fS+UahXVvNXUmwUrbTO8pGk9N774MMD8euT0nDcbnlfrNzw4cynZ6lyZ/X/Fc+5KCh6udrzzlg8jBxXjy/G3bDLXqcUbXgz9JHsRoK/HX33pPNu28OXUuXji4i0bef26BeLkKOPLUZdJTNbTsLYHY4eVZdC3F7kbmvv/0fNoVMOZAV2zb+L8MDfihfKRSCTcuZ/Biq1ZF0L3wrQUKaSgVQM1B0+lmL9rpy+nsfVgIgAh4TpKF1fSqoGaq7dzX0jh/1WDao583CV7eNykBdFPiZ07qUTCnQdaVu3IurEUEq4jwNeelvWcOXwmleKF5bRp5MLw6S927POqRmk73m+W3Rift/nFvsPXQrL/5yNiITQqnbF9nKkaZM/Jq6+m96/FG94MG5h9UTt83OWXzrONlfNAYrKeUVOuMfSTUrzTzh+jCfYdecjN2ym8zKLIp49sZ9Vv482vPxnxy8sUPVfOanf6Df6R1Qt+4NCOlUgkUmo0aE1A8bJI/jkp+AWUpNeg8axfNo3NK2YhlUpp/FY3XFw9kL7gRLszR7ex+rdx5tcDRvxqk/o8jSFTz+KfhmICuvSz7fy5Sye3sm35GPPrbl/Me+k8t68Yx8PwYPp8s9Lq+1pNKit//hgvvxI0fnvQS3+e8N/yShpoTZo0Ye7cucjlcvz8/LCzy9vHPpqbFh0dbdHQio6OpkqVKgAcOHCAbdu2kZCQgItL1jCHOXPmsHfvXpYtW8Y333yTa/7Ozs60a9eOdu3aMWHCBFq1asWECRMsGmienp54enoSFBRE2bJlCQgI4OTJk9StWxdPT0/S09PR6XTI5dbvtH3wwQdMmzaNCRMmEBgYaPFeUFAQ169ft5ruUXhQUPYP1MOHD7l16xYymYzg4GBat26dI118fDxeXl651tnasyK6fHE71/hPc+pCCjfvZc93ezSJ3s3FjoTHGliuLnbcffDsi0Mvd3uqlHNi4hzbNFqe5viZeK4HZ48dflR2d1d7i140N7Wc2yHPngvn4ymnekU1o6fdtAh/1FgLfaL+oWEavG04Z+vE30ncuJ1dTnv7rB9mV7W9RWPXTW3HndBnHwtvD3uqVnBm3M/3rL6frjGSrtESEa3l+u17bJhfkfrVXTl08uV7Po+djufazb/Nrx/Vxc1VbtH4dXeVE3zv2eO/fbwUVK/kysjJ1yzC/XyVdG7rR49B5wh5kHUheyckjcrlXej4lh/T577Y/8WTTl9O5VZIdo/co++a2llGQnJ2T42rs4x7YblPPk9IzuRBlGUDMyxKR90qWXeRU1INZBpMOVbTDIvSUba46EGz5uzVdG5PCze/tvvn2Lg6yUh8/Ng4yQiJyH2kQUKygbBoy4ZLeLSeOv8MLy1TXImLk4y5o7NvoshkEnq1d6fNGy4MHB9mk/pcvptJSFT2/4SdLKs+zo4SktOzb544O0gIj8n73DGNFh4mGPFyfXWDbo6djuParbPm13LzecCeuITsY+HmKuf23bydB2pUduO7STlv+Jw5n8B7/U+jdrHDYDCRmmZg8/K6REQ9tJJT3lSq2dhipcXMzKwyJyfGoXbL/o1OToqjcGBpq3k4ObshlcpITrIcxZCSGIeLa/aNhXJV6jHu1+2kJicglclwcHThm35N8fQpbI5Ts+Fb1Gz4FsmJccgVKiQS2L/td4s4z6NijSYElspexS5Tn1W/lCTL+qUkxeEfWOaFPuNxjxpn8bERfD56kc17z0pXbkLhMY/V55/jlZYch7Nr9g22tORYfAKsj3x63I4V4wi+eIjeX/+Bi3vOlQG1mlT++KkfcqUj7w36BZldAXZP24iJgh0G/V/zShpojo6OlCxZ8rnTFStWDF9fX/bv329ukCUnJ3Pq1Clzj9Sj3qLHx1Y/ev08zwSTSCSUKVOGv/76K9c4j/LTarMugB6V6dq1a+a/nySVSpk0aRKdOnXK0Yv2/vvv891333Hx4kWLeWhGo5GffvqJcuXKWYT36dOHihUr0rdvXz766COaN29uMUQyIyODO3fuULVq1VzroFAocjwbQiZ7sYaCRmtE8/DJYWd6Kpd15O6DrAtSlVJK6eIqdhyKf2Z+LRq4kZScyelLKc+M+7I0GcYcw9jiEnRUq6jmdkjWd8pBJaNcKSe27LG+Gujj3mzqTWKynpPnLBsoUQ+1xMTpCPC3vEAO8FNx6rzthnFqMoxoMiyPRVyinqrlnbl7P6tB5qCUUqa4I9v2P3t4RqtGHiQmZ3LqwrPnYmXdgJU8dXXI56HRGAjXWF48xsXrqF7Jldv3shqhDioZZYOc2bTr2ZPE32rmQ2KSnhNnLb+DSkXWOePJXmyjEZuuSJmhNRGltbx4j0/KpFJpB0LCs46ZSimlVKCSXcdy39837mbg7235v+rnbU9MfFbemQa4HZqBv8+TceTEFPAS+6+rrGNjuW8SkjOpEKQ0N8hUCgkliyrY/Vfu56Wb9zLw87a8wCrkbW/e70fOpnL5luWNkZEf+3LkXCoHT9lukrlWD9qkx7/PJpLSjJQOsCM8Jqs+SjkE+so4dinvQ9vl9uDpKuXMjVe3Gqi180BsvJYald0szgPlglzYtOPZPZNtmvuSkKTjxJnc59Qm/TM8v1olV9zU9hw7/eKP21CqHFGqsud/mkwmXFw9uXn5FAHFshosmvRUQoIv06jlu1bzsLO3p0jxsty8fIoqtbKmhRiNRm5ePsUbb76fI76TS9a0jpuXT5GSFE+lGo1zxHk0xPKv/Ruxt5dTpnKdHHFetn6FAx+r3+3LNGj53gt9xiOPGmcxUff5bMwiHJ1dXyo/axQqJxSq7EafyWTCSe3F3esn8C2SdZ2l1aQSdvcSNRp3zTUfk8nEzpXjufH3PnoNX46bV84GsFaTyh8z+iKzl9P1sznY2efP87qEf7fX+kHVEomEL7/8kgkTJrBlyxYuX75Mz5498fPzo0OHDgDUrVsXNzc3evXqxcWLF83PRLt37x5t2rQx51WmTBk2btwIwIULF2jfvj3r1q3j2rVr3L59m0WLFrF48WLat28PwKlTp/jll1+4cOECoaGhHDhwgK5du1KiRAnzMpxeXl5Uq1aNY8eOPbUebdq0oXbt2syfP98i/KuvvqJWrVq0a9eOtWvXcv/+fc6cOUPnzp25fv06ixYtMg+f/PXXXzlx4gTLli2je/fudOjQge7du1sstnLy5EkUCsVTlwnNb5v3xfF+G29qV3amqL+CIX0LE5+YyYnz2fMufhgSSNsmlisjSiTQor4r+08kvtSwkpexbnskPToXpl4NN4oVceDbz0oSm6Dj2OnsC/vpY8rRsbXl3TCJBFo38Wb3oRgMVsq+Zks4nd705Y067vj7KunzfgBF/FTs2P/id2fzYuOuh3Rr70Odqi4EFlYyfEBR4hL1HD+X3QiY8k1J3m5uuQKaRAItG3mw92h8jmPh6yXn/XY+lApU4eVhT7lSjoz8rBg6nZEzF/Nvbs2fW8Pp1SWA+rXcKV7UgZFfBhEXr+XoyezG5sxxFen0luWQZokkq4G282B0jmMTGqbhQYSGoZ+WomwpJ/x8lbzX3p8alV05eip/n4O27WAi77Z2p2ZFR4r4yfmihw/xSQZOXczuBR37mT9vNsqeWL71QAJBxZR0bumGr6c9DWs407K+mp1Hso/npn0J1K/mTIt6Lvh62vNmIzU1Kziy60hivtYnL2SODrhULoNL5ayLN4dihXGpXAZlgPVh6AVl++FkOrdwpUZ5B4oUsmdQdy8Skg2cuZw9XHD0J760bpC9QMG2w0mUKqqgY3M1vp52NKjmSPM6zuw6lvU/kZpu5EGU3mLLNJpISDYQ8Yxn372sQ+d1tKqloEJxOwp5SOnRSkVSmolLd7IbpoM6OdCocnYDs0NDBSX9Zbi7SChWSMZHbR0wGk2cu5ldVmcHCf5eUrzU/wyj85Dh7yXFIR+vNdduCafXe0WoX8uD4kUdGTm4TM7zwIRKdGrjZ5FOIoG3mvuy60DO8wBknSPKl3bGz1dJy8bejP+6HH9uDuNBuO2GBUskEpq26c7O9Qu4dOYQ4aHBLJs9ErWbF5VrZc/J//n7jzi0c5X5ddN2PTi+bwMnD20hMuwuqxdMQKvVULdJB3OcEwc2ce/WJWKiHnDqyDYWTh9G07Yf4OMfaI5zaOcq7t+9TnRECId3rmbNosm07/Y5Do55W2AlL/Vr/NYH7N4wn8tnDxJx/xa///ItajcvKtXMrt/scf04vCt7yJ82I52wkBuEhWRNM4h7GE5YyA3zCo2GTD2LZgzm/t2r9PxsMiajkeTEWJITY8nMzL//HYlEQu3mPTm6bR43LxwgOuwmGxd+jbOrN2WqZS9ct/zH3pze/4f59Y4/xnHpxFY69Z+GQulIalIMqUkx6HVZN4S1mlR+n9EXnU7D271/QJuRao5jNBb8arvC6+O1XzJm+PDhpKWl0b9/fxITE2nQoAG7du0yP73b09OTXbt28d1339G0aVP0ej3ly5dn8+bNFr1PN2/eJCkp60KmcOHCBAYGMnbsWEJCQpBIJObXX331FQAODg5s2LCBMWPGkJaWRqFChWjdujUjR4606IHq168fy5cvZ9Cgp48fnjJlCvXq1bMIUyqVHDhwgIkTJ/Ltt99aPKj65MmTVKhQAYAbN24wbNgwFi1aREBA1hCZOXPmUKlSJUaNGsWUKVMAWLVqFd27d8fBwXYPnnxe63bFolRI+aynH44OMq4FpzNqZojFkvKFvOS4OFt+9aqUdcLbQ86eY692cZDHrdoUgVIhY+jHxXFytOPyjWSGT7iO7rF5Vf4+CtRPLORQvZIaXy8FOw5Yb3Ct2x6F3F7KwN6BODvZcSc0naHjrz31gdG28Of2hygVUr7sUwQnBxlXbqXx7Y93LOaJFfKWo37iWFQr74yPp5zdR3I2UnR6IxVKO9KxlRdOjjISkzK5fDOVL8fdIjE5/3ppVm4IQ6WUMezTUlnH5noSQ8detTg2fr5K1C6WvRg1Krvi663MsXojgMFgYvi4K3zcsxiTR5ZHpZQRHqlh4s+3cvSE2trGfQkoFRI+6eqNo0rK9TsZjJ8TbvF/4utpj4tT9sIUt+9rmbIgkg/e9qDLm+48jMtk8foYjpzN7tk5dSmN+asf0qmlG33f8SLioZ6pCyO5fjf/Fj3JK3X1CtTd/7v5dblp3wLwYPkGLvUdkVuyV27zgSSUcgkfd/HAQSXlxj0tP8yPsjg2Pp52ODtmH5s7D3T8uDia7m3ceaelKw/jM1m6KZ5jf7/aR4VYs++sDrmdhK7NlKgUEu5GGJizMZ3Mx64FPV2lOKqy79e6Oknp/aYKB6WEVI2JuxEGZqxJI1WTvQ8aVJJbPMz6yy5ZvSl/7NFw6lr+XDivWP8ApVLG8EFBWeeBa0kMGXPZ8hztq8L1yfNAFTd8vZVs32t9NESRwg583Ks4Lk52RD3MYPmf91mz2TbDTh/XosOHaLUaVs4fR3paCiXKVGXQyDnmFRUBYqLDSE1OzC57/dakJiewbfWcrAdVB5Zm0HdzLBYbiY4IYfPKWaSlJuHh5Ufrzv1o2raHxWeHBF9h+5q5aDPS8fEvRrePR1L7jZyPCHoZzdv3QafVsGr+2KwHcZepyqffzrOoX2z0A9Ieq9/9O1eZNTZ7NeqNy38EoNYbb9Nj4A8kxj/k8tlDAEwZbvkc2s/HLKZU+Zo2rcPj6r/ZD71Ow9Zlo8lIT6ZIqep88NUCix6v+Jj7pKdm/16cPZTVuF42tadFXu0/nEiVBp2IDL1K+N2sVS1nj2hpEeeLKftw9XyxIaevgxddrEOwTmJ6nlUqhBw0Gg2lS5dmzZo1BdpzFRsbS+nSpTl79izFihV7rrRt+ll/Xty/VVpi/g+RfJXkqvxb7fFV0yTn3zNDCoJnQM65Bf9m/ea3Legi2MzSQbsKugg25Vs056JQ/2bn99tmeffXwdiJtZ8d6V/EYPxvXWjHpvz753c90q3B6zvPK+Km9cdFvAp+pSs9O9K/zGvfg/a6U6lULF++nNhY2y0j+yJCQkKYM2fOczfOBEEQBEEQBOFlvMgDo4XciQaaDTRu3Ligi0CNGjWsPjBbEARBEARBEIR/j/9WP7YgCIIgCIIgCMK/mOhBEwRBEARBEAThhYnnoNmW6EETBEEQBEEQBEF4TYgeNEEQBEEQBEEQXphYZt+2xN4UBEEQBEEQBEF4TYgeNEEQBEEQBEEQXpiYg2ZbogdNEARBEARBEAThNSEaaIIgCIIgCIIgCK8JMcRREARBEARBEIQXJhYJsS2xNwVBEARBEARBEF4TogdNEARBEARBEIQXJhYJsS3RgyYIgiAIgiAIgvCaEA00QRAEQRAEQRCE14QY4igIgiAIgiAIwgsTi4TYltibgiAIgiAIgiAIrwnRgyYIgiAIgiAIwgsTi4TYluhBEwRBEARBEARBeE2IBpogCIIgCIIgCMJrQgxxFFjqO72gi2BT9qUcC7oINiW1/+/8mxq0uoIugk0l34so6CLY1NBBuwq6CDbT+5fWBV0Em6p0bWNBF8Gm7MMXFHQRbOZgcqOCLoJN+blqCroINhUZa1/QRfi/YJKIIY62JHrQBEEQBEEQBEEQXhP/nVvzgiAIgiAIgiC8ciaT6EGzJdGDJgiCIAiCIAiC8JoQPWiCIAiCIAiCILwwk+jzsSmxNwVBEARBEARBEF4TooEmCIIgCIIgCILwmhBDHAVBEARBEARBeGEmxCIhtiR60ARBEARBEARBEF4TogdNEARBEARBEIQXJnrQbEv0oAmCIAiCIAiCILwmRANNEARBEARBEAThNSGGOAqCIAiCIAiC8MLEEEfbEj1ogiAIgiAIgiAIrwnRgyYIgiAIgiAIwgsTPWi2JXrQBEEQBEEQBEEQXhOiB00QBEEQBEEQhBdmMokeNFsSPWiCIAiCIAiCIAivCdFAEwRBEARBEARBeE2IBtorFhcXh7e3NyEhITbPe968ebRr187m+QqCIAiCIAhCbkxICmz7L/q/nYPWu3dvli1bxqRJk/jmm2/M4Zs2baJjx46YTCYADAYDs2bNYvHixQQHB6NSqahTpw4jR46kfv365nQbNmxg7ty5XLhwAa1WS/ny5fn+++9p1aqVxef+8MMPtG/fnsDAQIvw9evX8+uvv3L+/HkyMjIoUqQI9evX57PPPiMpKYkmTZo8tT4HDx6kT58+jB8/nqNHj9KwYcOX3EPPpqzdFIcGbyJ1UpMZdZ/UbSvIDL+Xa3yJUoVj887Iy1dHqnLEkBhH2o5V6G5dAsB9yI/I3DxzpNOc3E/qtj/yrR4A8qqNUNZuhsTRBcPDcDT71mKIDM01vkShQtmoHfZBlZEoHTAmJ6DZv47Mu9ey8qvSAEXVhkjV7gAYYqPI+Gun+f2CYF+5AYoaTZE4OmOMiUBzcD3GqPu5J1CoUNZ/C7uSlZAoHTGmxKM9tJHMe9dfXaEfFaVaIxS1WyB1csHwMIz0PX8++/i88Tby0lX+OT7xpO9bR+adqwDIqzZEUa0RMvPxiURzbMcrOT5Ojd9E3bIjMrUrurAQ4lctQBcSbDWuz5AJKEtXyBGefvksMbMnmF/b+RbGrXNPlEHlQSpDH/mAmHlTMMTH5ls9Hvdea1ea1XXGUSnlRoiWBWtjiYrNfGoad7WM7m3dqVpWhcJeQlRsJr+ujuHuA12OuB+960HLei4s2RjHjiPJ+VWNPHNvUIPiQ/qirlYBpZ83Zzt/SvSW/QVdrBw2b9vBnxs2EZ+QSIligQz6uB9lSgdZjRsSep+lK1YRfPsO0Q9j+OSjPnRun/sNv1Vr17No2R90erstn/bvm19VeCaHes1xfKMNUmc1+sj7pGxajv7BXatx3Qd8h7xE2RzhGdcvkLh4Wn4XNQeTycTBTbP5+8haMtKTCShZjbY9x+DhE5hrmqPb53P93F5iI+9iJ1cSULIqLd4Zgmeh4uY4Zw+t4fKpbUSGXkOXkcbXv5xG5eBi87JvWT2Xo3s3oklPoUSZynTv/y0+fkWfmu7gzjXs2bSMpMQ4CgcG0bXf1xQrlXWOi30YwbcD2lhN13/oVGrUa5H1d6eqOd7vN3gStRq0fslaWTKZTJzbO5vrp9ei0yTjG1iNBh3HoPYMzDXNtROruHZyFSkJ4QC4+ZSkWrOBFCnTyBxn6/weRN49Y5GubO33aNhprE3LL/y7/d820ACUSiVTpkzh448/xs3NLcf7JpOJ999/n3379vHjjz/SrFkzkpOT+fXXX2ncuDFr166lQ4cOABw5coQWLVowceJEXF1dWbJkCe3atePUqVNUrZp1MklPT2fRokXs3r3b4nO+/vprpk+fzueff87YsWMpWrQoMTEx7Ny5kxEjRrBlyxYiIyPN8b/44guSk5NZsmSJOczd3R25XE63bt2YNWtWvjfQFBVq4fTm+6RsWU7mg7uo6rVA3XsI8TNHYEpLyZlAJkPdexjGtGSSV/2KMTkBmasnxox0c5SEueNAmn0nxM6nMK4fDkN79UzO/GzIvkw1VE07otmzhsyIEBQ1muDYZSApC8ZhSk/NmUAqw/G9QZjSU0jbtAhTSiIStTumDI05ijElEc3hzRgTYgAJ8gq1cezUn5SlkzHGRuVrfayxC6qK8o0OZOzPatjIq72BY6cBpC6ZiEmTSx07f4IxPQXNtqUYU5OQurhZ1PFVsS9bHVWzzqTvWkVmRAjKmk1xeu8zkn/7Ptfj49T1c0xpKaRuWIApNRGpiwcmbfZ3zZSSiObQJozxD0EiQV6hDk7vDCB58SSMsZE587QRhxr1cX+3D3Er5qK7dwvnZm/j/cUYIkYPxJiSlCN+zNzJYJd9mpY5OlNo9EzSz/5lDrPz8sV3+ERSj+8nccsqTBka7P0CMOn1+VaPx7VvqubNRi78sjKWh3F63n/TjZEDfPlqcjj6TJPVNI4qKeM/L8TV4Awm/hZFcqoRXy970tKNOeLWquhAUFEF8YlPb/C9SjJHB5Iv3eTB0vXUWPdrQRfHqoNHjjFv4RK+GDiAsqWDWL95K9+MHseS+b/g5uqaI36GVkshXx/eqF+PuQuX5MzwMTduBbN91x6KP3Gj8VVTVq6Nc7vuJK9fgu7+bRwbtsat39fETs36rXlSwrKZSB77f5I6OOHx1US0l069ymKbHd+5kFP7fqdjv8m4ehbm4Maf+X16Pwb+sB17e4XVNCE3z1CzaTf8i1XEaDCwf8NP/D6jHwMnbEOucABAr8ugZIWGlKzQkP3rZ+RL2XdvXMqB7av48PNxeHr7s3nVHH4eP5CxP6/HXm697GeO7Wbtkul0//g7igVVYP+2lfw87lPGzd6Ei6s77h4+/Lhor0Wao3vXs3vTcipUrW8R3nvQWMpXrWd+7eDobPM6Xjy8kCvHf6dxl8k4uxfm7J6f2bGoH+8O3o5dLsfHUe1DrTeHoPYsislk4ta5TexZPpBOn2/A3beUOV6ZWu9So+Xn5td29iqbl/9V+6/2ZBWU/+sGWvPmzbl9+zaTJk1i6tSpOd7/888/WbduHVu2bLEYOvjbb78RFxdHv379aNGiBY6OjsycOdMi7cSJE9m8eTNbt241N9B27NiBQqGgTp065ngnT55k6tSp/Pzzz3z+efY/a5EiRahevTomkwmJRIKvr6/5PZVKhVartQh7pF27drRo0QKNRoNKlX//8Kr6Lck4ewTt38cASN2yHHnpyiirN0RzZEeO+MpqDZE6OJL42w9gNABgTIyziGNKt2zYyRtVxhAXjf7ezXyqRRZFzaboLv6F7vJJADS7V2NfojzyinXRntqbI768Ul0kSgdS/5gOxn8uKJPjLeJk3rli8Trj6FbkVRtg51cMXQE00BTVG6O/cgL91dNZ5dm3Frvi5bCvUBvdmZx3/u0r1EaidECzeqa5joYn6viqKGs1RXvxuPn4pO9ahbpkBeSV6qE9uSdHfHnlekiUDqQs/9FcdmOSZdn1ty9bvM44sgVFtYb/HJ/8a6C5tGhPyrE9pP11AID4FXNRVayOU/1mJO/akCO+8YkGqGPNhph0WtLPHTeHuXbojubK3ySuX2YOy4x5dd+xNm+4sH5PImevZDWAf1kZw4JxRahZ0YG/zqdZTdOhmZq4RANzVmf38D2Mz9kAc1fL6NPJgwnzoxjxkU/+VOAFxOw+QszuIwVdjKdav2kLb7VqQesWzQD4cuAATp05x669++n6bucc8csElaJMUNYF5MJlv+ear0ajYdK0n/jqs09ZsXpt/hQ+jxwavUn6qYNozmYdi+QNS1CUrYKq1hukHdyaI75Jk8bjtwyUletg0uvIuHj6FZX4sbKYTJzcu5xG7QZQpmrWMerYbwo/flmfG3/vo2Jt6z1JPQYvtHjdoc8kfvyyHhEhVwksXROAui17AXDvRv40PE0mE/u2raTNOx9RpVbW6J4PPx/P0D7NOX/6YK49WXu3/kGDFp2o36w9AN0//o7L545y/MAm3uzUB6lMhvqJUTTnTx2kRv0WKFUOFuEqR+cccW3JZDJx+dhyqjYdQGD5rOPTpMsUfp9Qn5Cr+yhZxfrxKVquqcXrWq2/4vrJ1Ty8f9GigWZnr8LB2Svfyi/8+/1fz0GTyWRMnDiR2bNnExYWluP9lStXEhQUZHVe15AhQ4iLi2Pv3pwX8ABGo5GUlBTc3d3NYUePHqV69eoW8VatWoWTkxOffvqp1Xwkkue7I1GjRg0yMzM5dSof7wjKZNj5BaL7Z7gYACYT+jvXsA8oaTWJvExV9Pfv4NTuAzy+mYnbZ+NxeKMN5FY/mQxl5bpk/H00HyrwGKkMmW8AmaGPNwJNZIbcxM6/mNUk9iUrYoi4h6rFe7gMmohzn29R1GmZe10kEuzLVkdiL3/qENB8I5Uh9SlMZuitxwJNZIbeQlYoUBl5cAABAABJREFU0GoSuxIVyIwMQdn0HZw+Ho9jz6+R12qeex3zi1SGzLcImfcsj48+5Eaux0deqiKZ4fdwaPk+6s8n49JvJMq6rfJ4fKwPjbIJmR3yIiXIuH4pO8xkIuP6RRTFS+cpC6cGzUk7cwyTTpsVIJGgqliDzOgIvL8YQ+FpS/EdMRVVldr5UIGcvD3scHOx4/KtDHNYeoaJ26FaSgdav8MMUKO8A3ceaBncy5uF44owdYgfzepY3gGXSOCz7l5sOZhEWNSr6Q38r9Dr9dy6fYdqVSqbw6RSKdWqVOLajZe74TVr7m/UrlmD6o/lXSBkMuz9i6ELtvwd0gVfxb6o9d+hJ6lqNSbjwglMem0+FTJ3CTFhpCbFULxcdi+Q0sGZwsUrEXbnQp7zydBk3dhUOaptXcRcxUaHk5wYS9nK2ecZB0dnipWqwN2bl6ymydTruX/nOmUrZaeRSqWUrVQ71zShd67x4N5NGjTrkOO9VQsm8VWvJkwc/gHH9m8yT0uxlZT4MDQpMfiXyj4+cpUz3gGVeHj/Qp7yMBoN3L6wHb0uHZ+iVSzeu31hK8vG1mHtjHac3jmdTN2rH50ivN7+r3vQADp27EiVKlUYM2YMixYtsnjv1q1blC2bc7w6YA6/deuW1fenTZtGamoqXbp0MYeFhobi5+eX4zOKFy+O3WPDLmbMmMHo0aPNr8PDw1Gr83bydXBwQK1WExpqfX6OVqtFq7X8MdJmGlDYyfKUP4DUwRmJTIYx1XIIiTE1CXvPnL16ADJ3L2SuZcm4dIKk5T8hc/fB6e0eILUj/eDmHPEVZashUTqQ8fdxK7nZjsTBCYlUhvGJYZnG9GTsPKzfsZe6eiBVB6G7doa0tXORunmhavkeyGRoj+/Mjufph3OPIVlD1HRa0jYuwBj36nvPJCpHJFJZjh5KU3oKMvdc6qj2QBpQCv2Nc6RvnI/U1Qtls3dAKkN3crfVNPnBfHzSLb9rprQUZLkeH0/sinqgu3qG1D9/RermjUOrrOOTcSy7d1fq5YdLz6FgZ49JpyV1w2/5enxkTln/N4bkRItwQ0oS9oUKPzO9PLAUcv+ixC37xRwmdVYjVapwad2JxM0rSFi/HFWFqngN+JroGaPQ3rr6lBxfnqtz1nkjMdVgEZ6YajC/Z423hx0t6zmz7VAyG/YlUrKIgj4d3ck0mDh8JqvXsH1TNQYjr8Wcs3+bpOQUjEYjbq6Wvxturq48CAt/4XwPHj5K8J27zPnpx5ct4kuTOj76HbIcGmxITULuXeiZ6e0DimNfKIDktQvyq4hPlZocA4CTi4dFuKOLJ6lJeZs7ajQa2bVqIgElq+FT2PrcwvyQnJhVPme1u0W4i6sHyQlx1pKQmpKA0WjAxdUyjbOrB5HhIVbTHNu3iUKFi1GiTBWL8Lff/4QyFWshVyi5duEEK3+bhDYjnWZtur1YhaxIT8k6Pg5OlsdH5eRJesrTj0985E02zemKIVOLvdyBlj1/wc0n+6ZBySptcXL1w9HFm7ioW5zeMY3EmBBa9pxts/IXBDHE0bb+7xtoAFOmTKFp06YMHTo0x3svcldm5cqVjB07ls2bN+Pt7W0O12g0KJXKZ6bv06cPb7/9NqdOneKDDz547jKoVCrS09Otvjdp0iTGjrWciDq0YWWGN8o56damJBKMacmkbloKJhOZEaFIXVxRNXzTagNNWb0RuuDLGFMS87dcL0IixZSegmbXKjCZMEQ/QOrsiqJWM4sGmjE+mpQlk5AoVNiXropDmx6krvy5QBppz00iwZSeSsbeNWAyYXwYhs5JjbxGk1faQHshEgmmtBTSd67IOj5RD8hwUqOs08KigWaMiyZ58SQkCiX2pavh2LYnKX/89NoeH6cGzdGFhVgsKPKoh11z4TQp+7KGdOnD7qEoUQbnRq1s3kBrUM2Rj7tkDyuatCD6hfKRSiTceaBl1Y4EAELCdQT42tOynjOHz6RSvLCcNo1cGD49wiblFl7ew5hYfl2wiKnjv0culxd0cV6aqlZj9JH3c11QxNYundjK1uVjzK+7fznvpfPc8cc4HoYH02fEypfO62lOHd7BH/OzFyUa9N2sfP08AJ02g9NHd9Lm3Y9yvNe2S3/z30WKl0Gr1bBn0/KXaqAFn9/K0Q3Zx6f1hy9+fNRexej8xUZ0GSncu7ybQ39+Q7uPfzc30srWfs8c171QaRycvdi+oDfJcfdx8Sjywp8r/LeIBhrQqFEjWrVqxYgRI+jdu7c5PCgoiOvXra9Y9yg8KMjyrtXq1avp168fa9eupXnz5hbveXp6kpCQYBFWqlQpjh07hl6vx97eHgBXV1dcXV2tDrvMi/j4eLy8rI9tHjFiBIMHD7YIS5446LnyN6anYDIYkDpZrgoldVLn6FUzp0lJzJp79lhj0xATiczZFWQyMGTfgZe6emBfohzJK3/JmZGNmdJTMRkNSB2debwPQOrggsnKJHMAU2oSxifrEheF1EkNUpl5jh1GA8Z/7jQaoh8gK1QERY3GaHavzq/qWC+vJg2T0YDE4YkhZA7OVifSA5jSkjEZLOtojI/OWcd8Zj4+/2PvvuObLv4Hjr+SjqQ73QM66WKVvWTvLSBfFAQZMkRBRVAEUbaCyhIVVEQE2SJ7L0H23hvaQoG2lLbpbpr1+yOQNjRllJSiv3s+Hp8/crn75K6fNv3c5+7eZ+9scn0kDk5F/65lpht+n57m+qQanpJqE+Kw9g1EXqsp2VuXlUhbtJmGvxsrZ4VJupWTC9q0VPOFHpDYynCo1QDlOtO6Gc6pQR0fZ5Kujr+NLNT86P/zOH4hm+vT8kdgrK0NHUSFoxXK9PwrpHC0IvZu4WiMD6Wma7mdaDpt8U6imrpRDgBEhshxdrRi7lh/4/tWVhL6dHKjfWNnhkwq3nfj/xcuzk5IpVJSlaajS6lKJa6uimKd89r1GyiVaQz+cIQxTafTce7CRdZu3MyWNSuxsnr6mRjPS5f18P+Q6SihlaOL2YA7BUlsZMir1CVz+18lWUUTEVWbUiYkyvhaqzH8fWSmJ+OkyH+Qm5V+H5+AJ//tblo8katn9tBv1GJc3MzPXLGUKrUbExyeH01W8yAAUUZaCgq3/HuNdGUy/sHmp2s7OrkilVqRrjRdD5yhTMZF4V4o/4lDO8nLy6Vekw5PrF9wWGU2/TkPtToPG5viPTwIrNAUL//C1yc7Mxl75wIP2jPv4+73+OtjZW2Li4chmqVn2Uok3T7Puf2LaNR1otn8XgGGz027f/Nf3UHT68UImiX9v16DVtDUqVPZsGEDhw4dMqZ1796da9eusWFD4cXG06dPx93dnZYtWxrTli1bRr9+/Vi2bBnt2xdeQFqtWjUuXjQN492jRw8yMzOZM2eORdpx48YNcnNzjYFJHiWTyXB2djY5nmV6IwBaLZq7sdiGVMhPk0iwCSmPOu662SKaW9cN0+kKrAOy8vBBm55q0jkDkFdvgC4rnbyrZ56tXsWh0xpuzgML/lORYB0UXuR6Mc2daKxcPaHAcL7U1ctwU/C4jotEgsSqFJ6J6LToEm9jHRBWIFGCdUA42vhYs0W0d2KQKh5to6dhOtEL6pwBD67PLayDTK+PTWBE0dfn9g2kj1wfKzfv/IcERZFIoCSvj1ZD3q0byCPzbwKQSJCXj0IV/fh1QfY16iOxtiHryN5C51TFXsfap4xJso23H9rkJEvV3ChXpSfhvsZ43E5Qk5quoVJ4/swAO5mE0EAZV2KLXtdzJSYXPy8bkzRfLxuSUg2BQv45nsnH397hk2n5R4pSw/q/0/jyp+KN2v1/YmNjQ3hoOU6eyV/bo9PpOHXmHBUin26946OqVYli3g+z+Hn2DOMRHhZK8yaN+Hn2jBfaOQNAq0V9Jwbb0Ir5aRIJtqEVUd80/3/oIXmV2kisrckp4Sn0BcnsHHH3DjQenn6hOLp4EnMx/54jNyeT29FnKVuuapHn0ev1bFo8kcsnd9Jn5O+4ej55evTzkts54OUbYDx8/UNwVnhwqUD0y5zsTGKunSckIsrsOaxtbAgoV57LBcrodDounT1qtsyBXWupUrNxoWmU5sTFXsHe0bnYnTMAW5kjLh6BxsPVOxQ7J0/uXs+/Pnm5mdyLO4tXQNVnOrder0OnLfqBVfLdywAmHUFBEB20BypXrkzPnj2ZPTt/6L579+506dKFPn36MH/+fGJjYzl79izvvPMO69ev59dff8XBwfDEd+nSpfTu3Zvp06dTp04dEhISSEhIIC0t/0le69atuXDhgskoWr169RgxYgQjRoxg+PDh7N+/n5s3b3L48GHmz5+PRCJBKn36y7Rv3z5CQkIoV66cBX4qRcs5sB15zcbIqtXHytMXx1d7I7GVkXvCENXRqesAHFr+Lz//0b+R2Dng2O5NrNy9sQ2Pwr5xe3KP7DY9sUSCvHoDVKcO5EdILGGqY7uxrfIKNpXqIHX3xq71G2AjM0YNtG//FvJGr+bnP7UPidweuxb/Q+rqhXVIReT1WqE6lR/VTd7oVazKlkPq7IbUww95o1exDggj7+LxF9KmR6lO7MGmcj1sKtRC6uaNvEU3JDa2qC8Y/lnK2/RE1iD/SWXemQNI5PbIm3ZBqvDEOrgCtrVbknd6/wuve+7R3ciq1se2ch2k7j7Yt+luuD5nDf847Tv0Qd64kzG/6uQ+pHb22LXshtTNC+tylZC/0hrVyQLXp3EnrP1Dkbq4IfX0M7wODCOvhLd0SN+xDqeGLXGo1xRrn7K49RyMxFZO5gFDJE33fh+i6NKrUDnHBi3IPn2k0FpJgPRta3CoWR/HBi2x9vTBqWk77KJqkbF3S6G8JWHT3nS6tlRQs6I9Ab42DO3pSWq6lmPn8qdZj33XhzYN8kdwN+5NIyxQRpcWLvh4WNOgugMt6jqxdb9hVDQzW0dcgtrk0Oj0pKZruZtU+gFDrBzsca4SiXOVSADsg8viXCUSuf+T1z69KF07v8rmbTvYvms3N+Pi+G7Oz+Tm5tKmhSEi3dTp3/Hr7/nRGtVqNdejY7geHYNGo+F+cjLXo2O4c9cQ1dTe3o7goECTQy6T4ezkRHDQ4/e+KinZ/2zBvk4T5DUaYuXlh/Nr/ZDYysg5ZniQ4dL9HRzbvl6onF2tJuReOGF+m44XRCKRULdlb/7Z+BOXT+0m8fYV1vz6KU4KLyKr58++WfhtX47syt8HdNPiiZw9tIGu70zDVu5ARloSGWlJqPPyA/VkpCURf+sSKfcM+1zeu32V+FuXyM5UWqzuLTq8yeZVv3L66B5u37zGb7O/QOHmSbXa+Xu2zhj3Drs3588YadmxF/t2ruHg3+uJvx3Nkp+/Ik+VQ/1mnUzOfy/+FtcunqRBiy6FPvvMsb3s27GaOzevcy/+Fnu2rmTLX/Np1q67RdpWsI2VG/Tm5O6fiL24m5T4K/y94lPsnb0Iqph/fTb+0pfzB/Ovz9Et04mPPkZGym1S4q9wdMt07kYfJbSqIdhcevItTu6cQ9Lt82Sk3Cb24m7+XvEpvsE1cfct3sOTl4UOSakd/0ViimMBEydOZMWKFcbXEomElStXMmvWLGbOnMl7772HXC6nXr167Nmzx2Sj6l9++QWNRsOQIUMYMmSIMb1Pnz78/vvvgKETWL16dVauXMk777xjzDNt2jRq167N3Llz+e2338jOzsbb25tGjRpx6NAhnJ2ffoPJZcuWMXBg4TnblqY6fxSJgxMOzTsbNqqOv0XawhnGaYFShbvp9Li0FNIWTsexXQ9ch05Cl5FKzqEdZD8Skt+mXAWsFB7knijh6I0FqC+fJMfeEbsG7ZE4OKG9d4eslT8ag2pInd1M2qLPUJK5cg52zV/D6e3R6DKUqI7vMQnJL3FwxKFDbyQOzuhVuWiT7pC1cg6a2MsvrF0Faa6eItfeAdkrbZHYO6NLukP26p+NNyhSJ1d0BduYqSR79U/ImnTGofdI9Jlp5J3aazYkf0lTXzpBjr0j8oYdkDoYNqrOXPlDgevjCvr8zrw+I5WMFT9g3/x/yPqPMVyfY3+TWyAkv9TBCfsOfZA6Prg+9+6QufyHEr8+2ccPkOrkguLVHlg5u5J3O4Z7sycYp2RZu3ma/K4BWHv7IQ+rQOLMceZOSc7pIyQv+QmXNl1x7T4ATeJdkn76GtX1F7Oh+LrdachtJbzzujv2dlIux6j48ucEkz3QvD2scXLIH2G5EZfHt78l0rO9G/9rpeBeiobf16aw/6T5sPwvG5calai3K79zU2HaZwDELVrN2f6jS6taJpo2akBaWjq/L15Oamoq5UKCmTJxrHGK472kJKQF9p1MTkll8Af509//XL2OP1evI6pSRWZMnfzo6V8KuWeOIHVwxql1V8NG1XdvkvrrN8bpz1YKj0J/T1aevtiGRJDyy9TSqLKJ+m0HkKfKYcPCseRmpxMQVoNew+eZ7IGWcu8W2Rn5D3WP/22Y5vz7171NztXp7a+o1uC1B3mWs3d9/v58C6b2KpTnebXu0heVKofFP00mOyuD0PJV+fCLH032QEtKiCOzQFCkWg1ak5Geyvplc0lXJlM2OIIPvvgR50emOB7YtQ6FuzcVqtYr9LlWVtbs2bqSlQumA3o8ffzp1ncEDVtapl0FVWk8AE1eDvv+Gktebjo+QTVo+/Y8kz3Q0lNukZuVf31yMlP4e+WnZKcnYSt3wt03gnZv/0rZcMP9otTKhjvXD3LuwEI0eTk4uPgSXLkV1Zu9a/H6C/9uEr2lY5MKj7Vp0yY++eQTzp8//0wjY0/jwoULNGvWjKtXrz511EeApM/7WbQepc3GyaG0q2BRUpv/znMUraroaR7/Rukx/60gFh87zCztKlhM3x/M78X0bxV1cU1pV8GibH75qrSrYDF/dy5637h/Iz/Ffyvk+/Fr//5NoB8a0fnlHS06fc3y0+qfVtWw/96ecmKK4wvWvn17Bg0axJ07xQ91XJT4+HgWLVr0TJ0zQRAEQRAEQXgeeiSldhTHjz/+SFBQEHK5nDp16nD06OM3rP/zzz+JjIxELpdTuXJlNm/e/Nj8z0t00ErBsGHD8Pf3f3LGZ9SiRQtat25t8fMKgiAIgiAIwn/BihUrGD58OOPGjePkyZNUqVKF1q1bc+/ePbP5Dx48SI8ePejfvz+nTp2ic+fOdO7cmfPnz5dYHUUHTRAEQRAEQRCEYtPrJaV2PKsZM2YwcOBA+vXrR4UKFfjpp5+wt7fnt99+M5v/u+++o02bNnzyySeUL1+eSZMmUb16dX74oeS2gxIdNEEQBEEQBEEQ/pVUKhXp6ekmh0plfquXvLw8Tpw4YbJXsVQqpUWLFiZbbRV06NChQnsbt27dusj8liA6aIIgCIIgCIIg/CtNmTIFFxcXk2PKlClm896/fx+tVou3t7dJure3NwkJCWbLJCQkPFN+S/jvhIcTBEEQBEEQBOGFK26wDksYPXo0w4cPN0mTyWRF5P53EB00QRAEQRAEQRD+lWQy2VN3yDw8PLCysiIxMdEkPTExER8fH7NlfHx8nim/JYgpjoIgCIIgCIIgFNu/JUiIra0tNWrUYNeuXcY0nU7Hrl27qFev8OboAPXq1TPJD7Bjx44i81uCGEETBEEQBEEQBOH/heHDh9OnTx9q1qxJ7dq1mTVrFllZWfTr1w+A3r17U6ZMGeM6tg8//JDGjRszffp02rdvz/Llyzl+/Di//PJLidVRdNAEQRAEQRAEQSi20lyD9qzeeOMNkpKSGDt2LAkJCVStWpWtW7caA4HcunULqTR/kuErr7zC0qVL+fzzz/nss88ICwtj7dq1VKpUqcTqKDpogiAIgiAIgiD8vzF06FCGDh1q9r09e/YUSuvWrRvdunUr4VrlE2vQBEEQBEEQBEEQXhJiBE0QBEEQBEEQhGJ71mAdwuOJETRBEARBEARBEISXhBhBEwRBEARBEASh2HSlXYH/GDGCJgiCIAiCIAiC8JIQHTRBEARBEARBEISXhJjiKAiCIAiCIAhCsYkgIZYlRtAEQRAEQRAEQRBeEmIETRAEQRAEQRCEYtMjRtAsSXTQBKwd7Eq7Chalzsgq7SpYVPa91NKugsXoNP+tOE8uwz4p7SpYlM9u99KugsVEXVxT2lWwqLMVupR2FSyq8axOpV0Fi+l4ZnRpV8GiLjf/tLSrYFH9zn9U2lWwnM4/l3YNhBdEdNAEQRAEQRAEQSg2sQbNssQaNEEQBEEQBEEQhJeE6KAJgiAIgiAIgiC8JMQUR0EQBEEQBEEQik0ECbEsMYImCIIgCIIgCILwkhAjaIIgCIIgCIIgFJtOX9o1+G8RI2iCIAiCIAiCIAgvCdFBEwRBEARBEARBeEmIKY6CIAiCIAiCIBSbCBJiWWIETRAEQRAEQRAE4SUhRtAEQRAEQRAEQSg2vV6MoFmSGEETBEEQBEEQBEF4SYgRNEEQBEEQBEEQik0vwuxblBhBEwRBEARBEARBeEmIDpogCIIgCIIgCMJLQkxxFARBEARBEASh2HQizL5FiRE0QRAEQRAEQRCEl4QYQRMEQRAEQRAEodhEmH3LEiNogiAIgiAIgiAILwkxgmYBb731FuXLl+ezzz4r1XrUrVuXTz75hK5du76Qz5NVb4SsTkukjs5o790me/tKtPE3i8wvkdkhb/wqthFVkcjt0aWnkL1zFZobFwCwrdYQWfVGWLm4AaC9H0/O/s1ooi+WeFvkdZph36AtUkcXNAm3yNy4BM2dmKLbIrfDoUVXbCvWQGrngFaZTNbmZeRdPfsggwT7Zp2RV62H1NEFXYaS3JP7yd6zocTbAuDYpC0urbpg5aIg73YsKcvmkRd7zWxe7xGTkUdUKpSefe44Sd9PNr629imLa9feyMMrgtQKdXwcST99jTblfom14yGnZu1wadMFKxdX8uJiSF7yC3kx5tvjM/JL7CIrF0rPPnOMxO8mARD823qzZVNWLiBt6xrLVbwIq7buZsn6baQo0wgN9Gf42z2oGBZiNu+6nf+wZe8houPuABAREsjgHl1M8v+6ch07DhzjXnIKNtbWZvOUpHZ1ZbxS2QY7mYSYu1pW7M4lSakrMn/bujLa1ZWZpCWmaJm8KMv4+pVKNtSMtKGspxV2Mgkj56aToyqxJhit27iZlavXkpKqpFxwEEPfGUBkRLjZvLE3b/H7kmVcu36DxHtJvDvwbbp26ljkuZf9+RfzFy7mtVc78N6g/iXVhGfm1qAmISP641K9EnI/L453fY/E9btKu1qFrDwTzaKT10nOVhHm4czIxlFU8nE1m3f9xVtM2HnKJM3WSsqhIfnXp8bsdWbLfli/Ar1rhFmu4kVYeSaaRSeukZydS5iHCyObRFHJx81s3vUXbzJhx0mTNFsrKYeGdjK+rvGd+e+uDxtUpHcN87/DT0uv1/PX0nn8vX0dWVmZhJevzNvvjsTHL+Cx5bZvWsWmNYtJS00hIDiUPoNGUC68ovH9vDwVS36bzeF9O1Cr1URVq0O/wZ/g4upuzHP+zDFWLfmFuJs3kMnkNGzWjtffGoyVleEW9uK5E2xZt5zoaxfJyc7C28+fDl16Ur9Jm2K3V1ajCfJ6LZE6uqBNvE3WtuVo78YWmV8is8OuaWdsI6ohsbNHl5ZC9vaVqG+cB8CuUQfsGpl+N2jvJ5D207hi11H4byvxDlrfvn1ZuHAhADY2NgQEBNC7d2+uXr3KkiVLiiwXGBhIbGwser2ecePGMW/ePJRKJfXr12fu3LmEheV/eV69epVPPvmEAwcOkJeXR1RUFJMmTaJp06ZFnj8mJoYxY8awZ88eUlJS8PDwoEaNGnz99ddERkaa5FWpVNSpU4czZ85w6tQpqlatanzvzJkzbN68mblz5xrTmjRpwt69e1m2bBndu3c3ps+aNYtZs2YRGxtrTMvJyWHq1KksW7aMmzdv4uTkRNOmTRk/fjwVK+Z/ic2bN49FixZx/rzhj71GjRp89dVX1K5d25jn888/56OPPqJLly5IpSU7OGpTvgZ2zbuSvXUZmruxyGs1w/GN90n/ZTz67MzCBaRWOPb4AH1WBpmr56HPVCJ1dkevyjZm0WcoydmzFl3KPZBIsK1UF8f/DSb9tyno7seXWFtklWrj2LY7GesXoYmLxu6Vlrj0HUHKrNHoszIKF7CywqXvJ+iy0klf9iO69FSsFB7ocvPbYt+oHXa1m5Lx169o7t3BukwwTq+9jT43h5zDO0usLQD2Nevj1u1tkpfMJS/mKk7NX8Xrw3HcHTsEXUZaofxJc6eCdf5XgZWDE75jZ5F9/KAxzdrTB5+RX5F5YBfK9cvQ5+Zg4+ePXq0u0bYAONRqgPsb/bn/xxxU0VdxbvkqPsMncPuzd822596PU5BY5bdH6uhEmQmzyTp+wJh2a1hvkzJ2UTXw6Ps+WScOUtJ2HjjK7IUrGTmoFxVDQ1ixaScffTmL5d9Nxs3FuVD+kxeu0LJBbSqHl8PW1obFa7cwbPJMlsyYiJe74ebU39eHEf3fpIy3J6q8PJZv3MGHk2by5/df4eriVKLtaVHTlsbVbFm8LYfkdB3t68l4r4s9Xy7KRKMtutzd+1p+WJ3/N6N7pD9nayPhUqyGS7EaXm0gL6Ham/r7n/389OsCPhwymPIR4fy1bgOjxk5kwc8/4KpQFMqfq1Lh6+NN4/qvMPfXBY899+Wr19i0dTshQUElU/nnYOVgT/rZK8T9/hc1V/1Y2tUxa/vVO8zYd4HPmkVRyduVpaejGbruEKvfao6bvcxsGQdba1a/1dz4WvLIDKxt/VubvD54M5GJO0/TLNTP4vV/1Part5mx7xyfNa1KJR9Xlp6+wdC1B1ndu+Xj29O7pfH1oxPKtg1oa/L6YGwiE3eepFlomeeu78bVf7Bt40re+XAsXt6+/LnkF6aOG8Y3Py7D1tZ8fQ/t28GS+d/x9nufUi68IlvXL2fquGFMm7sCF4WhI7r411mcPn6QD0Z+hb2DI7//PI2ZU0Yx/pt5ANyMuca3E4bT6fW+DB42ltSUJH6b8zU6nY6eb38AwLVL5wgICqVj17dwUbhx6tgB5s6aiJ2DI9VrNXjmttpWqIl9y/+RtWUpmjsxyGs3x6nHB6TNHYc+28w9gdQKp57D0GVlkPnXz+gylEhd3NDn5phk09y7Q8aSWfkJusd8Qf4LiX3QLOuFTHFs06YN8fHxXLt2jREjRjB+/HjCwsKIj483HgALFiwwvj527BgA33zzDbNnz+ann37iyJEjODg40Lp1a3Jzc43n79ChAxqNht27d3PixAmqVKlChw4dSEhIMFsftVpNy5YtSUtLY/Xq1Vy5coUVK1ZQuXJllEplofwjR47Ez8/8F/b3339Pt27dcHR0NEmXy+V8/vnnqB9zA6tSqWjRogW//fYbkydP5urVq2zevBmNRkOdOnU4fPiwMe+ePXvo0aMHf//9N4cOHcLf359WrVpx584dY562bduSkZHBli1bivxMS5HXbobqzAHyzh1Gl5xA9tZloMnDNuoVs/ltq7yCRG5P5l8/ob0TjS4tBU3cNbT38uuvvn4OzY0L6FKT0KXcI/ef9ejzVFj7BZdoW+zqtyL3+D+oTu5Hm3SXzPWL0KvzkNdoaDa/vHpDpPYOpC/5Hs2t6+iUyahjr6BNiDPmsfYPRXX5FHlXz6JTJpN34Tjq6xewLlvyIxrOLTuRsX87WQd3o46/TcqSuejzVDjWb242vy47E1260njIK1RFn6ci+0R+h0bRuSc550+i/Gsh6rgYNEkJ5Jw5ZraDZPH2tO5Exj/bydy/C/XdOJIXzUGfp8KpYQvz7cnKRJuuNB52Fauhz1ORdSy/PQXf16Yrsa9ah9zL59AkJZZ4e5Zt3MGrzRvSoWkDgv39GDmoFzJbWzbu3m82/4QPB9K1dVPCgwMIKuPL6MF90en1HD9/yZindcM61I6qQBlvT0L8y/BhnzfIysnh+q3bJd6eJtVs2XZExbloDXfv6/hjWw4uDhKiyj3++Z9ODxnZeuORlWv6333PqTx2HM8jJuHF3cT8tXY97Vq3pE3L5gQG+DNsyGBkMhlbd5gfTYoMD+Odt/vStHFDbGyKbm9OTg5Tps3ko/ffw9HRoaSqX2xJ2/7h6rhZJK4r2YdHz2Pxqet0qRTIqxUCCXF35rNmVZBbW7Hu4mNmbQAeDnLj4W5v2tEv+J6Hg5w90QnULOtBWZeSv0aLT16nS8UgXq34sD1VDe25EFtkGQkS0/Y4PKk98dQs6/nc7dHr9Wxdv4LOr/ejZt1GBASH8e5H41Cm3OfE4X+KLLdl3TKatupE4xYdKBsQzNvvfYpMJmfvzo0AZGdlsmfnBnr2/5CKVWoSHBrJOx9+zrXL57h22fAw+vC+nQQEhfJa9/74+PlTvlJ1evQdyo7Nf5GTbRhx7/R6X7r1eofw8lF4+5alzatvUKV6XY4f3FOs9srrtEB1aj95Zw6iux9P9uYloM5DVtX8/Y2san0kdg5k/jkHze0b6NKS0dy6hvbeI9+/Oh36rPT8IyfL7PkEAV5QB00mk+Hj40NgYCDvvvsuLVq0YOvWrfj4+BgPAIVCYXzt6emJXq9n1qxZfP7553Tq1ImoqCgWLVrE3bt3Wbt2LQD379/n2rVrjBo1iqioKMLCwpg6dSrZ2dnG0aZHXbhwgRs3bjBnzhzq1q1LYGAg9evXZ/LkydStW9ck75YtW9i+fTvTpk0rdB6tVsuqVavo2LHwlJYePXqgVCqZN29ekT+XWbNmcejQITZu3Mjrr79OYGAgtWvX5q+//qJ8+fL0798f/YNHEkuWLOG9996jatWqREZG8uuvv6LT6di1K//GwcrKinbt2rF8+fLHX5DnJbXCyicATcyVAol61LGXsS5jvjNlG1YZzZ0Y7Ft1x+WDqTgP+Bx5vdaFH2k+JJFgU74GEhtbNHeiLd+Gh6yssPYLIu/BNEsA9HrUNy5i4x9qtohtZDXUt27g2LEX7qNm4fr+JOwbtzdpiybuOrYhFbBy9zZ8jI8/NoFh5F07W3JtAbCyxjagHLmXCnyOXk/upTPIQiKe6hSODVqQdWw/+rwH88kkEuwq10STeBevD8dRdtrv+Iz+BruqdUqgAY+wskYWGErOxdP5aXo9ORfPICsXWWSxgpwatiDz6L789jxC6qzAPqomGft2WKDCj6dWa7gSfZNaURXyP18qpVZUec5ffbrf89y8PDQaLc5F3Oir1RrW7vwHR3s7wgLLWqTeRXF3luDiIOVKnKZA/SA2QUuwr9Vjy3oqpEwe4Mi4fo70bmOHq1PpLjBXq9VcvX6D6lWrGNOkUinVq0Zx8fKVx5R8stlzf6FOrZrUKHBu4emptTou30ujtr+nMU0qkVDb35Nz8alFlstRa2m/YDvtftvG8A1HuJGcXmTe5Oxc9scm0qlioEXrbo6hPUpqBzzSngBPziWkFFkuR62h/W9baTd/K8M3HHp8e7Jy2R+bYJH2JCXeRZmaTMUqtYxp9g6OlAuvyLUr58yW0ajVxFy/QqWq+WWkUimVqtTi2mVDmZjrl9FqNFQqcF6/skG4e/pw/cF51eo8bGxtTc5taytDnaci5sblIuucnZWJg1PhGQlPJLXCyjcAdcylAokP72/MP2C1CY9Cczsa+zZvohj2Lc6DxiKv37bQ/Y2VmxeKD7/GZchkHDq/jdTZ/PTcfys9klI7/otKJUiInZ0deXl5T8wXExNDQkICLVrkPyl3cXGhTp06HDp0CAB3d3ciIiJYtGgRWVlZaDQafv75Z7y8vKhRo4bZ83p6eiKVSlm1ahVabdFPZxMTExk4cCB//PEH9vb2hd4/e/YsaWlp1KxZs9B7zs7OjBkzhokTJ5KVZf4pydKlS2nZsiVVqpj+05ZKpXz00UdcvHiRM2fOmC2bnZ2NWq3Gzc10vnrt2rXZt29fkW2yBIm9IxKpFbps038O+qwMpI7mvxClCg9sI6uBVErmyh/JObAFWZ3mhi+xgvk8/VCMmIFi5Gzs2/Qgc/Uv6JLNj4RagtTeCYmVFbpM07boMtOKbIuVmyeyijVBKiVt0Uyy/96AXf022Dd51Zgn+5/NqM4dwfXDr/CYMA/X98aTfXAHqjOHzZ7TUqwcDe3RpitN0rUZaVi5PPmfgW1QGLZlAsncn99ZkTq5IJXb4dzmNXIunCRx1gSyTx3Gc/CnyAqsJSgJVk7O5tuTrsTKRfHE8rbBYdiWDSLjn+1F5nF6pRm63ByyTxx6zto+mTIjE61OV2gqo5uLM8nKpxuNnLN4FZ5uCmpVrmCSvv/EGZr1GkLjnu+yfOMOvvtiOArnkp3e6Oxg+BeSkWU6+pWRrTe+Z87NBC2Lt+cwZ202K3bn4O4sYVg3B2Q2JVrdx0pLz0Cn0+GqcDFJd1UoSE1VFvu8f+/dx7Ub0Qzo0+s5a/j/lzJHhVavx/2RqX/u9jLuZ+eaLRPk6sjYFlWZ0aEOk1rVQKfX0+/PfSRm5JjNv/FSHA421jQr52vx+j+q6PbIuZ9l/kFSkKsjY1tWZ0bHukxqXROdHvqt3PuY9twytMcC0zWVqckAxmmJD7ko3IzvPSojXYlOpy1UxlnhSprSUEapTMba2gYHR9PvqYLnjapel6uXz3Fw73Z0Wi0pyfdYvfw3Q/kU8599eP9Ooq9donGLDs/Y0vz7m0eXN+gy05E6upgtY6XwxLZ8dSRSCRnLvyd3/2bkdVogb9DemEdzJ4bMDb+TsWw22VuWInXxwKnPJ1DE9FBBeKEdNL1ez86dO9m2bRvNmjV7Yv6HUxS9vb1N0r29vY3vSSQSdu7cyalTp3ByckIulzNjxgy2bt2Kq6v5G9IyZcowe/Zsxo4di6urK82aNWPSpElER+c/wdbr9fTt25fBgweb7YAB3Lx5EysrK7y8vMy+/9577xnrY87Vq1cpX7682fcepl+9etXs+59++il+fn4mnVcAPz8/4uLi0D26oOMBlUpFenq6yaF63EIRS5FI0GdlkL1lCdqEONSXTpB7YCuyaqbTCHXJiaT/NoWMhd+gOrkPhw69kbr7lHz9noVEgi4rncy1v6O5exPV+aNk79mAvHYTYxZZpVrIqtQj48+fSZ0zgYzVv2LfoA2yavVLr95PwbFBC/Jux5oEFJE8eAqYc/ooGTs3oL4dQ/rW1eScO45To9ZFneql4NSwJXlxsUUGFAFwbNiCzMN70WtKfj3d81q0ZjM7Dhxl6ifvIbM17c3UqBjJwm/H8svkUdStWonPZ/xMSlrRT9iLo2aENdPeczIeVsX8D3IxVsPpa4YpkZdvavlpXTZ2MgnVwkuxh1YC7iXd58d58/ns44+wfWQUQChZUb5udCgfQISnCzXKevBt+9q42sn463ys2fzrLt6ibURZZNaPH/ktLVG+7g/ao3jQnjoP2mM+mNW6izdpG+lfrPYc2LOVt19vajy0Ws2TC5WQqGp1eLPvUH6b+zV9ujbi48GvU7WmYaqhRFp45OTC2RP88t1kBgwdTdmAFxMkyXBPkEHWpsVoE26Rd/E4uQe2IK/eyJhFfeMC6ksn0d67gzr6IpnLv0cis8e2gvn7S0F4IVEcN27ciKOjI2q1Gp1Ox5tvvsn48eMtcm69Xs+QIUPw8vJi37592NnZ8euvv9KxY0eOHTuGr6/5p2FDhgyhd+/e7Nmzh8OHD/Pnn3/y1VdfsX79elq2bMn3339PRkYGo0ePLvKzc3JykMlkxhvYR8lkMiZOnMj777/Pu+++W2T9n9XUqVNZvnw5e/bsQS43nYNuZ2eHTqdDpVJhZ2dXqOyUKVOYMGGCSdrIZjUZ1aJWobxF0Wdnotdpkdo7U7BrJ3FwKjQS9ZAuMx20WpNVpNrkBMMTKalV/mJZnRZdapLh/YQ4rH0DkddqaljjVgJ02RnotdpCo2VSR5ei25KhNNS3YFuS4rFyUoCVFWi1OLR5g+x/NqE6d9TwfuJtrBQe2Ddqj+rUAbPntQRtpqE9Vs4Kk3QrJxe0aUVPBQKQ2MpwqNUA5TrTn7XhnBrU8XEm6er428hCzT9gsBRtRrr59jgr0KYpH1tWYivDsXZDUtcuLTKPLKwCtr5lSfrpGwvU9skUTo5YSaWFOk4paem4K8w/nX1oyfpt/LF2C7PHjiA00L/Q+3ZyGf6+3vj7elMpvBzd3v+MDbv306dLO4vV/1y0htiE/CBA1laG7z4nBwnp2fl/D072Eu4kPf2DnxwV3EvV4akovZ1fXJydkEqlpD4ykpmqVOLqqijWOa9dv4FSmcbgD0cY03Q6HecuXGTtxs1sWbMSK6uXs0PwMlHYybCSSEjONh1dSs5W4WH/dAFkbKykRHi6cDut8IyWU3eSuZmaydQ2L+Zmuej25OLh8HQjKsb2KM21576hPW1rmyn5ZNVrNzSJtKh58PAqTZmCq5uHMT1NmUJgiPlol07OCqRSK9KUplM205WpuCgMERoVCnc0GjVZmRkmo2hpyhQUBaI4tuv8Jm079UCZch8HRyeS7sWzYtEcvHxMg59cOn+S6ZM/plf/YTRsVrzvvYf3NxIH01E9qaMzukzzsxx0mWmF7wnuxyN1euT+puDnqHLQpSRi5epZ6L1/K50IEmJRL+S/YdOmTTl9+jTXrl0jJyeHhQsX4uDw5EWrD9emJSaaLtxPTEw0vrd79242btzI8uXLqV+/PtWrV2fOnDnY2dkZo0cWxcnJiY4dO/Lll19y5swZGjZsyOTJk43nPXToEDKZDGtra0JDDeuRatasSZ8+fQDw8PAgOzv7sdM1e/XqRWBgoPG8BYWHh3Pp0iUzpTCmh4ebhsadNm0aU6dOZfv27URFRRUql5KSgoODg9nOGcDo0aNJS0szOT5qUr3I+pul06JNuIV1UME1TRJsAiOKDE2vuX0DqasnBeNOWbl553d2iiKRgFUJPkfQatHcjcU2pMB0MYkEm5DyqOOumy2iuXUdKzdvk/nlVh4+aNNTDZ1QQGJjWyikkV6nK3rNnaVoNeTduoE8ssDvhkSCvHwUqujHr6Oxr1EfibUNWUf2FjqnKvY61o/8M7Tx9kObnGSpmpun1aC6eR15+QLTgCUS7MpHoXrM+gMAh1r1wcaGzEN7iszj1LAlqthr5MXFWqa+T2BjYwiBf/xc/t+9Tqfj+LnLVAov+mnv4nVbWLBqIzPHDKN8uaCn+iy9Xv/YIEXFoVLD/TS98UhI0ZGWpSPCP/9vVG4LQT5WxMQ/fQfN1gY8FFLSs0rvP7yNjQ3hoeU4eSZ//aZOp+PUmXNUiHy69ZuPqlYlink/zOLn2TOMR3hYKM2bNOLn2TNE5+wp2VhJifRy4Vhc/veNTq/nWFwSlX2fbh2PVqfnenK62Q7d2os3Ke/lQrjn4x+SWIqhPQrz7SkizP6jjO0x06Fbe+Em5b0UxW6Pnb0DPn7+xqOMfzAKV3cunDlmzJOdncWNqxcIiyi8pQmAtY0NwaERJmV0Oh3nzx4j7ME2KMGhkVhZW3PhbH6eu7dvkpyUQOgj55VIJLi6e2Irk3Ponx24e3gTXGBd9cVzJ/h24gi69xlCszadi9VuQyW1aONvYRNc8OGjBJugyCLXw5u7v5E+6f7GRobU1bPITp8gvJAOmoODA6GhoQQEBGBt/fQ328HBwfj4+JgEwkhPT+fIkSPUq1cPMKzFAgqFlZdKpUVO8zNHIpEQGRlpXC82e/Zszpw5w+nTpzl9+jSbN28GYMWKFXz55ZcAxnD7Fy8WvU+XVCplypQpzJ071yS8PkD37t3ZuXNnoXVmOp2OmTNnUqFCBZP1ad988w2TJk1i69atRU67PH/+PNWqVSuyPjKZDGdnZ5OjOFMgco/uRla1PraV6yB198G+TXewkZF31rCOx75DH+SN8/dnUZ3ch9TOHruW3ZC6eWFdrhLyV1qjOpkfAUreuBPW/qFIXdyQevoZXgeGkXfhWKHPt6ScA9uR12yMrFp9rDx9cXy1NxJbGbknDFH1nLoOwKHl//LzH/0biZ0Dju3exMrdG9vwKOwbtyf3yG5jnrzLp7Fv3AHb8CikCndsy1fHvn5r8i6eLPT5lpa+Yx1ODVviUK8p1j5lces5GImtnMwDhr8j934fouhSeD2MY4MWZJ8+gs7M1gLp29bgULM+jg1aYu3pg1PTdthF1SJjb8lHDE3ftg6nxq1wfKUZNr5lcX/rXSQyORn7De3xGDAM1669C5VzatiS7JOHzbYHHuxlV6s+Gf+UfHCQgnp0aMn6Xf+wac8BYm/f5Zt5i8lVqejQ1DD9dcL385mz5C9j/j/WbuGX5esY815ffD09SE5NIzk1jewcw9qbnFwVc5eu5vzVG8QnJXP5RiyT5ywgKSWVZvVKfkRgz6k8WteWUSnEGl93KW+1tiMtS8/ZG/nTooa+Zk+jKvnTFzs3lBFaxgo3ZwnBvlYM7GCPTqfnxJX8DqWTvYQynlI8XQzf7X7uVpTxlFJEBHKL6Nr5VTZv28H2Xbu5GRfHd3N+Jjc3lzYtDBFQp07/jl9//8OYX61Wcz06huvRMWg0Gu4nJ3M9OoY7dw2Rie3t7QgOCjQ55DIZzk5OBAeVfDCKp2XlYI9zlUicqxgC79gHl8W5SiRy/5Jfj/W0elULZc2Fm2y4dIuYlAym/H2GHI2WVysY9uEau/0E3x/I/1/8y5ErHLp5j9tpWVy6p+SL7SdISM+mc0XTfbsyVWp2XrtL5xcQHMSkPdVDWXM+lg0XbxKTks6U3afJUWt5tYKhHmO3Hef7A/nBq345cplDNxPz27Pt+IP2BJmc19CeO4XSn4dEIqHNq2+wduXvnDjyD7dir/PTzAko3DyoUTd/Gt9Xnw9l+8Y/ja/bdurB39vX88+uTdyJi2HB3G9Q5ebSuLlhbZa9gyNNWnRk8fzZXDh7gpjrl/ll9mTCIisTFpm/N+fG1Yu5FXud27eiWbP8N9b/tYjeg4YjffCA48LZE0ybOILWHV6n9itNUaYmo0xNJrOYUYZzj+xEVq0BtlF1Dfc37d4EG1tUZwzbsDi82he7pp2N+VUn9iK1c8C+9RtI3bywCa2EXf225B7fY8xj17wr1gFhSF3csS4bglO3waDTlfj9zYuk10tK7fgveqk3qpZIJAwbNozJkycTFhZGcHAwX3zxBX5+fnTu3BmAevXq4erqSp8+fRg7dix2dnbMmzePmJgY2rfPX6AZGRnJlClT6NKlC6dPn2bcuHG89dZbVKhQAVtbW/bu3ctvv/3Gp59+CkBAgOmX+MMw+uXKlaNsWUNkNE9PT6pXr87+/ftN9kZ7VPv27alTpw4///yzyXq6jz76iHXr1tGxY0emT59OnTp1SExM5KuvvuLSpUvs3LnTOH3y66+/ZuzYsSxdupSgoCDjGjxHR0eTEP/79u2jVatWxfyJPz31pRPk2Dsib9gBqYNho+rMlT8Y9wiROruCPr+DrM9IJWPFD9g3/x+y/mPQZShRHfub3MP5wRukDk7Yd+iD1NEZvSoX7b07ZC7/AU3s40dKnpfq/FEkDk44NO9s2Kg6/hZpC2egzzJMQ5Mq3E1Gw3RpKaQtnI5jux64Dp2ELiOVnEM7yP5nszFP5sYl2LfoguOrbyF1cEaXoSTn2B6y/za/MaolZR8/QKqTC4pXe2Dl7Ere7RjuzZ5gDIlv7eZZaHTP2tsPeVgFEmea3zQz5/QRkpf8hEubrrh2H4Am8S5JP32N6rr5EWBLyjq2H6mTC66d38TKxRVVXDSJM8ejexA4xNrNs9DcChufMsjDKxI/bWyR53Ws0wiQkHmk6DDRJaFF/dqkpmfy64p1JCvTCQvyZ+aYYbg9mOKYeD8ZaYGR1tXb96DWaPhs+lyT8/Tv1pEBr3dCKpVy8048m/ccJC0jExcnB8qXC2buxE8J8X/+/Y+eZOfxPGytJfRoLsdOJiH6rpY5a7JN9kDzUEhxsMt/iKZwlNK3rR32cgmZOXqi72qZsSKLzJz869ggytZkM+thrxtmXSzensORiyWzXrBpowakpaXz++LlpKamUi4kmCkTxxqnON5LSkJaYN1Lckoqgz8Ybnz95+p1/Ll6HVGVKjJjauFZEy8rlxqVqLcrv+NZYdpnAMQtWs3Z/kVP83+RWoWXITVHxU+HL5OcpSLc05nvO9U1hs5PyMgxWW6Qocpj8u7TJGepcJbbEOml4LduDQlxN53Ovv3aHfRA6/CSjXj6qFbhZR+05xLJ2SrCPVz4vvMrxtD5hdqTm8fkXadIzlbhLHvQntcbF27P1duG9kRYtj0dXnsLVW4u83+cSnZWJuEVovh0/CyTPdASE26TUSCgU72GLclIU7Jq6TzSUpMJDAnj0/EzTTah7jVgGBKplO+mjkajzqNytTr0e3ekyWefOXGIdX/+jlqtJiAolOFjvqFqjfyQ9/t2b0KlymX9qoWsX5U/c6p8pWp8/pXp9+bTyLt4HIm9I3aNXzXc3yTeJmPZbGPgEKmLm+k9QXoqGUtnY9+yGy6DxqLLUJJ7bDe5B7ca80idXXHsMgCJnQP67EzUcdfJ+n2q+X1jBQGQ6IuzCOoZ9O3bF6VSaQyLX2RFJBLWrFlj7Hg99HCj6l9++QWlUkmDBg2YM2eOydS/48ePM2bMGI4fP45araZixYqMHTuWtm3bmpx/wYIF9O3bl/v37zNp0iR2795NbGwsEomEoKAg+vTpw0cffWR2k+fY2FiCg4MLbVQ9d+5cFi1aZIwqCYaNqqtWrcqsWbOMaYcOHeKVV14xbsD9UHZ2Nl999RUrVqwotFF1pUr5T5CCgoK4ebPwfi/jxo0zrue7c+cOwcHBREdHGzuRTyN1yntPnfffQJNlPqrVv1X2vcevHfs30WmeflT738Bl2CelXQWLGrf7vxMGfmT7kt8D7kU6W6FLaVfBohrP6vTkTP8WktJbO1kSLjf/tLSrYFEhf44q7SpYjNvnP5d2FYq0+WTpBdpqV/2/FWAKXkAH7b8uJyeHiIgIVqxYYZx2WVo+/fRTUlNT+eWXX56pnOigvdxEB+3lJTpoLy/RQXu5iQ7ay0t00F5eooNm3n+xg/ZST3H8N7Czs2PRokXcv3+/tKuCl5cXw4cPf3JGQRAEQRAEQRBeSqKDZgFNmjQp7SoAMGLEiCdnEgRBEARBEAQL0vHfDNZRWv5b4/KCIAiCIAiCIAj/YmIETRAEQRAEQRCEYhMRLSxLjKAJgiAIgiAIgiC8JEQHTRAEQRAEQRAE4SUhpjgKgiAIgiAIglBser0IEmJJYgRNEARBEARBEAThJSFG0ARBEARBEARBKDadCBJiUWIETRAEQRAEQRAE4SUhRtAEQRAEQRAEQSg2EWbfssQImiAIgiAIgiAIwktCdNAEQRAEQRAEQRBeEmKKoyAIgiAIgiAIxaZHhNm3JDGCJgiCIAiCIAiC8JIQI2iCIAiCIAiCIBSbCLNvWWIETRAEQRAEQRAE4SUhOmiCIAiCIAiCIAgFpKSk0LNnT5ydnVEoFPTv35/MzMzH5n///feJiIjAzs6OgIAAPvjgA9LS0p75s8UUR0EQBEEQBEEQiu2/uA9az549iY+PZ8eOHajVavr168egQYNYunSp2fx3797l7t27TJs2jQoVKnDz5k0GDx7M3bt3WbVq1TN9tkSv/y/+SIVncaZNo9KugkXlpqlKuwoWJXOyLe0qWIxWrSvtKliUwl9R2lWwqD6pI0u7ChazKnJeaVfBohyD/Eq7Cha1d9i60q6CxbiePV7aVbCoIGlMaVfBojKtFaVdBYsJLxdQ2lUo0p+HS+//e7e6lp8QeOnSJSpUqMCxY8eoWbMmAFu3bqVdu3bcvn0bP7+n+07+888/6dWrF1lZWVhbP/24mJjiKAiCIAiCIAhCsen1pXeoVCrS09NNDpXq+R7WHzp0CIVCYeycAbRo0QKpVMqRI0ee+jxpaWk4Ozs/U+cMRAdNEARBEARBEIR/qSlTpuDi4mJyTJky5bnOmZCQgJeXl0matbU1bm5uJCQkPNU57t+/z6RJkxg0aNAzf77ooAmCIAiCIAiCUGw6vaTUjtGjR5OWlmZyjB492mw9R40ahUQieexx+fLl5/55pKen0759eypUqMD48eOfubwIEiIIgiAIgiAIwr+STCZDJpM9Vd4RI0bQt2/fx+YJCQnBx8eHe/fumaRrNBpSUlLw8fF5bPmMjAzatGmDk5MTa9aswcbG5qnqVpDooAmCIAiCIAiC8J/n6emJp6fnE/PVq1cPpVLJiRMnqFGjBgC7d+9Gp9NRp06dIsulp6fTunVrZDIZ69evRy6XF6ueYoqjIAiCIAiCIAjFVppBQkpC+fLladOmDQMHDuTo0aMcOHCAoUOH0r17d2MExzt37hAZGcnRo0cBQ+esVatWZGVlMX/+fNLT00lISCAhIQGtVvtMny9G0ARBEARBEARBEApYsmQJQ4cOpXnz5kilUrp27crs2bON76vVaq5cuUJ2djYAJ0+eNEZ4DA0NNTlXTEwMQUFBT/3ZooMmCIIgCIIgCEKx/Rd3VXZzcytyU2qAoKAgCm4n3aRJEyy1vbSY4igIgiAIgiAIgvCSEB00QRAEQRAEQRCEl4SY4igIgiAIgiAIQrHp/oNTHEuTGEETBEEQBEEQBEF4SYgRNEEQBEEQBEEQik2vl5R2Ff5TxAiaIAiCIAiCIAjCS0KMoAmCIAiCIAiCUGz/xTD7pUmMoAmCIAiCIAiCILwkRAdNEARBEARBEAThJSGmOAqCIAiCIAiCUGwizL5liRE0QRAEQRAEQRCEl4TooL1Ab731Fl999VWJnPvixYuULVuWrKysEjm/IAiCIAiCIJij15fe8V/00kxx7Nu3LwsXLgTAxsaGgIAAevfuzdWrV1myZEmR5QIDA4mNjUWv1zNu3DjmzZuHUqmkfv36zJ07l7CwMGPeq1ev8sknn3DgwAHy8vKIiopi0qRJNG3atMjzx8TEMGbMGPbs2UNKSgoeHh7UqFGDr7/+msjISACCgoK4efOmSbkpU6YwatQo4+szZ86wefNm5s6da5LvwoULTJgwgb///pv09HQCAwPp3r07o0aNwt7e3pjvnXfeYefOndy9exdHR0deeeUVkzpUqFCBunXrMmPGDL744osn/bgtwr1jF7z+1x1rVzdyom9wZ8535Fy9VGR+j87dcO/QCVtPbzTpaaTt20P8gl/Qq/MA8O7VD59e/UzK5Mbd5MrAt0qyGUXy7vo/fHv2xMbNnezr14idMZ2sixfN5pVYWeHXpy8ebdth6+lJzq1bxM35gbTDh19wrQ08OnbBq1sPrN0eXJsfZ5F9pehr49mlG+4dOmPr5Y0mXYly317i5/9svDYANu4e+A54F+dadZDK5Kju3ubWtCnkXLtS4u3x7Pwa3m+8iY2bGzk3rnNr9kyyLxfdHq+ur+P5ahdsvb3RpClJ3buHO/N+ym+PVIpfn/64tWyFjZs76vv3ub9tMwl//F7ibXFu3h5Fu65YubiSFxfD/T9+QhV91Wxev9FTsCsfVSg96/QxEmaMB8DKWYHbG/2wr1QNqb0DuVcucP+Pn1An3i3JZpjo3zOIjq18cHKw5tyldKbNucbt+Jwi8//5ax18veWF0ldvusOMn64D4OcjZ+jb5ahcwRlbGylHTqYw8+frpCrVJdYOc+xfaYFD4/ZInVxQx98iY+0i1HHRZvO6DR6DbbnyhdJzL51G+du0kq6qWSvPRLPo5HWSs1WEeTgzsnEUlXxczeZdf/EWE3aeMkmztZJyaEhH4+sas9eZLfth/Qr0rhFm9r0Xza1BTUJG9MeleiXkfl4c7/oeiet3lXa10Ov1rF32M3t3rCE7K5OwyCq8NXgUPn4Bjy23a/NKtqz5gzRlMgFBYfQc+Akh4ZXMnn/mpA85d/Ig74+aRvW6TUze379rA9vWLyHh7i3s7B2o9UoL3nrnU4u0bc2mraxYs56UVCXlggP5YNDblA83//sQcyuOBUtWcPVGNIn3khjSvy//69S+UL6k5GR++X0JR0+eIlelooyvD59+MISIsHIWqXNBmzasY/Vff5KamkJwcDneeXcI4RGRRebfv28vi/9YyL3EBPz8ytD37QHUrFXH+H7Hdi3Nluv39kBe+9/rxtfHjh5h+dLFxMZGY2NrS6VKUXw+doLlGib8a700HTSANm3asGDBAlQqFZs3b2bIkCGMGzeO+Ph4Yx5fX18WLFhAmzZtALCysgLgm2++Yfbs2SxcuJDg4GC++OILWrduzcWLF5HLDTcCHTp0ICwsjN27d2NnZ8esWbPo0KEDN27cwMfHp1B91Go1LVu2JCIigtWrV+Pr68vt27fZsmULSqXSJO/EiRMZOHCg8bWTk5PJ+99//z3dunXD0dHRmHb48GFatGhBixYt2LRpE97e3hw9epQRI0awa9cu/v77b2xtbQGoUaMGPXv2JCAggJSUFMaPH0+rVq2IiYkx/gz69evHwIEDGT16NNbWJXtpFY2a4TdwCLe/n072lYt4du5GyJfTuDKgJ5o0ZeH8TVrg+/Yg4mZ8Tdal88jK+BMwYjSg5+4vPxrz5cRGEz16uPG1Xqst0XYUxa15CwI++JCYb74m68IFfN7oTuTM7zjT/XU0qamF8pd9ZzAebdoQPWUKuTdjcalTl/CpX3Nh0ECyr5q/+S4pisbN8HtnKLdnTyfr8kU8X+tGyFfTudz/TTSP/N4CKJq2wLf/O9yaPpXsi+eRlfUn4OPPQK/n7s8/AGDl6EjYzDlknDlF9JhP0KQpkZUpizYzo8Tb49q0OWXffZ9bM78l69JFvP73OmHfzOBC7x5m2+PavCVlBg0m9pspZJ0/h8w/gKBPxwB6bs/5HgCfHr3w7NSZmKmTyY2JwT4ikqBPx6DNyiRp9aoSa4tDnYZ4vDmQpN9/IPfGFRStO+P7ySTiRg5Cm5FWKH/C7C+RWNsYX0sdnfCf/ANZR/cb03yGfY5eoyVh1iR0Odm4tOmC76dfEjdqMPo8VYm15aGeXf35X4cyfDnrMvGJuQzoGcSMiZXp9d4x8tTmH20OHH4SaYH5GyGBDsyaXIW/9ycBIJdJmTkxiusxmXw45iwAA3oF8fUXlXjn41Mv7ImpvEodnDr2JP2vBeTduo5Dwza4DviU+998gi4rvVD+1IWzkBT47pXaO+L+0Veozh55MRV+xPard5ix7wKfNYuikrcrS09HM3TdIVa/1Rw3e5nZMg621qx+q7nxteSRvWe39W9t8vrgzUQm7jxNs1A/i9e/uKwc7Ek/e4W43/+i5qofn1zgBdm8ZiE7Ni5nwIfj8fQuw+qlc5kx4X2+/H4lNrbmr8eR/dtZ/ttMer87mpDwSuxYv4zpE95nyo9/4axwM8m7fcPSIj9727rFbF23hNf7fEi58EqoVDncv2eZhzi79x1g7vyFfPTeIMqHh7Jq/SZGjvuSRXO/w1XhUii/SqXCz8eLJvXr8eP8382eMyMzk/c//YJqlSsyddxnKJyduR2fgKOjg0XqXNC+vXv4dd7PDBn6AeGR5Vm/djVjvxjNT7/8hkJR+GHGpYsX+Pbrr+jTtz+1atdh756/+XLSeGbNnkNgUDAAixavMClz4vhRZn83g1fqNzSmHdi/jx9mz6R3n35EVamGVqflZmysxdsn/Du9VFMcZTIZPj4+BAYG8u6779KiRQu2bt2Kj4+P8QBQKBTG156enuj1embNmsXnn39Op06diIqKYtGiRdy9e5e1a9cCcP/+fa5du8aoUaOIiooiLCyMqVOnkp2dzfnz583W58KFC9y4cYM5c+ZQt25dAgMDqV+/PpMnT6Zu3bomeZ2cnEzq6eCQ/yWi1WpZtWoVHTvmP4XU6/X079+f8uXLs3r1amrXrk1gYCDdunVjw4YNHDp0iJkzZxrzDxo0iEaNGhEUFET16tWZPHkycXFxxBb4Y27ZsiUpKSns3bv3eS/FE3m89jopWzeSumMLqls3uf39dPSqXNxaF34KBuBQoRJZF86j3LMTdWICmSePkbpnF/YRjzxt1mrRpKYYD2164ZvWF8G3Rw/urV/H/U0byYmNIeabqehUuXh26Gg2v0ebttxduJC0QwdR3b3LvTWrUR48hG+PN19wzcGz6xskb9lAyvbNqG7Fcvu7aeie5tr8vZO8xAQyThwj9e+dJtfG6/We5CXdI276FLKvXCIvIZ6ME8fIiy/5URrvbm9wf9MGkrduJvdmLLdmfIsuV4V72w5m8ztWrEzm+XOk7tphaM/xo6Tu3oFDZH57HCpWQnlgH+mHD5GXmIDynz2kHz+KQ2SFEm2Lok0X0vdsJWPfTtR340j6/Qf0qlycGrcym1+XlYk2LdV42Feqhj5PRebRfQDY+PghDy1P0sIfUcVcQ51wh/sLf0Rqa4tjvcYl2paHur1ahkUrb7L/SDI3YrOYPPMy7m4yGtb1KLKMMl1NijL/eKWWO7fv5nDqvOHvvXIFF3y85Hw56wrRN7OIvpnFlzMvExnqRI0oxQtpF4B9o7ZkH/mbnOP/oL13l/TVC9CrVdjVNv+z1edkoctIMx62YZXQq/PIPXP0hdW5oMWnrtOlUiCvVggkxN2Zz5pVQW5txbqLN4ssIwE8HOTGw93edKSz4HseDnL2RCdQs6wHZV0sf+NcXEnb/uHquFkkrttZ2lUx0uv17NiwjI6v96d6nSb4B4Ux8MOJpKYkcfLIniLLbV+3hEatOtOw+auU8Q+h97ujsZXJ2bdrvUm+W9FX2LZuCf3fH1voHFmZ6axeMpeBH06gXuM2ePmWxT8ojGpF/B4/qz/XbaR9q+a0bdGUoAB/hr83CLnMli07d5vNHxkWyuB+vWnWqD42NjZm8yz7ay1eHu58+uEQyoeH4evjTa1qVSjjW/hh+vNau+YvWrdpS4tWbQgICOS9oR8ik8nYsX2b2fzr162heo1avPa/1/EPCKRX776UKxfKxg35o8uubm4mx+HDh6gcVQUfX1/AcF847+c59Os/kLbtO1KmbFkCAgJp2OjFfG+XBDHF0bJeqg7ao+zs7MjLy3tivpiYGBISEmjRooUxzcXFhTp16nDo0CEA3N3diYiIYNGiRWRlZaHRaPj555/x8vKiRo0aZs/r6emJVCpl1apVaJ8wkjN16lTc3d2pVq0a3377LRqNxvje2bNnSUtLo2bNmsa006dPc/HiRYYPH45UanoZqlSpQosWLVi2bJnZz8rKymLBggUEBwfj7+9vTLe1taVq1ars27fvsXV9XhJra+zDwsk4dTw/Ua8n49QJ7MtXNF/ni+exDwvHLtxwk2zr44tzrbqkHzWdAmhbpiwVlqwmcsFyAkZ+gY2nV4m1oygSa2scIiJJP1bgpkqvJ+3YMZwqVTZfxtYW3SOjFTpVLk5VqpRkVQvX48G1yTx1Ij9Rryfz1HEcnnBtHnbIbH18ca5tem1c6jUg+9oVgj6fSMWV6wmfMx+3tuY7q5YksbbGPjyC9BPH8hP1ejJOHsexYuEpPgCZF85hHx6B/YMOma2vH8516pF2JL89WRfO41S9JrKyhr8fu3KhOFaKKvT7aFFW1siCQsm+cDo/Ta8n5+Jp5KFFT6UpyKlRKzIP/2McGXs4ulZwKip6PXq1Gnm4+ettSX7ecjzcZBw7nT+qnJWt5eLVdCpFOj/VOaytJbRq6s2mnQnGNFtrKXpArdYZ0/LydOj0EFWh8BP5EmFlhU2ZYPKuXchP0+vJu3YBm8DQpzqFXe0m5J4+hF5d8iOZj1JrdVy+l0Ztf09jmlQioba/J+fiC88CeChHraX9gu20+20bwzcc4UZy4ZHCh5Kzc9kfm0inioEWrft/UVLiHdJSk6kYVduYZu/gSLnwSly/cs5sGY1aTeyNy1SMyp86J5VKqVClNtevnDWmqVS5/Dzjc3oNGomLa+EHIxdOH0Gn15Oaco/Phv6P4f3bMeebUSQnJRTK+6zUajVXr0dTo2r+VGypVEr1KlFcuFz82SMHjx4nIrQc46dOp8tb/Rn44Sds3Gb5Drdareb69atUqVrdmCaVSqlatTpXLptf0nD58kWqVqtuklatRk0uFzHtPjU1lePHjtCyVVtj2o3r10hOvo9UIuHDoYPp3fMNxn3xGTdjYyzQKuG/4KWa4viQXq9n165dbNu2jffff/+J+RMSDF8y3t7eJune3t7G9yQSCTt37qRz5844OTkhlUrx8vJi69atuLqan49fpkwZZs+ezciRI5kwYQI1a9akadOm9OzZk5CQEGO+Dz74gOrVq+Pm5sbBgwcZPXo08fHxzJgxA4CbN29iZWWFl1d+Z+Pqg2lv5csXXq/wMH3//v0maXPmzGHkyJFkZWURERHBjh07jFMgH/Lz8yu0Hq4glUqFSmV6s5Cn02Erffq+upWzCxIrazRK03/yGmUKMn/zc+mVe3Zi7eJC6PQfkEgkSKytub9xLfdWLDbmyb58kbjpU1DdvoWNmzvePfsROu0Hrgzugy6n6PUslmatUCCxtkadkmKSrk5JwS7Q/I1I2pHD+HR/k/RTp1HduY1zzVq4NmmK5Bl+rpbw8NqoUx+pe2oqMn/zdVf+/eDazPgx/9psWMu95X8Y89j6+uLRoRNJf60kcdkf2EdEUva9D9Fr1KTu2Fpi7bF2URh+1wq1JwV5gPnftdRdO7B2cSFi9lxje5LWrSFhySJjnoSlf2Blb0/FhUtBpwOplLvzfyFl5/YSa4uVkzMSKyu06UqTdE2aEjtff/OFCpCFhCPzDyJp/nfGtLz426jv38O9W1+SFvyATpWLok1nrN09sTYzNcfS3FwN3z+PrgtLVeYZ33uSRnU9cHSwZvOu/JvFC1fSyc3V8m7fEH7+IwYJMLhPCNZWEtzdnu68z0vq4ITEygpdpukovjYzDVsv3yeWt/EPwcbXn/Q/55VUFR9LmaNCq9fj/shURnd7GbGp5qcmB7k6MrZFVcI8XMhUqfnj5HX6/bmPP3s2w9vJrlD+jZficLCxplm5J/88/r9LUyYD4KxwN0l3dnEjLTXZbJmMDCU6nbbQVEYXFzcSbscaXy+bP51ykVFUr9PE7HmSEu+g1+vYuGoBbw74GHt7R1Yvmcu08UOYNGs51kWMYj1Vu9Iz0Ol0haYyuipcuHXnTrHPezfhHuu2bKdbpw707PYal69d5/t5v2FtbU2b5k2Kfd5HpaenGer/yH2gQuHK7bg4s2WUqakoFIpC+ZWP/J96aPfO7djZ2fNK/QbGtIQEw9KdpUv+oP/AwXh7e7Nm9SpGj/qYn+ctwMnp6R5wvUxEmH3LeqlG0DZu3IijoyNyuZy2bdvyxhtvMH78eIucW6/XM2TIELy8vNi3bx9Hjx6lc+fOdOzY0WSN26OGDBlCQkICS5YsoV69evz5559UrFiRHTt2GPMMHz6cJk2aEBUVxeDBg5k+fTrff/+9sSOUk5ODTCZD8uhk/gf1elo9e/bk1KlT7N27l/DwcF5//XVyc3NN8tjZ2ZGdnV3kOaZMmYKLi4vJMT/a/JeQJTlEVcXrjV7c+XEGV4cOIGbiGJxr18Przd7GPBnHj5C2bw+5MdFknDhG9BcjsXJ0RNGoWYnX73ndnDmD3Lg4qixfQe1/9hM04mPub9oIet2TC5cyx6iqeHd/i9vfz+DKe/2JmfAZznXq4d2zT34miZSca1eJX/ALOTeukbx5A8lbNuDRvlPpVbwIjlWq4duzN7dmTefioH7c+GI0LnXr4fNWX2Me1ybNcGvRipjJ47k4qB+xUyfj/XoP3Fq3LfrEpcypUStUt2JMA4potSTM/hIbnzIE/7SCkF9XY1c+iqwzx0rkv2XLxl5sX9nAeFhbF/5Oe1btW/pw5EQKySn5o4DKdDVffH2R+rXd2bGyAVtXNMDR0Zor1zPQvfx/UoBh9Ewdf6vIgCIvoyhfNzqUDyDC04UaZT34tn1tXO1k/HU+1mz+dRdv0TaiLDJrqxdb0X+BQ3u3MLh7Q+OhLTCrxpJOHd3LpXPHebP/iCLz6HU6tBoNPQd8QuVq9SgXUZl3RnxJYnwcl84fL7JcadLrdYSXC2Zg7zcJKxdMxzYtad+qBRu2ltxDtJKyY8c2mjRtZvJAXffg+/n17m9Sv0FDQsPCGTb8YyRI2L/vn9KqqvASealG0Jo2bcrcuXOxtbXFz8/vqQNdPFyblpiYiK9v/pO8xMREqlatCsDu3bvZuHEjqampODsbnkzMmTOHHTt2sHDhQpOIi49ycnKiY8eOdOzYkcmTJ9O6dWsmT55My5bmo/TUqVMHjUZDbGwsEREReHh4kJ2dTV5envEPNDw8HIBLly5RrVq1Que4dOmSMc9DDztUYWFh1K1bF1dXV9asWUOPHj2MeVJSUihXrugIR6NHj2b48OEmaVf+167I/OZo09PQazWFntBbK9wKjXQ85NO7P6m7t5OydRMAubHRSOVy/D/4hHvL/jA7iViXlYnqThy2fmWeqX7PS6NUotdosHEzfWpp4+aGOtl8+zRKJddGjURia4u1iwvqpCT83xtC7p0XF0kP8q+NjesjdXd1RZNi/imtT58BpO7aTsrWjcDDa2OH/4efkLh0Eej1aFKSyb1lOjKbe+smLg1Kdr68Jk1p+F0r1B63QiOcD/m9PZDk7dtI3rzBUM8Yw+9a4IhPSVi8EPR6yg4eQsKyxaT+vcuYx9bbB9833yJl25YSaYs2Ix29VouVs8Ik3dpFgTat6ClnABJbGY51G5G6enGh9/Jir3P7i/eR2tmDtTW6jHTKjJuBKuaaJasPwP6jyVy8mn9DZ2tjeMbnqrAhOTW/g+WqsOV6dOYTz+ftKaNmFVfGTLlQ6L1jp1J5Y9BRXJyt0Wr1ZGZpWbeoHncT7lmgJU+my8pAr9UidTQdGbBydEFnJqBLQRIbGfIqdcnc/ldJVvGxFHYyrCQSkrNNZ0wkZ6vwsC8cQdMcGyspEZ4u3E4rvH3LqTvJ3EzNZGqbmmZKClVrNzKJtKh5MA05XZmMwi1/GmJ6Wgr+weGFygM4OSmQSq1IV5p+16WlpeDsahiJu3T2OEkJtxnS0zQa9Q/fjCS8fFVGffkLLg8+z88/2Pi+s4srTk4KUp5zmqOLs2FGUqrS9G8iVZmG2yOjTM/C3dWVQP+yJmmBZcuw76Blp6E7O7sY6v9I8C+lMhVXN/OzEBSuroUCxSmVqSge+T8FcOH8Oe7cjuPTUWNM0t0e3F/4B+TPbLGxscXHx5ekpBfzHWdp/9W1YKXlpRpBc3BwIDQ0lICAgGeKQhgcHIyPjw+7duWH0U1PT+fIkSPUq1cPwDiq9Oh6L6lUiu4ZHslKJBIiIyMfu9/Y6dOnjVMoAWMn8WKBEO1Vq1YlMjKSmTNnFvr8M2fOsHPnTpOO16P0ej16vb7QdMXz58+b7fA9JJPJcHZ2NjmeZXojgF6jIfvaVZyqFli7J5HgWLU62ZcK32gBSGXywk/0H7bbzMgigFRuh61vmSI7FiVFr9GQdeUyzjVr5SdKJLjUrEXGefNrBYxl8/JQJyUhsbLCrWlTUl/wk7CH18ax0LWpQVZR10YuR//I76AxeuaDa5N14ZxxvdZDsrL+qBOffw3D4+g1GrKvXsG5eoGbQIkEp+o1yLxgPriPVC4rNHKpf+R3TSor3GZ0uiJ/Fy1Cq0EVex37ilXz0yQS7CpUJff65ccWdazdEIm1DRkH/y4yjy4nG11GOjbefsiCQ8k6afn1dDk5Wu7E5xqPmFvZ3E9RUbNK/o2MvZ0VFcKdOX+56LVLD7Vv4UNqWh6HjhX9N56WriEzS0v1KAWuLjbsP/qCvg+0WtR3YrANLbCWTyLBNrQi6pvXH1tUXqU2Emtrck4eKOFKFs3GSkqklwvH4pKMaTq9nmNxSVT2fbrpr1qdnuvJ6WY7dGsv3qS8lwvhni9oTeC/jJ2dA96+/sbDzz8EF1d3Lp7NX0+bk53JjavnCY0wv7bZ2saGoHKRXDybvx5ap9Nx6ewxQiMMa77ad+3DxFnLmDBzifEA6PH2cPp/MA6AsEjDWuiEO/kP2TIz0sjIUOLu+XzTU21sbAgPDeHkmfz/jTqdjpNnz1Ex0nzH82lULB9B3CMPOG/fjcfby7OIEsVjY2NDaGg4Z8/kby+h0+k4c/oUEUUEjYqMrMCZ06bbUZw+dZLIyMJLVrZv30JoaBjBIaYPzkPDwrCxseHO7fwZTBqNhnv3EvDy8n70NML/Qy9VB624JBIJw4YNY/Lkyaxfv55z587Ru3dv/Pz86Ny5MwD16tXD1dWVPn36cObMGeOeaDExMbRvnx/dLjIykjVr1gCGjlanTp1YtWoVFy9e5Pr168yfP5/ffvuNTp0MU7sOHTrErFmzOHPmDNHR0SxZsoSPPvqIXr16Gec0e3p6Ur16dZM1ZRKJhPnz53Px4kW6du3K0aNHuXXrFn/++ScdO3akXr16DBs2DIDo6GimTJnCiRMnuHXrFgcPHqRbt27Y2dnRrl3+6FdsbCx37twxCZZSUu6vXolb2w64tmiDzD+Qsu+PQCq3I2X7ZgD8P/4Mn36DjPnTjxzEvX0nFI2bYevti2O1mvj07k/6kYPGjprvgPdwqFwFG28f7MtXImjsZNDqSN3z4iNxxS9bhternfBo1w55YBBBIz9FKpeTtNEwyhQydhz+775nzO9QoSKujZsg8/PDqUpVImZ9BxIp8Yv/KOojSkzSXytwb9cB15YPrs0HD67NNsO1CfhkDL5vv2PMn374AB4dOqNo0hxbH18cq9fEt88A0g4fMF6be6tX4lC+Il7d38LWrwyKpi1wb9eR+xvWlHh7Ev9cgUeHjri1bos8IJCAjz5GKpeT/GA0Nmj05/gNGGzMn3bwAJ6vdsG1qaE9TjVq4ff2QJSH8tujPHQA3159cK5bD1tvHxQNGuHV7Q2U+0u2Q63cuganxq1xatAcGz9/PPoMQSKTk/GPYcq016DhuHXrU6icU+OWZJ08hM7MtgYOtRogj6yMtacP9tXr4jtyMlknDpNz/lShvCXhz/V36PNGAPVruxMS6MDnwyNJTlGx7/B9Y55Zk6N4rb1pGHaJBNq18GHr7kS0Zp6RtWvuTcUIJ/x85LRq4sWkTyuwct1t4u68uPWo2f9swb5OE+Q1GmLl5Yfza/2Q2MrIOWaIlOvS/R0c275eqJxdrSbkXjiBPvvJo4glqVe1UNZcuMmGS7eISclgyt9nyNFoebWCYf3m2O0n+P5A/oPDX45c4dDNe9xOy+LSPSVfbD9BQno2nSuarvfMVKnZee0unV/S4CBWDvY4V4nEuYoh+I59cFmcq0Qi9y+9tXISiYSWHXuw4c/5nDq6l7jY68ybNQ5XN0+TtWPffPEuOzflh2hv1akne3esZf/ujdyNi2HRT1NQ5ebQoLkhSJOLqwdlA0NNDgB3Dx88vQ2zT3zKBFKtdmOWzp/GtctnuH3zOr9+Nx7fMkFEVn7+EdBunTqwcfsutu7aw82428ycO4/cXBVtmhtG9b6a+T3zFubvZ6tWq7keHcP16Bg0Gg33U5K5Hh3DnbvxJue8eOUai1eu5s7deHbu3cfGbTvp1K7Nc9f3UZ27dGXb1s3s2rmduFs3mfPjbHJVubRoadhSYsa0r1m4YL4x/6udunDyxDHWrP6TuLhbLF28iOvXrtKho+mU/+zsLA7s20crM1Pn7e0daNuuA0sXL+LkyePcvh3HnB8M64sbNGhk8TYK/z4v1RTH5/EweMagQYNQKpU0aNCArVu3GvdA8/DwYOvWrYwZM4ZmzZqhVqupWLEi69ato0qBSHtXrlwhLc0wVF+2bFmCgoKYMGECsbGxSCQS4+uPPvoIMIxILV++nPHjx6NSqQgODuajjz4qNI1wwIABLFq0iKFDhxrTXnnlFQ4fPsyECRNo27YtGRkZBAQE0KdPH0aPHo1MZljcLZfL2bdvH7NmzSI1NRVvb28aNWrEwYMHTQKPLFu2jFatWhFYRCALS1L+sxsrFwU+b739YKPq68R8/rExcIitl7fJePfDqXI+fQZg4+6JJk1J+pGDxP+ev4DexsOTwFHjsHJyRpOmJOvCOa59NBht2osPtZ+yayc2rgrKDhiEjbs72deucvmjYcYpnDJvbwouhpHKbPF/ZzAyPz+0OTkoDx3kxoTxaDNf/A2acu9urF0U+Pbub7w20WOKvjYJSxah1+vx7TMAGw/DtUk7fICEBfnXJufqZWImjMH37UH49OpDXkI8d+Z+T+ruHYU+39JS/96FtYsCv74DHmxUfY1rn44w7kdn6+WNvsDobPwfhmmMfv0HYevhiUaZivLQAe7++osxT9zsmfi9PZCADz/GxtXVsFH1hnXEL1pQom3JOrKPZCcXXF/rhbWLK6pb0cR/O9YYOMTa3bPQulQbnzLYRVTi7tdjzJwRrBWueLw5ACsXBRplKhkHdpG6dnmJtqOgJX/FIZdbMXJoOI4O1py7mMaIcedM9kAr42OHwtk0EEHNqq74eMnZtMP8KGxAWXve6ROCs6M1CfdyWbTyFivW3S7Rtjwq98wRpA7OOLXuatio+u5NUn/9Bl2mYXTQSuFRaF6PlacvtiERpPwy9YXW1ZxW4WVIzVHx0+HLJGepCPd05vtOdY2h8xMyckzWRmeo8pi8+zTJWSqc5TZEein4rVtDQtxNAxZsv3YHPdA63HQK2svCpUYl6u3KfzhWYdpnAMQtWs3Z/qNLq1q069KHvNxcfp/zFdlZGYSXr8rwsbNN9kC7l3CbzAKBhOo0aEVGWiprl/1EWmoyAcHhDB/3PS6PBBt5koHDJrBs/gxmTRqGRColomJ1ho+dbZE9U5s1rE9aWjq/L11h2Kg6JIivx4/BzVVhaFOSIVrhQ8kpqQwcNtL4esWaDaxYs4EqlSow6yvDJs2RYaFM+uwT5i1awqIVq/D19mLIgL60bNIQS2vYuAlp6UqW/LGQ1NRUQkLKMWHiV8aH7ElJ95BI8+tfvkJFPh45msWLfmfR7wvwK1OGMV+MN+6B9tA/e/egR0+jJubX0ffrPwiplRUzp32NSpVHREQkk6d8i+Mj++j+W/xb1gf/W0j0zxKlQii2nJwcIiIiWLFihXHapSXl5eURFhbG0qVLqV+//jOVPdPmv/W0JjftxYe0LkkypxcTte5F0Kr/W9/gCn9FaVfBovqkjnxypn+JVZGlEz2xpDgGvTwbQVvC3mHrnpzpX8L17MsZaKO4gqT/rVDvmdaK0q6CxYSXMx+9+GXwcynGb3nH/Fai/2r/mRG0l52dnR2LFi3i/v37T85cDLdu3eKzzz575s6ZIAiCIAiCIDwPMdxjWaKD9gI1adKkxM4dGhpKaOjTbZ4qCIIgCIIgCMLL6T8RJEQQBEEQBEEQBOG/QIygCYIgCIIgCIJQbGKKo2WJETRBEARBEARBEISXhBhBEwRBEARBEASh2HRiBM2ixAiaIAiCIAiCIAjCS0KMoAmCIAiCIAiCUGylu62y5MlZ/mXECJogCIIgCIIgCMJLQnTQBEEQBEEQBEEQXhJiiqMgCIIgCIIgCMUmwuxblhhBEwRBEARBEARBeEmIETRBEARBEARBEIpNpyvtGvy3iBE0QRAEQRAEQRCEl4TooAmCIAiCIAiCILwkxBRHQRAEQRAEQRCKTQQJsSwxgiYIgiAIgiAIgvCSECNogiAIgiAIgiAUm06MoFmUGEETBEEQBEEQBEF4SYgOmiAIgiAIgiAIwktCTHEU0H//V2lXwaLsJf+tzTi88+JKuwoWY63NLe0qWNRfiQ1KuwoWNcH7v3N9/k5vVNpVsKiOZ0aXdhUsyvXs8dKugsWkRtUs7SpY1Pyx+0u7Chb1jfY/9Lcz/tfSrkGRRJAQyxIjaIIgCIIgCIIgCC8JMYImCIIgCIIgCEKx6Us1SoikFD+7ZIgRNEEQBEEQBEEQhJeEGEETBEEQBEEQBKHYRJh9yxIjaIIgCIIgCIIgCC8J0UETBEEQBEEQBEF4SYgpjoIgCIIgCIIgFJsIs29ZYgRNEARBEARBEAThJSFG0ARBEARBEARBKDadiBJiUWIETRAEQRAEQRAE4SUhOmiCIAiCIAiCIAgvCTHFURAEQRAEQRCEYhNBQixLjKAJgiAIgiAIgiC8JMQImiAIgiAIgiAIxSZG0CxLjKAJgiAIgiAIgiAUkJKSQs+ePXF2dkahUNC/f38yMzOfqqxer6dt27ZIJBLWrl37zJ8tRtAEQRAEQRAEQSg23X9wCK1nz57Ex8ezY8cO1Go1/fr1Y9CgQSxduvSJZWfNmoVEIin2Z4sOmiAIgiAIgiAI/0oqlQqVSmWSJpPJkMlkxT7npUuX2Lp1K8eOHaNmzZoAfP/997Rr145p06bh5+dXZNnTp08zffp0jh8/jq+vb7E+X0xxFARBEARBEAThX2nKlCm4uLiYHFOmTHmucx46dAiFQmHsnAG0aNECqVTKkSNHiiyXnZ3Nm2++yY8//oiPj0+xP1+MoAmCIAiCIAiCUGx6Xel99ujRoxk+fLhJ2vOMngEkJCTg5eVlkmZtbY2bmxsJCQlFlvvoo4945ZVX6NSp03N9vuig/Ys9aW7ruHHjGD9+vEU+a9vGv9iwehnK1BQCg8vR752PCI2oUGT+Q/t3s3LxryQlJuDjV5aefd+lWq16xvePHNzLzi1rib5+hcyMdL6evYCgkDCTcyhTk1n82xzOnjpGbk42vmUDeO313tSp3+S527Nl42rW/7Xc2J7+gz8k7DHtObjvb5Yvnk9SYgK+fmXo1W8w1R+0R6PRsGzRPE4dP0xiQjz2Dg5UrlqTXn3fwc3dw3iOu3fiWDR/DlcunUejVhMYXI7uvfpTqUr1527Po1Zv3s6ytZtIUaZRLiiAYQP6UCG8nNm8MbduM3/ZKq7ciCEh6T7vv92L1zu2NcmTnZPDr0tX8c+RY6SmpRMeHMQH/d+ifJj5c1raqi27WbJ+KynKNEID/Rne/00qhoWYzbtux1627D1EdNwdACJCAhn85msm+X9dsY4dB45yLzkFG2trQ54er1Ex3Pw5LU2v13Ng02zOHfgTVU46fiHVadl9PK5eQUWWObLtZ66e3k5KYjTWNnLKhFSjUeePcfPOr3NWWhJ713xD7OWD5KmycPMOpm7rwYRXa23Rum9cMYcDO1eTk51BSERVegwag5dv4GPL7d2ynB3rF5KuvE/ZwHBe7z+KoLDKxveTEuJYvWg6Ny6fRqPOo0LV+rzefxTOCndjnlvRl1i7eBY3r19AKpVStW4Luvb5GLmdvUXb9/fa7zn5z5/kZqfjH1qdDr3H4e4dVGSZfZt+5tKJHdyPj8baVo5/aDVa/m8EHr751+b4nhWcO7KR+JsXycvN4tMfjmJn72yxehdl5ZloFp24RnJ2LmEeLoxsEkUlHzezeddfvMmEHSdN0mytpBwamn+jUeO7NWbLftigIr1rhFuu4hiuxdplP7N3xxqyszIJi6zCW4NH4eMX8NhyuzavZMuaP0hTJhMQFEbPgZ8QEl7J7PlnTvqQcycP8v6oaVSv28Tk/f27NrBt/RIS7t7Czt6BWq+04K13PrVkE5/IrUFNQkb0x6V6JeR+Xhzv+h6J63e90Do8i06N7WlUTY69XMr1ODV/bMnkXoq2yPyvNrKnU2MHk7T4+xo+n5tqklaujDVdmjoQUsYGnV5PXIKGGUvTUGtKpBnIajXFrn5rpI4uaBLiyN6yDM2dmCLzS+R22Dfrgm356kjsHNClJZO1dQXqa+eMeaROCuxb/g+b0EpIbGzRptwjc90CtHdvlkwj/h95lumMo0aN4uuvv35snkuXLhWrHuvXr2f37t2cOnWqWOULElMc/8Xi4+ONx6xZs3B2djZJ+/jjjy3yOQf/2cWiX3+ga49+TP1uPoHBoXw1djhpylSz+a9cOsfsbybQtGUHps7+jVp1G/Ltl6O5FRttzKPKzSGiQhRv9n23yM/9ccZk7t6+xcgvpvLtjwupXa8RM78eS8yNq8/VngP/7GLhvB/p9mZfvpn9K0HBoUz+4uMi23P54jlmfTOR5q3a8+3sX6lVryHfTB5jbI9KlUvMjWv8r0cfvpn9K5+MMdR76sTRJueZMv5TdFot476axTffzSMwuBxTJowiNSX5udrzqF37D/HDgiX0feM1fp0+mdCgAEZMnEqqMs1s/lyVCl9vL955qzturgqzeb7+cR7Hzpzj8w/fZeGsqdSqWpmPxk8hKTnFonU3Z+eBo8xeuIL+3V7l92/GERbkz0eTZ5KSlm42/8kLV2jZoDY/jP+EX776DG8PN4ZNmsG95Pzr6+/nzYgBPVk8YyI/TR6Fr5cHH06eQWpaRom3B+Dojnmc2vMHLbuPp+cnK7GxtWPVD/3RqFVFlom7dpRqjXrS8+OVdHt/AVqthj+/70+eKtuYZ/OiT0m5F0OXwXPpO2YDYVVasmH+MBLjLlqs7jvWLmDP5mX0GPQ5n3y1GJnMju8nvYs6r+i6Hz+wlb8WTqN9t3cY/c1yygRF8P3kd8lIM/zuq3Kz+X7SYEDCh+PmMWLyQjQaNXOnvo9OZ3gsq0y5x+yJg/D08WfklMUM+XwO8XE3+OPHLyzWNoADW37lyM4/6NB7PAM+X4mtzI4/pg9A/ZhrE3vlGLWavcmAz1fQe8Rv6LQa/pgxwOTaqPNyCa3UkIbt37FofR9n+9XbzNh3jkF1IlnSoynhni4MXXuQlOyi2+Jga822AW2Nx8Z+pp37gu9tG9CWcS2qIwGahZaxeP03r1nIjo3L6T14NF988zu2cjkzJrz/2N+1I/u3s/y3mXTqPpDxMxbjHxTO9Anvk64s/F21fUPRC/23rVvMX0vm0O61vnw5eyWfTJhDpWp1LdKuZ2HlYE/62Suc/2DCC//sZ9X2FTta1Lbjj82ZfPlbKiq1nuFvumBt9fhyd+5p+GjGfeMx9Xelyfvlylgz7E0XLkTnMfm3VCbNV7LreG6JhXW3rVgLh9avk7NnA2k/T0SbGIdTr2FIHJzMF7Cywvmt4UgVHmSs/AnlD5+TuX4RuvT8/zkSuT3O/Ueh12rJWPIdyh/Hkr19JfqcbPPn/BfS6/WldjyLESNGcOnSpcceISEh+Pj4cO/ePZOyGo2GlJSUIqcu7t69mxs3bqBQKLC2tsba2jAO1rVrV5o0afJM9RQdtH8xHx8f4+Hi4oJEIjFJc3R0tMjnbFq7nOatO9K0ZXvKBgQzYMgn2Mrk/L1jo9n8W9b/SdUadXi165uU9Q/ijbcGElwunG0b/zLmadSsDf/r0Y/KVWuaPQfAlUvnadOxK6ERFfD2KUPX7n1xcHAk+vqV52rPhjUradGmA81atsM/IIhBQ0cgk8vZvX2T2fyb16+iao3adOrag7IBQfR4awDB5cLZsnE1AA4Ojoz9cgavNGxGmbIBhEdWZMC7w4i+foWke4kApKcpib97m87dehIUXA7fMv706jsYlSqXuJtFP5UrjhXrt9CxZVPaN29MsH9ZPh78NnKZjE279prNXz6sHEP6vkmLhvWwtS48qK5S5bH30DHe7d2DqhXLU9bXh7e7d6WMjzdrt+60aN3NWbZhO6+2aESHZg0I9vdj5KC3kMls2bh7v9n8E4YNomubZoQHBxBUxpfRg/ui0+s5fi7/iVjrhnWpHVWBMt6ehPiX4cM+b5CVncP1m3El3h69Xs/JvxdRt827hFZpgWeZSNr1+YbMtHtcP1P0z/N/Q+dTqd5rePiF4VU2krZvTSUj9S6Jty4Y89yNPkW1xr3wDYpC4eFPvbbvIbN3NsnzvHXfvWkJbboOpErtppQNCqfP+5NJS03izNHdRZbbveEP6rd4jXrNOuPrX44egz7HVibn4O61ANy4fJrkpLv0HjqJMoFhlAkMo8/QSdy6cZGr548CcP7EP1hZWfPGgM/wLhNEUGglegz6nFOHd3Iv/pbF2nd4xyIadRxMZLXm+PhH0GXA12Qo73H5ZNHX5q3hv1KtwWt4lQnDJyCSzm9PIS35Lndj83/u9Vr1oWH7QZQtV8UidX0ai09ep0vFIF6tGEiIuzOfNauK3NqKdRdiiywjQYKHg9x4uDvITd4v+J6Hg5w90fHULOtJWReHIs5YPHq9nh0bltHx9f5Ur9ME/6AwBn44kdSUJE4e2VNkue3rltCoVWcaNn+VMv4h9H53NLYyOft2rTfJdyv6CtvWLaH/+2MLnSMrM53VS+Yy8MMJ1GvcBi/fsvgHhVGtdmOLtvFpJG37h6vjZpG4ruS/a59Xi9p2bNyXzemredy+p2X+ugwUTlKqRz5+ZEOrg/QsvfHIzDG94X6jlSO7juWw5WAOd5O0JCZrOX5RhabogbnnIq/XEtXJfahOH0CbFE/WxsWgzkNWrYHZ/LJqDZDYOZCx/Ec0cdfRKZPR3LyKNvG2MY9dg7bo0lLIWrcAzZ0YdMr7qG9cRJeaVDKNEIrk6elJZGTkYw9bW1vq1auHUqnkxIkTxrK7d+9Gp9NRp04ds+ceNWoUZ8+e5fTp08YDYObMmSxYsOCZ6ik6aMJjadRqoq9fNelISaVSKletybXL5m/6rl4+T6VHOl5Vqtfh6uXzz/TZEeUrcWjfbjIz0tHpdBzYuxN1Xh4VK1d79oY8oH7QnqhC7anBlSLbc4GoqjVM0qpWr83VIvIDZGdlIZFIcHjQSXZydsGvbAB7d28jNzcHrVbD9i3rcFG4EhIaUez2PEqt1nD1Rgw1quRP55FKpdSMqsSFK9eKdU6tTotWp8PW1sYkXWZry9lLzzea+SRqtYYr0TepFVXemCaVSqlVuQLnr9x4qnPk5qnQaLU4O5q/gVSrNazdsRdHezvCgvwtUu/HSUu+TVZ6EoERrxjTZHZO+AZV4W7M00+LUOUYRvvkDi7GNL+Qalw5uYWcLCV6nY7LxzehUavwD6ttkbon37tDuvI+kVH5/5zsHJwICqtM9NWzZsto1GpuRV8iIip/9EEqlRJZuS4xVwxlNJo8JEiwtrE15rG2lSGRSLl+yfAzUavzsLK2QSrN/7dlY2u48btx+fmnkwCkJt0mMy2JkAr510Zu70TZkChu3zj91OfJfXBt7ApcmxdNrdVx+Z6S2gGexjSpRELtAE/OJRQ98p2j1tD+t620m7+V4RsOcSPZ/Eg1QHJWLvtjE+hU8fHTW4sjKfEOaanJVIzK/921d3CkXHglrl85Z7aMRq0m9sZlKhb4/ZRKpVSoUpvrV/J/P1WqXH6e8Tm9Bo3ExdWj0HkunD6CTq8nNeUenw39H8P7t2PON6NITip63cn/dx4KKQonKy7G5BnTclR6ou+oKVfm8atpvN2smD7MjalD3RjY2Qk35/y/cSd7CeXK2pCepWN0XwUzPnJnZG8XQv1LaIWOlRXWfoHkRReYdaDXkxd9CZuy5qfA20ZURXM7Gof2b+L68Qxc3puAXcN2UGAZik1EFTR3b+LYbTCun8zA5Z2xyKo3LJk2CBZRvnx52rRpw8CBAzl69CgHDhxg6NChdO/e3RjB8c6dO0RGRnL0qOFBoo+PD5UqVTI5AAICAggODn6mzxcdtP9nVCoV6enpJkfeY6aLpKenodNpcVGYrllwUbihTDU/NU+ZmoJC4fpIflfSzEwxeZxhn05Eo9HQv0c7enVpyrwfv2XEmK/w8Sv7TOcpKMPYHtP6KRRuKFPN18/Qnkfb71pk/rw8FYsX/ET9xs2xtzd0CiQSCeO+nEHMjWu89b829Ojcko1rVzJm4rc4OhUxbaIY0jIy0Op0uLmY3hi6KpxJLmKK45PY29lRKSKMhSvXcj8lFa1Wx7Y9+7lw9RrJqUoL1LpoSmN7TNfquD1De+YsXoWnq4JaUaZrDPcfP0OzXu/R+M3BLN+0g+/GjkDhbLlrUZSsdMMTU3tnd5N0eyd3stLvP9U59Dodf//1FWVCquPpl7/up2P/Wei0Gn4cWYeZH1Zm+7KxdB70A65elrmBTks11K/gujAAZxd30pXm656ZkYpOp8XZxbSMkyK/THBYFLZyO9YunkWeKgdVbjarF01Hp9OSrjT8vCIq1yZdmcyOdb+jUavJzkxn3ZLvTOr1vDIfXBvHR66Ng7MHmWlP9xk6nY6ty77CP7Q63mUtuybrWShzVGj1etztTUcv3O3l3M8y/50f5OrI2JbVmdGxLpNa10Snh34r95KYkWM2/8ZLt3CwsaZZaNHhposrTWn4/1L4d82NtCL+92RkKA2/a49+X7u4kV6gzLL50ykXGUX1Ok3Mnicp8Q56vY6NqxbQo/8Ihoz8mqzMdKaNH4JGrX6OVv13uTgabifTs0xHv9KzdDg7Fn2rGX1Hw2/r05m5NI0/NmfgobBiVB8FcltD58bT1TA/slMjB/45lcOsZWncjNfwcS8FXm5PmDtZDBJ7RyRSK/SZpg8m9FnpSBzNP3CxcvXAtkINkEhJX/IdOXs3Iq/XCrtGHQrk8UReqwnalETS/5hJ7vE9OLTtgazKK2bP+W+k05XeUVKWLFlCZGQkzZs3p127djRo0IBffvnF+L5arebKlStkZ1t+qqoIEvL/zJQpU5gwwXQu+ztDP2bwByNLqUZFW7H4V7KzMvh88iycnF04dngfs74ey4SvfyQg6MUEp3hWGo2GGVPGoUfPoCEjjOl6vZ55c2biolAw6ZsfsLW1Zde2TUydMJqvZ/2Mq1vhp7gvk88/fJcpP/xCl/5DsZJKCQ8JonmDV7h6w7LTMy1t0ZrN7DhwlDnjRyJ7ZASwRqVIFn47jrSMTNbt/IfPZ/zEr1PGFOoMPq+LR9ezY9k44+vX3vv5uc+5c8UE7t+9Ro/hpmtoDmz8jtzsdLq9/zt2jq5cP7OTDfOH0f2jJXiWefaR2qP/bGLZL5OMr98d/cNz190cJxc3Bgz/luXzvmTP5qVIJFJqNmiDf0h5JBLDzZ2ffyh9hk7ir4XTWLdkNlKplCbt3sRZ4Y60mJuBnj20gQ2L8q9Nz2E/PXdbNi+eyL0713h79JM3Mn3ZRPm6E+XrXuC1G//7Yyd/nY/hvXqFgyitu3iTtpH+yJ60yOgpHNq7hYVzvzK+Hvb5rOc+pzmnju7l0rnjTJixpMg8ep0OrUZDzwGfGNedvTPiS4b1a82l88epXK1ekWX/v6hTSUbv9vkPtL5bVrwHgOdv5I+43b6nJfpOGt984EbNCjL2n841DkLtPZnLgTOGBwu3EjSUD7alQVU5q3dnFb8RliKRoMtKJ2vDItDr0cbfROqswO6V1uTs3WDMo7kbS84uQ5AdbUIcVl5lkNVsjOrMwVKsvPA4bm5uj92UOigo6Ilr4J51jdxDooP2/4y5UKSX44qewuLs7IJUalVo9CtNmYLC1d1sGYWrG8pHAm6kKVMLjcI9TkL8HbZt/ItpPy7CP9AwrSAoJIzLF86wbeNqBg795KnPVZCTsT2m9VMqU1C4mq+foT2Ptj+1UH6NRsOMqeNISkpk/FezjKNnAOfOnOTksUP8vmKTMT0kNIIzp4+xZ+dWurzeq1jteZSLkxNWUikpaab/LFOV6bgrij/dqoyvNz98+QU5ublkZefg4ebKuGmz8fXxenLh56Awtsf0dzTlKdqzZN1W/lizmdljPybUzNRFO7kMf19v/H29qRRejm5DR7Nh1z76vNbeom0IjWqGb1D+uiOtxnBDkp2ejKNL/s8vOyMZr7KRTzzfzhUTiT6/hzc+WoyTa/5CZWXSLU7tXUzfMRvx8DNERPUqG8ntG8c5/c8SWvaY+Mx1j6rVxCTSouZB3dOVybi45k+dS09LpmyQ+Q6go5MrUqkV6Wmmox4ZymScFfkPJipUfYWJP24iMz0VqZUV9g7OjBrQDA/v/BHzWg3bUathO9KVydjK7JBIYNfGP0zyPIuIqk0pExJlfP3w2mSmJ+OkyL82Wen38QkoX6j8ozYtnsjVM3voN2oxLm7F3//GEhR2MqwkEpIfCQiSnJ2Lh8PTRTuzsZIS4enCbWXhm+BTd+5zMzWTqW0tM322au1GJpEWNer83zVFgQdY6Wkp+AebH5l0clIYftce/b5OS8H5wf+rS2ePk5RwmyE9m5rk+eGbkYSXr8qoL3/B5cHn+fnnT0lydnHFyUlBipjmCMCZq3lMuJP/c7a2NvSknB0kpGXm53N2kBKX8PShFnNUehJTtMbRsbRMw/DI3fum54i/r8Hd2fKTwPTZmeh1WiSOpg/qJA7O6DPNd0J1GWnodVoKRi3RJsUjdVKAlRVotegy0tAmxZuU0ybFIytv+SjOpaW4HRHBPDHF8f8ZmUyGs7OzyWFrW/Q/a2sbG0JCwzl3Jn+RpE6n4/yZE4RFVjRbJjyyEudPHzdJO3fqGOGRhcMcFyVPlQuARGr6KyqVWqF/js02bB6257Rpe86dPklEke2pyLkzpqGnz5w6RniB/A87Z/F3bzP2y5k4OZt2HozteeRJv1QiRWfBLzUbG2vCywVz4mz++jidTseJc+epGBH2mJJPx04ux8PNlYzMLI6eOkfD2jWeXOg52NgYQuAXDPCh0+k4fu4SlSKKHkVdvHYLC/7ayMzPP6J8aNBTfZZer0ddAjGbbeWOuHoFGg9331AcnD25eeWQMY8qJ5P42DP4BRe9vlKv17NzxUSun9nB6x8uROFh2ulU5xmmoZn/myne75jczgEv3wDj4Vu2HM4KD66cy9+kMyc7k9hr5wgJjzJ7DmsbGwJCypuU0el0XDl3hOCIwmUcnV2xd3DmyrkjZKSlEFWzSaE8zgp35Hb2nDiwDRsbWyKrFC+6nszOEXfvQOPh6ReKo4snMRfzr01uTia3o89StlzVIs+j1+vZtHgil0/upM/I33H1LP40bEuxsZIS6aXgWFx+EAKdXs+xuCQqFxFm/1FanZ7ryelmO3RrL9ykvJeCcE/LrLOzs3PA29ffePj5h+Di6s7Fs8eMeXKyM7lx9TyhEZXNnsPaxoagcpFcPHvUmKbT6bh09hihD37X2nftw8RZy5gwc4nxAOjx9nD6f2AYTQ2LNDxQSbiTH/48MyONjAwl7p6+Fmnvv11unp57qTrjcTdJizJDS/ng/HWkclsJIWVsuHHn6b9XZTbg5WpFWobh//x9pY7UdC0+7qajtD7uViSnlcDcNq0Wzd2b2AQXeCAjkWATEon6drTZIuq461i5eZmsObNy90aXoQStIZKJJu46Vu7eJuWs3L3Rplk2irPw3yFG0IQnat+5O3Nmfkm5sEjKhZdn87qVqHJzaNLCMNLww/RJuLl78mbfwQC0fbUbE0YNZcPqZVSv9QoH/9nJjeuXGTg0fxplZkY695MSSU02rOu4e9sQhU3h6obC1R2/soH4+JZl3g/f8tbbQ3B0duHYoX84d/oYn4795rna07HL6/wwYwrlwiIIDS/PpnV/osrNoWnLdgDMnv4l7u4e9OxrCIfd7tX/MW7UB6xfvZwateqx/59dRF+/wuD3DaN4Go2GaV99QcyNq4we9zU6rdYYOt/RyRkbGxvCIyvi4OjEDzO+oluPvtjKZOzcuoF7ifHUqGXZ6TJvvNqWr2b/TGS5YMqHlePPjVvJyVXRrrkhAtnk7+bi4ebK4Le6A4YgGbG3DdGm1BoNScmpXIuJxU4up6yvYRTgyKmzoNfjX8aXO/GJzFm4lICyvrRr1siidTenR8dWTPphPpHlgqgYGszyTTvJVano0LQ+ABNm/4qnuyvv9ewKwB9rNjNvxTomDBuIr6cHyamGp552chn2dnJyclX8/tdGGtaqirurC2npmazaupuklFSavVJ0VFFLkUgkVG/am8Nb5+LqFYiLe1kObPwORxcvQqu0MOZb+V0fQqu0pHoTw+jqzhUTuHx8I53fmYOtzIGsNMONt62dEza2ctx8QlB4BrJj6Vgav/Ypdg4Krp3ZSezlA7w2+PmnVT6se7P2Pdny1zy8fANx9yrDhuU/4uLqSZXazYz5vhs/kCp1mtGkbQ8AmnV8i0U/fEFguYoEhlbi702LUalyqNe0s7HMod1r8SkbgqOzK9FXz7Dqt29o1qEX3mWCjHn2bFlGSERVZHI7Lp85zOo/ZtK55wfYO1hmWqpEIqFuy978s/En3LyDcPUsw+41s3FSeBFZPf/aLPy2L5HVW1CnueHabFo8kXOHN9Ljgx+xlTuQ8eDayB9cG4CMtCQy0+6Tcs/wXXfv9lVs5Q64uPli76iwSP0f1at6KOO2n6C8l4JKPq4sPXWDHLWWVysY1iSO3XYcT0c73q9veNj0y5HLVPZxxV/hSIZKzR8nrpGQnk3nikEm581Uqdl57Q4fNTTfUbIEiURCy4492PDnfLz9/PHwKsOapXNxdfM0WTv2zRfvUr1uE1q0fwOAVp168ut34wkKrUBIWEW2b1iKKjeHBs07AuDi6mE2MIi7hw+e3oatAnzKBFKtdmOWzp9Gn/fGYGfnwKo/fsS3TBCRlUv+O6IgKwd7HELz932zDy6Lc5VI8lLSyI2Lf0zJF2/n0Rw6NLAnMUXLfaWWLk0cUGboOHk5fxT3414unLysYvdxw0PL11s4cPpqHslpWhROUjo1dkCngyMXco1lth7KoVNje+ISNcQlaHilihwfd2vmrCp69s/zyD20A8cub6O9exPNnRjkdVsgsZGhOnUAAMcub6NLV5K9yxDJWXVsD/LazbBv053co7uxcvPCrmF7co/k71eXc2gHLv1HYdewHaoLx7EuE4S8RiMyNywqkTaUBp0YQLMo0UETnuiVRs1JT1OycvGvKFNTCAoJZfTE6cYpfslJiSaR1SLKV+b9T8ax4o95LF/0Cz5+ZflkzBQCggps2npkP3Nn5a83+O4bw5PL//XoR7ee/bG2tmbU+G9ZuvAnvpn0Kbk5OXj7luG9j8aYbHhdHPUftGf54t+M7RkzcZqxPfeTEk3WtERWqMyHn4xl+R+/snThPHzLlGXk518a25OSnMTxI4Yv7o/ff9vks8ZP+Y5KUdVwdlEwZuK3LFs0j/GfDUOr0eAfGMzIL74iKCT0udrzqOYN6qFMz2D+8lWkpKYRGhzItLGf4vZgSmBiUrLJSN791FTeHj7G+Hr5uk0sX7eJqhXL8/3kzwHIys7m5z9WkJScgpOTI03q1mJgz9eNe3yUpBb1a5OansGvy9eSrEwnLMifmWM+ym/P/RSk0vz2rN6+B7VGw2fT5pqcp3+3VxnwRiekUik37ySwee8c0tIzcXFyoHy5YOZOGkWIv+X3cjKndsuBqPNy2L50LKqcdMqUq0HXIb9ibZM/UqG8H0dOVv5U3DP7lgGwYtZbJudq02sKleq9hpWVDV3f+4V/1k1nzU+DyVNl4+oZQNu3phJSyXLhwVt27odKlcPSnyeSnZVBuchqDP18jjGiIkBS4m0y05XG1zXrtyEzPZWNy+cYNqoOimDomDkmASAS78aybulssjLTcPf0o03XATTrYNrW2Gvn2bRiLqrcbLzLBPPmO59Tp3FHi7UNoH7bAeSpctiwcCy52ekEhNWg1/B52BS4Nin3bpGdkX9tjv9tuDa/f93b5Fyd3v6Kag1ee5BnOXvX/2h8b8HUXoXyWFqr8LKk5qj46fAlkrNVhHu48H3nV4yh8xMycky+CzJy85i86xTJ2SqcZTZEein47fXGhLibdoC3X72NHmgdUbIjhe269CEvN5ff53xFdlYG4eWrMnzsbJPftXsJpr9rdRq0IiMtlbXLfiItNZmA4HCGj/seF4X5KflFGThsAsvmz2DWpGFIpFIiKlZn+NjZL+Q7ryCXGpWot+sP4+sK0z4DIG7Ras72H11UsVKx5WAOtjYS+rR3wl4u4dotNTOXppmEw/d0tcLRPv9+wdVZyjuvOf1fe/cdFdXRBnD4x9J7UURBVBQQ7L333jX6mdhiLzEx9liS2KNRE3tPAvbee2+x1yhWFBU1KjZ6L7vfH5jVlUUBgQV8n3P2HPfe2cs7zu7efe/MncHcVEFYpBK/x3FMXhpEeOTbX/uHzkdhaAAdGlpgbqrg8fN4ZqwO5mVQxswOEXvjApHmFpjWbY3Cwor4gMeErZqNKiIxIVRY59IYlaAMDSJs5SzMmnyFTf/xKEODiD53iKiTe9VlEp76E7Z+IWb122JauyUJQa+I2LeO2HdGFgjxLj2VDBr97F25m7PW4dDXy8ApfXTAITbj1+bKLAYJ0R8vlI1sfq59XZzsqrBDzmmfF6Epu88qu2h5NWv9GP9UPnV/+nihbCKoVOb2qmW0LWO1rzGZXU1PyDmfnVzj/9J1CMn6eVnsxwtlkF+6G328UDYjPWhCCCGEEEKINFPJGMd0JZOECCGEEEIIIUQWIT1oQgghhBBCiDSTG6bSl/SgCSGEEEIIIUQWIQmaEEIIIYQQQmQRMsRRCCGEEEIIkWZKmSQkXUkPmhBCCCGEEEJkEdKDJoQQQgghhEgzWVY5fUkPmhBCCCGEEEJkEdKDJoQQQgghhEgzlVLXEeQs0oMmhBBCCCGEEFmEJGhCCCGEEEIIkUXIEEchhBBCCCFEmillkpB0JT1oQgghhBBCCJFFSA+aEEIIIYQQIs1kmv30JT1oQgghhBBCCJFFSIImhBBCCCGEEFmEDHEUQgghhBBCpJlSKUMc05P0oAkhhBBCCCFEFiE9aEIIIYQQQog0kzlC0pckaAITRYyuQ0hX1nGvdB1Cunqo76rrENJNnJ6+rkNIV+3ynNB1COnqorKyrkNIN442UboOIV3drj9S1yGkq0I80HUI6cZr7Eldh5Cu2k6soesQ0lXctXW6DkGIVJMETQghhBBCCJFmKrkHLV3JPWhCCCGEEEIIkUVIgiaEEEIIIYQQWYQMcRRCCCGEEEKkmVJmCUlX0oMmhBBCCCGEEFmE9KAJIYQQQggh0kwmCUlf0oMmhBBCCCGEEFmEJGhCCCGEEEIIkUXIEEchhBBCCCFEmskQx/QlPWhCCCGEEEIIkUVID5oQQgghhBAizaQDLX1JD5oQQgghhBBCZBHSgyaEEEIIIYRIM7kHLX1JD5oQQgghhBBCZBGSoAkhhBBCCCFEFiFDHIUQQgghhBBpplLJEMf0JD1oQgghhBBCCJFFSA+aEEIIIYQQIs2UMklIupIeNCGEEEIIIYTIIqQHLQvq3r07y5cvB8DAwID8+fPTvn17Jk6ciImJCQB6enps3bqVNm3aJHltcHAw27ZtS9eYdu/cxrbNGwgKCqSQSxH69v8e96IeyZY/deI4q1cu5cXzABwd89O1Zx8qVKysUebxo4csX/onN675kJCQgHOBgoz6aRz2eRwA+GnkUK5fu6rxmsZNW/Dt90PStW4AW3fvZ922nQQGBeNaqCAD+/bA091Va9kHjx6zdM0GfO894PmLl3zXqyvtWzXXKPNVnwE8f/EyyWvbNG3E4G96pXv8KpWKzWv+4OiB7URGhOPuWYoe/UeQ17HAB193cPdGdm9dTUjQawq4uNG17zCKuBcHIDwshM1r/uTalXO8fvkcKysbylepzf8698PM3AKAhw/usHPTCu7cukpYaAj2efJRr8kXNGnV4ZPrs23tEo4f3EpkRDhuHqX5+ptRH63P4T0b2Lt1JSHBrylQyI3OfX6gsHsJrcefNWkQ1y6f5vtRv1OuSh2N/ScP72T/jtUEPH2EqZk5Fas14Ot+Iz+pTu/atO8Iq3fsJzA4BNeCzgzt2ZHiboW1lt1+6G/2Hj/D/cdPAChauCDfdPxCo/xfG7Zz8NQFXrwOxNDAQGuZjKJSqdizYQGnD28mKiIMF48yfNV7DHnyFUz2NX43L3J4xzIePbhJaNBLeg+fTelK9dX7E+Lj2LVuHjf+OcHrF08wMbOgaMkqtO40GGu7POke/451izhxcCtRkWEU8ShN574/4uCYfPwAR/eu58C25YQEvyZ/IXc69h6Ji1vie+3Vi6f8+E1zra/rO3w6Fao1TPx327JJ9vce+iuVajRJceyb1/zJ0QPbiYgIx92zJD1T8Lk/sHsTu7euIiQokAIurnR753MPEBsbw2rvuZw9cZC4uDhKla1Mj29+wNo2l7rM9asX2LT6Dx4/vIexsQk16zXjy6+/QV8/8WfFzWuX2Lt9Hffv3iQqMgIHR2dafNGZ6nVSVjdttu7ex/qtOwgMCqaIS0EG9u2Jp7ub1rIPHj1m6er13Ll3/833dHf+1zppm7x8/Zo/lq3m/OV/iI6JwSlfXkYO/I6ibkXSHGdqtK5tRq2yJpiZKPB7HMfKveG8CExItnyrWma0rm2use3Zq3h+XhSksa2IkwFf1DWnsJMhSpWKxwHxzFwTQlx8hlQjxexqVKDwsF5YlyuBiWMeLrb7luc7Dus2KC3e/iYIoUihAgz6yG8C7zUbuXPvPgEvXjGgV1fat2qmUSYhQcmydRs5cOwkgcHB5LazpUm92nT9si16enqZUSWRzUiClkU1adKEpUuXEhcXx6VLl+jWrRt6enpMmzYt02M5cfwo3n8upv+Awbh7eLBz2xbGjxnJwj+WYWNjm6T8rZs3+H3aL3zdvTcVK1Xh72NH+HXSWGbOXUzBQi4APHv2lNE/DKJBo6Z06tINUzNzHj30x9DISONYjZo0p1OX7urnxibG6V6/IydOs9B7BUP798bT3Y1NO/fww/gprFw4C1sb6yTlY2JiyOfgQO1qVVjgvULrMZf8PoUEpVL9/MHDRwwfN5na1auke/wAu7as5MCuDfQbNBZ7B0c2rV7CtHGDmLZgHUZG2v/Pzp44yGqvOfT4diSu7sXZt2Md08YN4rdFG7C2sSMo8BXBgS/p1GMgTs4uvHoRwNJFUwkKfMmgUVMB8Pe7jZWNLf2HTiBXbgfu3PLBe8GvKBT6NGrRPs312bN1OQd3raP3oPHYOzixZc0iZk74nsnzNmCYTH3OnTzAOu9ZdO0/msLuJTi4Yy0zJnzPrws2Y2Vjp1H2wM41yf7t/dtXsW/7ar7sNogi7iWIiYni1Yunaa7L+w6dOs/c5RsY0bcLxV0Ls373IYZMns26Ob9gZ22VpPzlG740rFGJku5FMDIyZNW2vQz+ZRarZ04kT67Ez59zvrwM69UJJwd7YmJjWbfrIIMmzWLjvCnYWlumW+xa67Pdm+N719Dlu1/IlceJ3evns3ByP36auT3ZtoqJicKpkDtV6n3BX78PTrI/Njaaxw9u0aRdP5wKFSUyPJTNy6axZPr3jJi6Pl3j3791GUd2r6XHwInkzuPE9rULmTPpOybM2Zxs/BdO7mfj0hl07vcTLu4lOLxrDXMmfsvEeduwsrHDLpcDv3kd1HjNiYOb2b9tBSXKVtfY3n3ABIqXraZ+bmae8vbatWUl+9987vM45GPj6j+YOm4w0xesTfZzf+bN577ntyMp8uZzP3XcYH5ftB7rN5+TVX/N5srF0wwcMQUzcwuWLfmdWb+OYvz0PwF4+OAuv00YSusvu/PN4LEEBb7Ee+E0lEolnXsOBODurWsUKORKy3ZfY21jxz8XTrFo9kRMzS0oV7FGiuv4nyMnTrHIazlDvu2Lp7srm3bsZsS4yaxYNCfZ72nHvHmoU70qC7yWaT1mWHg4348cQ9mSxZk67kdsrKz491kAFhbmWsunt6bVTGlQyRSv7WG8Ck6gTR1zhnay5udFgcQnn6Px5EU8v68KVj9/51QDJCZngztZs+dUJGv2h5OgBGcHA7LCHA765maE+vjyeNlmKmxaoOtwtDpy4jQLvFcytH9virm7snHnHoaP/5VVC2dqfa9Fx8Ti6JCHOtWqMD+Z3wRrtmxn+95DjB7cn0LO+fH1u8/UuYsxNzPjfy2bZnSVMoVMEpK+ZIhjFmVsbEzevHlxdnamTZs2NGjQgIMHD378hRlg+9ZNNGrSjAaNmlCgQCH6DxiMsbExhw7s01p+5/YtlCtfkbb/+wrnAgXp3LUHhYu4sXvnNnWZVcu9KF+hMt179aNwETfy5XOkcpVqSRI+Y2NjbO3s1A8zs/Q/cW7cvpvmjerTtEFdChXIz9D+vTExNmLPoaNay3u4udK/Rxfq16qOoaGh1jI21lbksrVRP85cvIxjXgfKlCiW7vGrVCr27VhH6y97UL5KbQq4uPHNkPEEB77i0tnjyb5u7/a11G3UmtoNWuJUoDA9vh2FsbEJxw/tBMC5YBEGjZ5GuUo1cciXn+KlK9C+S3/+OX+ShITEy7C1G7aia59heJYoR568TtSo25RaDVpw8Yz2/7uU1ufgzrW0/LIX5SrXwbmQG30GTSQo8CWXzx1L9nUHtq+mVqM21KzfCifnwnTtPxojYxNOHN6hUe7RfV/2b19Nr+/HJjlGRHgoW1Yvos+gCVSt3YQ8+fLjXMiNspVqp7k+71u76yCt6tekRd0auDg7MqJvF4yNjNh15KTW8hMG9aFd47q4uxSgkFM+Rn/THaVKxcXrt9RlGtesTKVSxXBysKewsxODun1FRFQUfo/+Tbe4tVGpVBzbs4rGbftSqmI9nAoW5esBUwgJeonPhSPJvq542Zq06DBQo9fsXaZmlgwY8yflqjXBwdEFF/fStO/5I4/v3yTw1bN0jf/QrjU0/18fylSqS/5C7vQYOIngwJf8cz759/DBnauo0bAt1eu3xtG5CJ37/YSRsQmnjmwDQKGvj7Vtbo3HP+eOUqF6Q0xMzTTram6pUS65pFBb7Pt2rKfNlz2oUKUWBVzc6D9k3JvP/d/Jvu7t574F+Qu40PPbkW8+97sAiIwI59ihnXTuNYjipSvg4upBv0E/c/f2Ne7evg7A2ROHKFDIlbYdepHX0RnPEuXo2H0AB/dsJioyAoDWX3anfZd+uHuWwiFffpq0+orS5apw8fSxFNXvfRu373rne9qZod/2xcTYiL2HtL/PPNxc+aZHV+p94Ht67eZt5Mmdi5GDvsPT3Y18eR2oWLY0TvnypinG1GpQyZRdJyK5cieWf18k4LU9DBtLBeU8PvweSFBCaIRK/QiP0vxh/FUjCw5fiGLv6Sievkzg+esELt6M+WDSl1le7v+bO+Nm83z7IV2HkqwN23fTolE9mjWoQ6EC+Rmm/k1wTGt5T7cib34TVMPIUHu/x43bd6heuTxVK5Qjn0Me6lSvQsWypbh9914G1kRkZ5KgZQPXr1/n9OnTGL3Xu5QZ4uLiuOd3h9Jlyqm3KRQKSpcph+/tm1pf43v7JqXLltfYVrZ8BXV5pVLJxQvncHTKz7ifR9K1YzuGD/6Os6eT/kA9fvQwXTp8wff9e7Fi6V/EREenY+0gLi4e33v3KV+6pHqbQqGgfOmS3PS9m25/4+CxkzRrUDdDhjK8fP6UkKDXlChdSb3NzNyCIu7Fuet7Tetr4uPieOB3m+Jl3r5GoVBQvHRF/G5rfw1AZGQ4pmbm6mFMWstERGBumbQnKKVePn9CSNBripd6vz4l8PtAffzv3aZ4qbfDaBUKBcVKV8LP10e9LSYmmiUzf6ZL3xFY2+ZOcpwbV86hVKkICnzBjwP+x9BezVg4fRSvXwakuT7viouLx/f+QyqWepuoKxQKKpby5Pqd+yk6RnRsLPHxCVglc5U/Li6ebYf+xsLMFLeC+dMl7uS8fvEvocGvKFrqbc+wqZklhVxL8uDO1Q+8MvWiIsPQ09PD1Cz9egRfPX9CaPArPEu/fd+YmVvi4laC+++8b94VHxfHo3u38HzvveZZqnKyr3l47yaPH/hSo36bJPvW/vkrQ7rVZcqILpw8vC3FV6FfPn9KcNBripeu+E7sKfnc+1KizNvXKBQKSpSuyN03n/sHfrdJiI+nxDvHdcxfiFz2edWfv7i42CSjHYyMjImLjeHBvdvJxhwZEZ6m74a4uDju+N2nfJlSGnGXK12KG7fvpPp4/zl9/iJFXYswfuoMvvi6F30G/cCu/ZmTOOS2UWBjqc/NB7HqbVExKu4/iaOI04cHNznY6TNjsB1TB9jRp40ldlZvf8pZmulRJL8hoRFKRne3YeaQXIzoao2rswyYSom4uHju3Hug9TfBDd+0v9eKe7hz2ec6j58kjsbwe/CQazd9qVyuzKeGnGWolCqdPXIi+cRmUbt27cLCwoL4+HhiYmJQKBTMnz9fo0zHjh3R19fX2BYTE0Pz5trvfUiL0NAQlEolNraaPVs2Nrb8+/ix1tcEBwUm6QmzsbElKCgQgJDgYKKjoti8cR2du/agW48+XL50gamTx/PL1BmUKFkagFp16mGfxwE7u1z4+99nhfefPHnymNE/T0i3+oWEhqJUKrF7b9iCrY01j/5Nn2FtJ89dIDwigib10q8X5l3BQa8Bkgzjs7KxI+TN//n7wkKDUSoT1EOa/mNtY8ezJw+Tfc229d7Ubdwm2Vju3PLh3MmDDB87MxU10BQS/F99cmlst7K2I+RNXZPEFpZYn/f/D6yt7Qj411/9fK3XDIp4lKJc5Tpaj/Py+RNUKiW7Ni2lU+/hmJlZsGX1In4f/x2TZq/DIJkr8SkVHBZOglKZZCijnbUVD5+kLAlcuGoT9nY2VCyp2Rt78tJVxs76g+jYWHLZWDNnzFBsrDJ2eGPom7aytNZsK0vrXIQGv0q3vxMXG8OO1bMoX70ppmYW6Xbc/2K0tH7/s5OL0GTea+FhQVrfa5Y2uXj2xF/ra04e2ka+/C4U8Sijsb1Vh/54lKyEkbEJN6+cYc0fvxITHUn95p0+Gvt/n3ttn+Hg5D4nyXzurWxsefom9uDg1xgYGGJuofneefe4pcpVYd/O9Zw+foAqNeoTHPyaLeu8E18fqP1vnz15iPt3b9Hru1Efrdv7QkLDUCqVSYaX2dpY8+jJk1Qf7z9PA16wfe8B2rduQef2bbl91495f3pjYGBAk/p10nzclLC2SEyqQiM0f1yGRiixskj+2vn9J/F47wgl4HUC1hYKWtUyZ1Q3G8YuCSI6VoW9beJvgta1zNlwKJzHzxOoWtKY4V0Sy3zo/jaR+JsgIbn32r9pf691bteayMgovv5uGAqFAqVSSe8uX9GwTuqH+4rPgyRoWVTdunVZtGgRERERzJo1CwMDA9q1a6dRZtasWTRo0EBj28iRI0lISP4LOCYmhpiYGI1tsTExGBmn/71dyVGqEgfMV65SjdZf/A+AwkVcuX3rBvv27FQnaI2btlC/ppBLYexsczHmx+E8e/aUfPkcMy3eT7Xn4BEqly9D7lx2Hy+cAqeO7cN74VT1809JhlIqMjKc3ycOxcnZhbYd+2gt8/jhPWZN/oEvOvSmZNmU32t35vheli+aon4++OfZnxquVv+cP86taxeZMHN1smVUSiUJ8fF07v0DJd7Uod+wyQzu0Zhb1y9SsmzVDIktpVZs3cPBU+dZOOEHjI00k8XyxT1Y/ttYQsLC2X7oBD/PXMJfv/6o9b62tLpwYhfr/piofv7N6Iy/hyQhPg7vWcNRAV/2HvNJxzp3fA+rlvyifj7gp7mfGN3HxcZEc/7EXpq3T/q5afFlX/W/CxT2ICYmigPbVmhN0E4d24fXwrf3IP8wdkbGBJwCpcpWplP3AXgvmsaiWRMwNDSkzVc98b15BT1F0lECN3wu8cecX+g9YDT5C2T8xDUppVIpKepahD5dE/+/3Yq48ODRY3buO5DuCVrlEsZ0bf426Z2zNiRNx7l+722P278vErj/JITpA+2oUMyYk1ei+W+QxvHL0Zy6mniufxQQj6eLETXKmLDlSETaKyHS7OjJsxw8fpIxQ7+nUIH8+D3wZ77XCvVkIUK8TxK0LMrc3BxX18QZg7y9vSldujReXl706vV2BsC8efOqy/zH0tKS4ODgZI/766+/MmGCZg/Ud98PYcCgoVrLW1lZo1AoCA7SnCEqODgIWzvtCYeNrR3BwVrK29qpj6mvr49zAc1Z0pydC3DzxvVkY3f3SJw18tnTJ+mWoFlbWaFQKAgM1jxZBgWHYGdr88nHD3jxkks+15g4atgnH+s/5SrV1JhxLT4+DoDQ4EBs7d4O2wsNDqRAYe0znFla2aBQ6BMSrNnDFhIcmOTqelRkBL+NH4yJqRmDf5yGgUHSr40nj+7z68/fUbdxG9p81TNV9SlTqZbGTIvxcbFv4n+Nzbv1CQnE2cVde30sE+sT+n59QgKxejPz3C2fi7wM+JfvOtfVKDN/+gjcPcswavIfWL/5e47OLur9Vta2WFraEJgOwxxtLC3QVygIDAnV2B4YEkouLTefv2v1jv2s3LaXuWOH4VrQOcl+UxNjnPM54JzPgRLuRWj//Y/sPHKSbl8003K0tClZoS6F3N4OM/uvrcJCXmNta6/eHhbyGqdCyc/ymlL/JWeBr54ycKzXJ/eela5UGxeN91riZycsJBAbu7fxhwa/xtmlqNZjWFjaan2vhQW/xvq9Xl+AS2cOERsbTdU6LZLse5+LW0l2b/wzcQihoeYQwuQ+9yHvfe5DggMpmMrPfWhwkDp2G5tcxMfHEREeptGLFhIciM07szg2a9OJpq07Ehz4CnMLS16+eMb6FQvJk9dJ49i3rl9mxi/D6dJrMDXrpe29aG1liUKhIEjb97SNTZqOCZDL1paCzprDgAvmd+LE6bNpPmZyrt6JZcKTt//vBgaJmZSVuR4h4W/LWZkreByQ8qkWo2JUPA9MII9dYs9ZSHjiBdCnrzSP8exVPLms5K6Wj7G2skI/uffaJ/wmWLRsFZ3btaZ+rcQJgYoUKsDzl69YvWl7jknQcupQQ12RT2s2oFAo+PHHH/n555+Jior6pGONHj2akJAQjUffb75LtryhoSFFXN3xufqPeptSqcTnyj8U9dA+4UVRj2L4XLmsse3KP5fU5Q0NDXF1L8qTfzWHSD558i953kyxr82De4k309olkximhaGhAUWLFOayz9t7NpRKJZd8rlOsqPYfOamx9/AxbKytqVKh3McLp5CpmTl5HZ3VDydnF6xtc3Hj6gV1mcjIcO7duYFb0ZJaj2FgaIiLq4fGa5RKJTd8LuDq8fY1kZHhTBs3EH0DQ4b+/LvWmeH+fXSfyT99S816zfny6/6pr4+pOQ75nNUPR+fCWNvm4qbP29iiIsO5d+c6rh+oT6EiHtz0Oa9Rn1s+F3AtmphQNG/XjYmz1zJh1mr1A6Bjz6H0GjgOADePxN7bgHeGeYaHhRAWFkwu+3yprtv7DA0Tp8C/eO3tBB9KpZKL125Twj35noVV2/eydNMuZv00GM8ihVL0t1QqFXFvEpD0YmJqjn3eAupH3vxFsLLJje+1c+oyUZHh+Ptdw8W99Cf9rf+Ss5cBjxgw5k/MLW0+MfrE+PPkK6B+5HMujJVNbm75aMb/4O51ChctpfUYBoaGFCjiye13XpP4Xjuv9TWnDm+jdIXaSYZRavPY3xczC6skyRlo/9zbJPncR6Tgc180yef+us8F3N587l1cPdA3MODGO5+/p/8+5PXLgCSfPz09PWxz2WNkbMKZvw+SK7cDLoXfJrY3r13it4nD6NDtO+o1afPR+ifH0NAQd9fCXL6q+T192ecaxT20X7RJieKeRdX3BP3n36fPcMhjn8wr0i46VsWLIKX68fRlAsFhCXi6vG1rEyM9CjsZcu9JyhM0Y0PIY6tPSFhiYvYqWElQaAJ5c2ne/pA3lz6vQ5TaDiHeYWhogHsRFy75vL1YnPheu07xoml/r8XExibpXVYoFOoRRUK8T3rQson27dvzww8/sGDBAoYPH57m4xgbG2P83nBGI+PQZEonav3F/5gzcxqubu64uXuwc/tmomOiadCwMQCzfp9Krly56dqjNwAtW7flp5FD2LZlAxUqVuHE8aPcu3uH775/20v3Rbuv+H3qJIqXLEXJUmW4fOkCF86dYfK0xOF6z5495e+jhylfsTKWVlb4P7iP9x8LKV6iFIVc0nd9mvatm/PrnIUUdS2Cp1sRNu3cQ3R0DE0b1AFgyqz55M5lR983w2Di4uLxf5w4O158XDyvXgdx974/pqYm5H9n9i+lUsm+w8doXLc2Bu/dK5ie9PT0aNKqA9s2LMXB0Zk8b6bZt7HLTfkqb6/MTfn5OypUqaOe/r5p644smT0RF1dPirgXY9+OdcRER1O7fuKV/sjIcKaNHUhsTAz9h04gKjJCPUOblZUNCn19Hj+8x68/f0fJspVp2qaT+h4VhUKBlXXSJRhSWp+GLTuyc6MXDo7O5M7jxNY1i7C1s9e4d2z6mP6Uq1KHBs2/AqBR6878NWc8hVyLUditOAd2riEmOooa9VsCqGfJe1+u3Hmxd0i86p/XqSBlK9VmjdfvdPv2J0xNzdm0cgH5nArhUbJCmurzvo4tGjJpgTceRQpS3NWFdbsPER0TQ4u6idOvT5jnhb2dDd92ThzSvHLbXv5cv50Jg/qQzz43r4MSr+yamhhjZmpCVHQMy7bspmaF0uSytSEkNIxN+4/yMjCIelXTJ+bk6OnpUadZF/ZvWUKefAXIlceJXevmY21rT6mK9dTl5k3sTalK9ajdJPEzFBMdycuAR+r9r1884V//25hZWGOXOx8J8XF4zRzK4we36DdyASqlUn2/mJmFNQYGn3Yv4LvxN2jRiT2b/iJPvgLkdkicZt/Gzp6yld72tM4c148yletSr1ni+n4NW3Zh6byxFHQthotbCQ7tXENsTBTV67XWOP6LZ4+4e/My3/80L8nfvnrhOKHBrynsXgpDIyNuXj3L3s1eNGrdNcWxN2n1Fds2LCOvo/Ob5TX+ePO5r6UuN+XnAVSoUvu9z/2kdz7369987hPvXTYzt6BOg5as8pqLuYU1ZmbmLP9jBm4eJXHzeNv7uGvLKkqVq4JCoeDC6WPs2LyCgSMmo3jzXXfD5xIzJg2jccuvqFStrvq7wcDAAAvLD/cWa9O+dQumzl6Au2sR9TT70dExNKmf2E5TZs3D3s6OPt06A4kTizz873s6Pp5Xga/xu/8AUxMTnBzzqY85YMTPrNqwhbo1qnLrrh+79h9i6Hf9Uh1fWhw6H0WLGmY8D0zgVXACX9QxJzhMyeXbb29DGN7Fmsu3YzhyMXGCrC8bmHPlTiyvQxKwsVTQurY5SiWcu/F2Aq19Z6JoXduMx8/jeRwQT7XSJuTNZcDCTR8+12cGfXMzzF3frtNn5pIfq9IexAaGEP04/WZo/RRftm7Or3MW4eFaGA83Vzbt3ENUdAxNGySeTyfPWoB9Ljv6du0IaP4miItL4NXrwCS/CapVLMeqjdtwsM9NIef83L3vz4btu2n25ndGTqCUafbTlSRo2YSBgQEDBgxg+vTp9O+f+l6KT1Gzdl1CQ0NYs3IZQUFBuBQuwriJU7F5M2Tx1csXKN65MuRZrDjDRvzEqhXerFzmjaOTE6PHTFSvgQZQtVoN+g8YzKYNa/lz8Xyc8jsz6qfxFCteUl3fq1cuJyaD0dHkts9D1eo1+bJjl3SvX72a1QgODWXpmg2JC1W7FGL6uNHqoTPPX71GT/G2s/lVYCB9hrxdtHj9tp2s37aT0iWKMWfyOPX2S1ev8fzlq0z5Am7R9mtioqPwXvBr4kLVxUozYvwcjR6vFwFPCAsNVj+vUrMhoSHBbF7zByFBrylY2J0R42erF6P1v+fLvTs3ABjW7737H//cir2DI+dPHSE0JIhTx/Zx6tjbZRdy58nH7L+2pbk+zb7oRmx0NMsWTiEyIgx3zzIMHTtXYwryFwH/Ev5OfSrXaERYSBDb1i5+s/C2O0PHzdM67OxD+gyewFqvmcyeNBg9hYKixcsxdOxcrUM706JB9UoEhYbz1/rtvA4Oxa2QM7N+GqyeqOb5q9co3pntc8uBY8TFx/PjjEUax+nVviW9v2yNQqHg4ZNn7Dl2mpCwcKwtzfEs4sKiiSMp7Kw53CwjNGjdk9iYKNYumUBUZBiFPcry7Y+LNdrq1fPHRLzTVo/u3WDuhLdDYbeu+A2ASrVb8fV3kwkOfMG1i8cAmDbifxp/b+A4b9yKVyS9NP6iOzExUaxa/AuREWG4epZh0JgFGvG/DHis8V6rWKMxYaFB7Fi7iNDg1+R3KcrAMQuSTGxz6vB2bHI5UKxM0nsX9fUNOLZvAxuWzgBU2Od1pn33YdRs2DbFsSd+7qPxWjD1zee+FCPHz9b43D8P+Ffjc1+1ZkPCQoLZtObPN597N0aOn6WxCHWX3onv/TlTRxMfF0vJspXp0X+Ext++eukM2zcuIy4ujgKFXBn603TKlH+7ntuJI7uJiYlmx6bl7Ni0XL3ds0RZfp6i+V5OiXo1qxMSEsqyNesTF6ouXIhp439SDzt78fKVxufmdWAQfQa/jXn91p2s35r4PT17SuIwfw83Vyb9+AN/rljNivWbyOeQh+96d6dhnZqpji8t9p6OwshQj27NLTEz0ePuozhmrQnRmA7f3lYfC7O35x9bKwX92lpibqogLFKJ3+M4Ji8NIjzy7Y/jQ+ejMDSADg0tMDdV8Ph5PDNWB/MySPe9NdblS1D18Er182K//wjA4xVb8Ok1WldhafjvN4H3mo1vfhMU5Ldxo9S/CV68eqXxm+dVYCC9h7yd/Gbdtl2s27aLMiU81b8JBvXpgdeaDcxa7E1QSAi57Wxp1bgB3b7SPLcK8R89laws99m7fS9j10rKbNZx6Td7XFbwr14hXYeQbuKUGdeTqAsecVd0HUK6uqiq/PFC2YSxQcqHiWUH5gYxHy+UjTiifRbg7GjMhk8f/pyVtJ2Ys2YWLH9tna5DSDd5PcrqOoRkdRubPsvRpMXyiZmzdmFmknvQhBBCCCGEECKLkARNCCGEEEIIId4RGBhI586dsbKywsbGhl69ehEeHv7R1505c4Z69ephbm6OlZUVtWrVSvUkf5KgCSGEEEIIIdJMpVLp7JFROnfuzI0bNzh48CC7du3i77//pm/fvh98zZkzZ2jSpAmNGjXi/PnzXLhwgQEDBqBQpC7lkklChBBCCCGEEOKNW7dusW/fPi5cuECFCokzIs+bN49mzZrx+++/4+iofT3eIUOGMHDgQEaNejtxTNGi2tfV/BDpQRNCCCGEEEKkmVKp0tkjJiaG0NBQjUdMzKdNrHTmzBlsbGzUyRlAgwYNUCgUnDt3TutrXrx4wblz58iTJw/VqlXDwcGB2rVrc/LkyVT/fUnQhBBCCCGEENnSr7/+irW1tcbj119//aRjBgQEkCdPHo1tBgYG2NnZERCgfcbK+/fvAzB+/Hj69OnDvn37KFeuHPXr1+fu3bup+vuSoAkhhBBCCCGypdGjRxMSEqLxGD1a+7p6o0aNQk9P74OP27dvpykOpTJxrcF+/frRo0cPypYty6xZsyhatCje3t6pOpbcgyaEEEIIIYRIM5VSd8sqGxsbY2xsnKKyw4YNo3v37h8sU7hwYfLmzcuLFy80tsfHxxMYGEjevNrXXcuXL3FNxGLFimls9/T05NGjRymK7z+SoAkhhBBCCCFyPHt7e+zt7T9armrVqgQHB3Pp0iXKly8PwJEjR1AqlVSuXFnrawoVKoSjoyO+vr4a2+/cuUPTpk1TFacMcRRCCCGEEEKkWU6bZt/T05MmTZrQp08fzp8/z6lTpxgwYAAdOnRQz+D45MkTPDw8OH/+PAB6enr88MMPzJ07l02bNuHn58eYMWO4ffs2vXr1StXflx40IYQQQgghhHjH6tWrGTBgAPXr10ehUNCuXTvmzp2r3h8XF4evry+RkZHqbYMHDyY6OpohQ4YQGBhI6dKlOXjwIEWKFEnV35YETQghhBBCCJFmqjcTZOQkdnZ2rFmzJtn9hQoV0tqDN2rUKI110NJChjgKIYQQQgghRBYhCZoQQgghhBBCZBEyxFEIIYQQQgiRZkodTrOfE0kPmhBCCCGEEEJkEdKDJoQQQgghhEizjJru/nMlPWhCCCGEEEIIkUVIgiaEEEIIIYQQWYQMcRRCCCGEEEKkmUomCUlXkqAJVOjpOoR0laDIWW/rAgn3dB1CulEo43UdQro6HV9V1yGkq9BIfV2HkG6evTLUdQjpqsf1IboOIV296viTrkNIN9MTRus6hHQVd22drkNIV5dKdtB1COmmeZyvrkMQmSRn/ZIVQgghhBBCZCrpQUtfcg+aEEIIIYQQQmQR0oMmhBBCCCGESDOlSqnrEHIU6UETQgghhBBCiCxCEjQhhBBCCCGEyCJkiKMQQgghhBAizWSSkPQlPWhCCCGEEEIIkUVID5oQQgghhBAizaQHLX1JD5oQQgghhBBCZBGSoAkhhBBCCCFEFiFDHIUQQgghhBBpplLJEMf0JD1oQgghhBBCCJFFSA+aEEIIIYQQIs2USqWuQ8hRpAdNCCGEEEIIIbII6UETQgghhBBCpJlMs5++pAdNCCGEEEIIIbIISdCEEEIIIYQQIouQIY5CCCGEEEKINFOpZJKQ9CQ9aEIIIYQQQgiRRUgPmhBCCCGEECLNZJKQ9CUJWhZ25swZatSoQZMmTdi9e7d6u7+/Py4uLvzzzz+UKVNG4zV16tShTJkyzJ49O11j2bNzG1s3ryc4KJBCLkXo0/973It6Jlv+1IljrFm5lBfPA8jnmJ+uPftQoWIV9f45M6dx9NB+jdeULV+RcZOmqZ/f87vDCu8/uXv3NvoKfapUr0nPPt9iamqarnUD2LZ7Lxu2bCcwKJgiLoX4vl8vPNzdtJb1f/iIZavXcefefZ6/eMm3vXvQrnULjTLL16xnxdoNGtucnRxZtnheuseuzZY9B1i3dReBwSEUKVSAQX26UczdVWvZB4/+xWvNRu7ce0DAy1cM6Pk1X7ZqqlEmMiqKv1Zv5MS5iwSFhODmUoiBvbvi6VYkM6rD5r2HWLttz5v6ODOk99cUS+Zv7zh4lH3HTnH/0b8AFC1SiH6d22uUP372Atv2H8X33gNCwyNYOmMSbi4FM6UuACqViv2b5nP2yCaiIsJwKVqWdj3HYp8v+Rju3brIsV3e/Hv/JqHBL+k+dC4lK9ZPtvymvyZw5vAGWn89klrNumZENdRUKhXHts/j8t8biY4Mxdm1HM2/Hkcuh0LJvubE7iXcvnyQV8/uY2BkgnORsjRoP4zceQsDEBUezNHt87h/4xQhgc8ws7TDo2x96rYZhImZZYbW5dLBedw6v5HYqFDyFipHjS/GYZ07+brcPLOWm2fXEhb0BABbB1fK1f+OAh611GV2LvmaZ/cvaLzOs/JX1Gw7IUPqAWBcvg4mVRuisLAm4fm/ROxfR8JT/2TL6xmbYlq3DUZFy6JnaoYyJJDIAxuIu3cdANNaLTCt1VLjNQmvAghZPC5D4t+9cztbNm8kKCgQF5ci9Ov/He5FPZItf/LEcVatXM6L5wE4OjrRvWdvKlSsrN7fsllDra/r0bMPbf/3pfr5hfPnWLdmFf7+9zE0MqJEiVL8PDb928m4Yl1MqzdGYWFNfMBjIveuJf7Jg2TL65mYYlbvC4w8y6Fnao4y5DUR+9YTd/eauozC0gazhv/D0LUEeoZGJAS+IHz7UhKePkz3+N+1dfd+1m3bSWDQm3NO3x54JnvOeYz3mo3cuXefgBevGNCrK+1bNdMok5CgZNm6jRw4dpLA4GBy29nSpF5tun7ZFj09vQytS2rY1ahA4WG9sC5XAhPHPFxs9y3PdxzWdVgiB5AELQvz8vLi+++/x8vLi6dPn+Lo6KiTOE4eP4r3n4voP2Aw7h6e7Ni2mQljRrLgj+XY2NgmKX/75nVmTPuFr7v3pkKlqvx97DBTJ41lxtwlFCzkoi5Xrnwlvh8yQv3c0NBQ/e/A168Y9+MP1KhVh77ffk9kZCReSxYwd+Y0Rv40Pl3rd/TEKRb/tYzB3/XDw92NLTt2MXLsJJYtnoetjXWS8tExseTL60CtGtVY9NfSZI9bqIAzv/3y9oeLvkI/XeNOzuGTZ1jgvYph/XtSzN2VjTv2MnzCVFYvmJFMfWJwzJuHutUrM897ldZjTpv/Jw8ePeanwf3JbWfLgWMnGTpuCivm/YZ9LrsMrs9Z5i9dw/B+3SnmXoQNu/YzdOJvrJ03HVsbqyTl/7l+mwY1qlDSww0jQ0NWb93N0Am/sXLOFHWsUdGxlPJ0p161Skxb5J2h8WtzdKcXJ/atpmP/KdjZO7Fv4zz+mNqXEb/twNDIWOtrYmOicCxQlEp12rJs5qAPHv/ahUM89LuKlW2ejAg/iVN7/+LcoZW06TUV29z5ObptDqtm9ua7X3ZjYKi9Pg/vXKBi3U44upREqUzgyOZZrJrRm29/2YWRsRlhwS8ID35Bwy9HYO/oSsjrp+xaOY6w4Bd8+e3cDKvL1eN/cf3USup8ORVLu/xcPDCHPV69aT80+bqYWztQqekwrHMXRKVScefSNg6s+I62A7dgl/fthR6PSu2p0Gig+rmBYfpfbPqPUbEKmDX8HxF71xD/5AEmlepj2XEgIYvGoYoMS/oChT6WnQejjAgjfPMSlGHBKKztUEVHaRSLf/GEsNWz325QJmRI/CeOH+OvP5fw3YCBb847Wxg7ZjSL//DWet65dfMGv02bQrfuvahYqTLHjx1l8qTxzJ67UH3eWbFqvcZrLl08z9w5M6lWvaZ626mTJ5g/dxZdu/WgVOmyJCgTeOjvn+71MypeEfPGXxKxaxXxT+5jUqUBll0GEzz/Z1QRWtpHXx+rr4eijAgjbMNilGFBKKxzoYqOVBfRMzHDqtco4h74ErZ6DsqIMPRz5UEVFZn0eOnoyInTLPBeydD+vRPPOTv3MHz8r6xaODPZc6ijQx7qVKvCfO8VWo+5Zst2tu89xOjB/SnknB9fv/tMnbsYczMz/teyqdbX6IK+uRmhPr48XraZCpsW6DockYPIPWhZVHh4OOvXr6d///40b96cZcuW6SyW7Vs30qhJM+o3aopzgUL0HzAEY2NjDh/Yq7X8zu1bKFe+El/8rwPOBQrSuWtPChdxY8/ObRrlDAwNsbWzUz8sLN9eFb9w/iz6Bgb0/XYQTvkL4ObuwTcDhnDm1N88e/okXeu3adtOmjVuQJMG9ShUwJnB3/bD2NiYfQe1XwXzcHelX89u1KtVQyOpfJ++vj52trbqh7V10mQiI2zYvocWjerSrH4dCjnnZ1j/XpgYG7P78HGt5T3divBt987Ur1kNI4Ok12xiYmL5+8x5+nfrRJninuTPl5eeHf+HU14Htu07lNHVYd3OfbRsWIfm9Wvh4uzED/26Y2JszK4j2uszbkh/2jZtgJtLQQrmd2Tkt71QqpRc9LmpLtOkTnV6fNmGCqWLZ3j871OpVPy9dyUNvuhHiQr1cCxYlI7f/kpo0AuuX0z+yqtnmZo0/WoQJSs2+ODxQwKfs3XZFDp/Nx19/Yy/BqdSqTh3aAW1WnyDR9n6ODgXpU2vaYQFv+D25eTfH12G/EWZGm3J4+RGXmcPWvf6lZDApzzzvwFAnvzufPndPIqWqYddngK4eFah3hdDuHP1KMqE+Ayry7WTKyhb7xsKFa9PrnxFqfvlNCJDX+B/I/m6FCxWjwIetbHOXQgbexcqNRmCoZEZLx5d1ShnYGiKmaW9+mFkYpEh9QAwqdyAmH9OEnv1NMpXz4jcsxriYjEuU01reeMy1dEzNSd840Li/72HMuQ18Y/ukvDiX82CSiWqiNC3j6iIDIl/29bNNG7SlAaNmlCgQEG+HTAIY2NjDh7Yr7X8ju1bKVe+Im3/9yXOBQrSpWt3ihRxZdfO7eoy755vbO3sOHv2DCVLlSZvvnwAJCQk8OeShfTo1YemzVvilD8/BQoUpGat2uleP5OqDYm5fIKYK6dIePmMiF2rEtunbA2t5Y3L1kDP1JywdQuIf+yHMvg18Q/vkPD8bfuY1miKMiSQiO1LiX/yAGXwK+Lu3UQZ9DLd43/Xhu27adGoHs0a1KFQgfwM698bE2Mj9hw6prW8p1sR+vfoQv1a1TAy1P4ddeP2HapXLk/VCuXI55CHOtWrULFsKW7fvZeBNUm9l/v/5s642TzfnvHnwqxOpVTp7JETSYKWRW3YsAEPDw+KFi1Kly5d8Pb2RqXK/DdhXFwc9/zuUKpMefU2hUJB6TLl8b19U+trfG/fpFTZchrbypaviO/tGxrbrl+7QreObfm2T1cWz59FaGjIO383FgMDAxSKt29RY+PEq9c3b1wjvcTFxXHH7x7lSpdSb1MoFJQrU4qbvnc+6dhPnj7jy2696dK7P1N+n83zFxl7kgSIi4vnzr0HVChVQr1NoVBQvnQJbvjeTdMxE5QJJCiVGL2XjBobG3Htpu8nxfsxifXxp0Kpt4mUQqGgQqli3PD1S9ExYmJjiE9IwMrSPKPCTJXAF/8SFvwK9xJvh/yamllSoEgpHt69+oFXfpxSqWTNglHUadGDvM7ahxelt+BX/xIe8pLCxd7+8DcxsyR/4VI8vnclxceJedOrY2qe9Iq7ukxUGMYmFigyKPEMC/yXqLCXOLm9rYuRqSV5nEvx4tGVFB1DqUzA78pu4mIjcShYRmOf35WdLJ9QhY0zW3J+7wziY6O0H+RTKfTRz1eAuAe33tmoIs7/NgZOhbW+xNC9FPH/3sesSSdsBv+GVd+xmFRvCu8NJ9O3y4PNoGlYf/cL5m16orBK2pv1qeLi4vDzu0PpMm/PIwqFgjJlyiV73rl9+yZlkpx3KnD79i2t5YOCgrh44RwNG73tjbnnd5fXr1+h0NNj0IBv6Nr5K8aN+ZGH/skPO0wTfX0MHAsSe/+duqhUxN6/hWF+7e1jVLQM8f/ex7x5J2yHz8T62wmY1mym0T6GRUsT//QhFu2/wfaHmVj3G4txuZpaj5de/jvnlC9dUr0t8ZxTkhufcA4t7uHOZZ/rPH7yFAC/Bw+5dtOXyuXKfGrIQmQLMsQxi/Ly8qJLly4ANGnShJCQEI4fP06dOnXUZapVq6aRwABERUUluS/tXTExMcTExGhsi42JwchY+9CdsNAQlEolNraaJ2FrG1v+ffxI62uCgwKTDEGxtrElKChI/bxc+YpUrVaDPA75CHj2lFXLvZg0dhRTZ8xHX1+fUqXLsvTPRWzdtI4WrdsREx3NiqV/AhAUGJhs/VIrJDQMpVKJra2NxnZbG2se/5v2njoPdzdGDB5AfidHAoOCWLF2I4NH/YzX/NmYmWXcsKaQsDASlMokw0rsrK159O/TNB3TzNSU4kXdWL5hKwWdnbC1tubwidPc8L2LU9686RF2sv6rj917QxntbKx5+ORZio6xcMV6ctvaaiR5uhQa8goAS+vcGtstrXMRGvzqk459dIcXCn0Dajbp8knHSY3wkMQLD+ZWuTS2m1vlJiI0ZfVRKZXsWzcFZ9dy5MnvrrVMZFgQf+9cRLnaX2rdnx4iwxLrYmahWRdTi9xEhn24LoHPfNm2sCMJ8TEYGpnRqOt8bB3eJsmuZVpgYeOIuVUeXgfc4fye3wl+6U+jrul/X6qemQV6Cv0kQ+WU4aEY5tL+mdW3sUdRKBex188Rtm4e+nZ5MGvSERT6RJ/YBUD8kweE71yG8vVzFBbWmNRsgWW3HwhZMgFiY7QeNy1C35x3bN8779jY2PLv48daXxMcFISNjU2S8sFB2s8XRw4dwNTUjGrV3/ZYBQQkfqesWb2SXn2+wcHBga1bNjF61HCW/LkUS8v0GQWhbp/wUI3tqohQ9HIn0z62uVG4eBDjc5bQ1XPQt8uDefPOoNAn6vjON2Xs0a9Yh6gzB4g6sRsDJxfMm3aEhARirp5Ol9jfFxIaqvWcY2tjzaNPOId2bteayMgovv5uGAqFAqVSSe8uX9GwjvYeRqF7SplmP11JgpYF+fr6cv78ebZu3QqAgYEBX331FV5eXhoJ2vr16/H01Jyoo3Pnzh889q+//sqECZo3O3/7/RAGDBqWPsGnUM3a9dT/LuRSmEIuhfmmVxeuX7tK6TLlKFDQhYFDR7H0r4WsXPYXCoU+LVp/gY2tLQpF1rlBODmVK7y9klvEpRCe7u506vUNx06eolmjDw9Ry4p+HvwtU+cvoW3P79BXKHArUoj6Navhey+dryyns5VbdnL41DnmTRyNsZGRTmK4dHIXm/4ar37ee8SiDPk7j+/f4MS+lQyZsilDb6L3ObuTXSve3lvZadDiTz7m7tUTefHkLj1HrdG6PyYqnDVz+mHvWIQ6rQZ88t/7z91/dnJiy9u6NOmR9rpY27vQbtBWYqPDeHBtP8c2jKJlv5XqJM2z8lfqsnb5imJmac/uP7sT+voRVrkKpL0S6UVPD2VEGBG7V4FKRULAIxSWNphUaaRO0OLuvR0FkfDiCfFPHmD9/a8YFatA7JVTuoo8TQ4e3E+duvUweud7QflmqNSXHTpRvUZiz9PgocPp/nUnTp74m6bNWmg9VqbQ00MZEUrEzhWJ7fPsIQorG0yrNVYnaOjpEf/Un6jDib8dEgIeo5/HCeMKtTMsQcsoR0+e5eDxk4wZ+j2FCuTH74E/871WqCcLESKnkwQtC/Ly8iI+Pl5jUhCVSoWxsTHz589Xb3N2dsbVVXMY08dmOBw9ejRDhw7V2Pbg3+SvDFtaWaNQKAh+p/cLICQ4CFs77ZND2NjaERyspbxt8kNh8uZzxMrKmoCnT9TDWmrXrU/tuvUJDgrE2MQUPT3YsXUTDnnzfbCOqWFtZYlCoSAoKFhje1BwCHbv9ap9CgsLc/I75uPps4B0O6Y21paW6CsUBAWHaGwPDPm0+jjlc2De5LFERUcTERlFbjtbxv02F0eHjJ2E4r/6BAZrXmkODA4hl5abz9+1ZtseVm/ZzezxI3AtpLsfwMXL16Wg69vhP/FxcQCEhbzCytZevT0s5DVOhZKfoe5jHty+RHhoIL98//YCgFKZwI5Vv/H33pX8PO9gmo/9rqKl65J/3NshwfHxsQBEhL7G0ubt+yEi9BUOzsnP9PqfPasncvfqMbqPXIWVXdLeg5iocFbN6o2RiTlfDZiPvkHy932mVsFidcnj/LYuCW/qEhn+GjOrt3WJCn9FLscP10XfwAjr3ImzcNrnL8HLf69z7eQKarWbqLV8ngKJfzfk1cN0T9BUkeGolAnomWvOdqmwsEIZHqL1NcrwkMQJP94ZSp/w6hkKS2tQ6GudDEQVE4Uy8Dn677yP04PVm/NO0HvnneDgIGzttJ9HbGxtCQ4OTlLexjbpeerG9Ws8+fcxI0f9pLHd7s05zbnA29lUDQ2NyJs3Hy9fvkhLVbRSt4+FZo+cnrkVquTaJywE1fvt8/IZCksb0NeHhASUYSEkvNQcWZDw8hnGnuXIKNZWVlrPOZ96Dl20bBWd27Wmfq3E4cZFChXg+ctXrN60XRK0LCqn3gumK3IPWhYTHx/PihUrmDFjBleuXFE/rl69iqOjI2vXrv2k4xsbG2NlZaXxSG54IyTOrFjE1R2fq5fV25RKJT5XLlPUo5jW1xT1KIbPlcsa2678c5GiHskPMXv16iVhYaFakz4bWztMTU05+fcxDA2NKF22wseqmWKGhoa4uxbhH5+397UplUr+uepDsaLah1qlRVRUFE8DnmP3gSQ1PRgaGuBexIVLPm+vdCuVSi773KB4Ue3LBqSGqYkJue1sCQsP58I/PtSoVP7jL/oEifUplKQ+l3xuUrxo8vdYrd66m+WbtvP7mOF4uGq/pyOzmJiakztvQfXDIX8RLG1yc/f6OXWZ6MhwHt3zoaBb6TT/nfI1WzFs2laGTt2sfljZ5qFOyx70Hf1HelQFAGNTC+wcCqof9o6uWFjbc//WGXWZmKhw/r3vg3ORMskeR6VSsWf1RG5fPkTXH5Zha58/SZmYqHBWzeyFvoEhHb9fmOwsimllZGyBde6C6oetgyumlvY89Xtbl9jocF489iFPgeTroo1KpUSZEJvs/tdPbwNoJILpRplAwrNHGLq8m1TqYVjIg/gn97W+JP7feyhs7YG3va8KOweUYcHJz9RoaIzC1j7ZpC+tDA0NcXV1x+fqP+ptSqWSq1f+Sfa84+FRjKtX/tHYduWfy3h4JE2sDxzYi6urGy6FNZfqcHVzw9DQkCf/vh1GGR8fz4sXAeTJ4/ApVdKUkED804ea7aOnh2FhD+L+1d4+cY/90LfLo3HPmX6uN+2TkNg+8Y/90M+lGad+LgcSQl6nX+zveXvOua7elnjOuU7xTziHxsTGovfeaBmFQiHD6MRnQ3rQsphdu3YRFBREr169sLbW7CFo164dXl5eNGnSJFNjav1Fe+bMnIqrW1Hc3D3YuX0z0THR1G+YGMfs338lV67cfN2jDwAtW7flp5FD2LZlAxUqVuHE8SPcu3uHb79PHEYZFRXF+jXLqVq9Fja2dgQ8e8py7yXky+dE2fIV1X93986teHgWx8TElKv/XGKZ9xK6du+DhUX6znz2vzYtmTZrHu6uRfBwd2Pz9l1ER8fQuEHiMMypM+eSO5cdvbsl3tcTFxfHw8eJM2fFx8fz6vVr/O4/wNTEBCfHxN69xV7LqVqpAg557HkdGMiyNetRKBTUq53x4+e/bN2MX+cspqhrYTzdirBx516ioqNpVj/xquPk2QvJncuOfl93eFOfePzf1CcuPp5XgYHcve+PqakJ+fMl9mic/+cqKhU4O+XjybPnLFq2hgL5HdXHzEgdWjZh8rw/8XB1wdOtMBt2HiAqJobm9RLXmJo0Zwn2uWz5pkvivUmrtuzCa90Wxg3pT748uXn9pnfU1MQEM1MTAELDwnn+6jWvAhP3PXpzP5udjTW50rHnVBs9PT1qNf2aQ9uWkDtvAXLlyc/ejfOwss1DiQpv1zVb9EtPSlasT43GicOWY6IjeBXw9r7PwJf/8sT/FmYW1tjmdsTc0gZzS83Y9fUNsLLOTR5HFzKKnp4elRt05cSuxeRyKIRNbieObp2LpU0ePMq97c1b8Vt3PMo1oFL9xM/RnlUTuXZuFx2+X4Cxibn6XjZjU0sMjUyIiQpn5cxexMVG8VWf34iJDicmOhwAM0s7FBmwbIWenh4la3Tl8pHFWOUuhJWtExcOzMXMKg+Fir+ty64/ulOoRANKVEusy/m9M3AuWgsLm3zExUTgd2UXT++fp1nPvwAIff0Iv3924exRCxMzG14H3OHMzl/J51KBXPmKpns9AKLPHcK8VXfin/kT/8Qfk8r1wdBIPdTNvFV3lGHBRB3dBkDMpeOYVKiDWeOviL5wBH27PJhWb0r0hSPqY5rWb0fcXR+UIYEoLK0T10RTKom9cUFbCJ+kzRftmDVzOq5u7ri7F2X79q1Ex0TToGFjAGb+Po1cuXLTrUcvAFq1/oLRI4exdctGKlSszInjx/C7e4cB3w/WOG5kZASnTpygV+++Sf6mmZk5TZu1YM2qFeS2tydPHge2bEpcz7JGjVpJyn+K6DMHsfiiJwlPHyYug1ClAXqGxsT8kzhU1OKLnihDg4k8vAWAmAvHMKlUD7MmHYg+/6Z9ajYn+tzbmV+jzhzEutcoTGs2I+bGRQycCmFSvhbhO7VPZZ9evmzdnF/nLMLDtTAebq5s2rmHqOgYmjZ4c86ZtQD7XHb07doReO+cE5fAq9dJzznVKpZj1cZtONjnppBzfu7e92fD9t00a1AnQ+uSWvrmZpi7vu0BN3PJj1VpD2IDQ4h+nLL7pIXQRhK0LMbLy4sGDRokSc4gMUGbPn06oaGhWl6ZcWrUrktIaDBrVy4lKCgIl8JFGDdxmnroyMuXL9B7Z7ISj2IlGDriJ1av8GbVMi8cnZwYNWaiei0ahUKB/4P7HD10gIiIcGztclGmXAU6f90DQ8O39wPc9b3NulXLiYqKIr+zM/0HDKFu/UbpXr+6NasTEhLCstXrCAoKpkhhF6ZO+Fk9POPFy1ca9/S8Dgyi36Dh6ucbtu5gw9YdlC5RnJm/Jg5nevn6NZN/n0VoaBjW1laUKObJ/N9/xUZLu6a3+jWqEhwSivfaTQQGBePqUpDfx43C7s2QwOcvX6On97a9XgUG0Wvoj+rn67btZt223ZQp7sncyWMACI+I4o+V63j5OhBLSwtqV61In85fYaBlWv70r08VgkPD+GvtFgKDQ3B1KcCMMT+8rc+r1xr3JW7bf4S4+Hh+/k1z8oUeX7ahV4e2AJy88A9T5v+p3jdu5sIkZTJS3Za9iI2JYtNf44mKDMOlaDn6jlqisQba6+ePiQgLVj9/fP8Giyb1UD/fsXI6ABVqtaZj/ykZHvOHVG/am7jYKHYuH0t0ZCgF3MrTZcifGj1egS8fERn+dsjaxWOJowGWT9dcRLt1jymUqdGWZw9v8OR+4qyW80Zrfu4HTTuETe6kPW7poXTt3sTHRnFi81hio0PJW6g8TXtq1iU08BHREW/rEhUeyNENI4kMfYmRiSW58hWlWc+/yO9eHQCFviFP/E5z7dRy4mOjMLfOh0vJRpSr1z9D6gAQe/MiemYWmNZuhcLcioTn/xK2dq564hCFtZ3GcDllaBBha+Zi1rA91n3HogwLJvrCEaJP71OXUVjZYvFFb/RMzVFFhhP32I+IZVNRRYane/w1a9chJDSY1SuXExQUROHCRZgwcYp6qHzieeft596zWHGGjxjNqhXLWLFsKY5OTvw0ZrzG2psAfx8/hgoVterUQ5sevfqi0Ndn1u/TiImJpWhRD3759TeNZWDSQ+yNC0SaW2BatzUKCyviAx4Ttmo2qojE87vCOpfGzM3K0CDCVs7CrMlX2PQfjzI0iOhzh4g6+Xa5m4Sn/oStX4hZ/baY1m5JQtArIvatI/bauSR/Pz3Vq1mN4NBQvNdsVJ9zfhs3Crs3k7a8ePVK4zv6VWAgvYeMUj9ft20X67btokwJT+ZMTrwndFCfHnit2cCsxd4EhYSQ286WVo0b0O2rdhlal9SyLl+CqodXqp8X+z3xXPp4xRZ8eo3WVVg6oVJK72Z60lPpYu52kaXcupe+64rpmmVC0McLZSOGCek3O5quKZQZs36VrpyLydghnpktNDJzFlPPDM8+bULMLKfH9W90HUK6etXxp48XyiZyrdR+n2F2FdfhO12HkK4uleyg6xDSTfO4jF3a5lM07HxJZ3/74OqcdS4G6UETQgghhBBCfAKZJCR9ySQhQgghhBBCCJFFSIImhBBCCCGEEFmEDHEUQgghhBBCpJlKlkBIV9KDJoQQQgghhBBZhPSgCSGEEEIIIdJMKZOEpCvpQRNCCCGEEEKILEJ60IQQQgghhBBpJgtVpy/pQRNCCCGEEEKILEISNCGEEEIIIYTIImSIoxBCCCGEECLNVDJJSLqSHjQhhBBCCCGEyCKkB00IIYQQQgiRZrJQdfqSHjQhhBBCCCGEyCIkQRNCCCGEEEKILEKGOAohhBBCCCHSTCYJSV/SgyaEEEIIIYQQWYT0oAkhhBBCCCHSTKWUSULSk/SgCSGEEEIIIUQWIQmaEEIIIYQQQmQVKiEyQXR0tGrcuHGq6OhoXYeSLqQ+WVdOqotKJfXJynJSXVQqqU9WlpPqolJJfYT4GD2VSiXTrogMFxoairW1NSEhIVhZWek6nE8m9cm6clJdQOqTleWkuoDUJyvLSXUBqY8QHyNDHIUQQgghhBAii5AETQghhBBCCCGyCEnQhBBCCCGEECKLkARNZApjY2PGjRuHsbGxrkNJF1KfrCsn1QWkPllZTqoLSH2yspxUF5D6CPExMkmIEEIIIYQQQmQR0oMmhBBCCCGEEFmEJGhCCCGEEEIIkUVIgiaEEEIIIYQQWYQkaEIIIYQQQgiRRUiCJoQQQgiRBUycOJHIyEhdhyGE0DGZxVFkmpiYGJmCVgghhEiGvr4+z549I0+ePLoORQihQwa6DkDkXHv37mXdunWcOHGCx48fo1QqMTc3p2zZsjRq1IgePXrg6Oio6zA/a48ePeLhw4dERkZib29P8eLFJYkW4jMmF9J0S66ZZ21Dhw5NUbmZM2dmcCQip5MeNJHutm7dysiRIwkLC6NZs2ZUqlQJR0dHTE1NCQwM5Pr165w4cYIzZ87QvXt3Jk2ahL29va7D/qjg4GC2bt3KiRMnNJKasmXL0rhxY6pVq6brEFPE39+fRYsWsW7dOv7991+NHwRGRkbUrFmTvn370q5dOxSK7DMK+sGDB1rbpmrVqpiYmOg6vFS5deuW+uKGtvdau3btst2P6JzUPjExMZw7dy5JXVxcXHQdWqrltAtpSqWS48ePa32vNWjQAGdnZ12H+EEKhYLnz59ni3NiSrVt2zZF5bZs2ZLBkXy6unXrajw/efIk5cuXx9TUVL1NT0+PI0eOZHZoIoeRBE2ku6pVq/Lzzz/TtGnTD/7Af/LkCfPmzcPBwYEhQ4ZkYoSp8/TpU8aOHcvq1atxdHTUmnBeunSJggULMm7cOL766itdh5ysgQMHsnz5cho3bkzLli2TTZ7XrVuHvr4+S5cupWLFiroO+4NWr17NnDlzuHjxIg4ODhr1uXfvHiYmJnTu3JmRI0dSsGBBXYf7QZcvX2bEiBGcPHmS6tWrJ9s+oaGhjBgxgsGDB2f5RC0ntc+pU6eYM2cOO3fuJC4uDmtra3VdYmJiKFy4MH379uWbb77B0tJS1+F+UE67kBYVFcWMGTNYtGgRgYGBlClTJkl9nj59SqNGjRg7dixVqlTRdchaKRQKrK2t0dPT+2C5wMDATIro0/Xo0UPj+Zo1a2jZsmWSz8jSpUszM6x0YWlpydWrVylcuLCuQxE5jCRoQnyEg4MD3bp1o3v37hQrVkxrmaioKLZt28bcuXNp164dw4cPz+QoU2b06NEMHz6cXLlyfbTsvn37iIyMTPHVT10oW7YsRkZGdOvWjZYtWya5Oh4TE8OZM2dYt24dmzdvZuHChbRv315H0X6ci4sLw4cPp3PnztjY2CRb7syZM8yZM4dSpUrx448/Zl6AqZST2qdVq1ZcvnyZTp060bJlSypUqKBx1fz+/fucOHGCtWvXcvXqVVasWEHDhg11GPGH5bQLac7OzlStWpXu3bvTsGFDDA0Nk5R5+PAha9asYcmSJfz000/06dNHB5F+mEKhYPbs2VhbW3+wXLdu3TIpovSXk5KanFQXkbVIgiYyROHChblw4UKKEoGs7vXr16mqR2rLZ7ZHjx7h7Oz80Su02cH+/ftp3Lhxisq+fv0af39/ypcvn8FRpV1cXJzWH5bpVT6z5aT2WbJkCT179kzR//fNmzd59uwZ9evXz4TIBCQOC/b09ExR2bi4OB49ekSRIkUyOKrUUygUBAQE5OhJQnJSUpOT6iKyFknQRIb4HE4y2ZXMEpa11atXjy1btnywB03oxt9//021atUwMJD5tUTG+By+n3NSUpOT6iKyluwzA4AQWcDGjRtp27YtJUqUoESJErRt25ZNmzbpOqxUyWnXZJ4+fcrw4cMJDQ1Nsi8kJIQffviB58+f6yCytDl27BixsbG6DiPdLVu2TOv2+Ph4Ro8enbnBpFHdunWz1b0/KfHs2TNWrVrFnj17krzvIiIimDhxoo4iSxuVSsWDBw+Ij48HIDY2lvXr17NixQpevXql4+g+Lqd9P+c0Pj4+Gg+VSsXt27eTbBfiU0kPmsgQCoWC5cuXf3QcfatWrTIpok+jVCrp2LEjGzduxN3dHQ8PDyBxWI2fnx/t27dn7dq12WLYYE6bJey/5OyPP/7Quv+bb77B2tqaadOmZXJkaZNTe5+trKxo3Lgxf/zxB7a2tgD4+vrSqVMn9fDGrC6ntc2FCxdo1KgRSqWSuLg4nJyc2LZtG8WLFwfg+fPnODo6kpCQoONIU8bX15fGjRvz+PFjChcuzIEDB2jfvj23b99GpVJhZmbG6dOncXNz03Won5UdO3ZoPO/YsSOzZ8/GwcFBY3t2+D2gUCjQ09P7YCKtp6eXbT4zIuuSBE1kiJRMz56dvsRmzZrFL7/8wvLly2nRooXGvh07dtCjRw/GjBnD4MGDdRNgKigUCvr27YuZmdkHy2WXdVxKlCjB4sWLqVGjhtb9p0+fpk+fPty4cSOTI0sbhULBkSNHsLOz+2C5UqVKZVJE6ePevXt06dKFx48fs3TpUu7cucOIESNo06YNCxcu/OjFnKwgp13caNiwIc7Ozvz1119EREQwcuRINmzYwMGDBylbtmy2S9DatGmDSqXil19+wdvbm/379+Pu7s7GjRtRKpW0b98ea2trVq5cqetQPys56ffAw4cPP1omLCyMEiVKZEI0IieTBE1kiJx2pblUqVIMHjyYnj17at3v5eXFnDlzssXQBoVCQdWqVTEyMkq2THZax8Xc3Jxbt25RoEABrfsfPXqEp6cnERERmRxZ2nzoCu1/27PLj5n3KZVKBg8ezIIFC9DX12f58uV07NhR12GlmEKhoGnTph9d2iA7rOcEYGdnx9mzZ3F3d1dvmzp1KtOnT2f//v0UKFAgWyVoefLk4cCBA5QpU4aIiAgsLS35+++/1RdvTp8+TceOHVP0I1tkrsjIyI9eNMzKwsLCWLt2LV5eXly8eDHbfGZE1iV3OosMkR2G+qXG3bt3adCgQbL7GzRowIABAzIxok+zdevWHJM8m5qa4u/vn2yC5u/vrzEdenZw7ty5HNNL867du3ezbt06qlatyp07d/Dy8qJ27drZaiFkS0vLbPd++pDo6GiN56NGjcLAwIBGjRrh7e2to6jSJjw8XN3zbG5ujrm5Ofny5VPvd3Z2zlb3o34OYmJiWLBgAdOnTycgIEDX4aTa33//jZeXF5s3b8bR0ZG2bdsyf/58XYclcgBJ0ESGyGkds6ampgQHByebBISGhmJiYpLJUaVNTkueK1euzMqVK6lVq5bW/StWrKBSpUqZHNWnKVCgQI5JoP/Tr18/li9fzuTJkxk6dCjPnz+nZ8+elCxZkkWLFvHll1/qOsQUmTt3bo5pmxIlSnD69Okkw2WHDx+uvu82O3F0dOTRo0fq7+np06drtNXLly/V9z+KzBMTE8P48eM5ePAgRkZG6qHN3t7e/Pzzz+jr62fpNfbeFxAQwLJly/Dy8iI0NJQvv/ySmJgYtm3bluxaqUKklsziKDJEt27dctRV5qpVq7Jo0aJk9y9YsICqVatmYkRpl9OS5+HDh7N06VKGDx+ucXX8+fPnDBs2jGXLlmXZhcM/J6dOneLcuXMMGzYMPT098ubNy549e5g4cWKyQ4ezmpx2caNr166cOnVK674RI0YwYcKEZC9KZUUNGjTg9u3b6uf9+/fH0tJS/fzAgQOUK1dOF6F91saOHcuiRYsoVKgQ/v7+tG/fnr59+zJ79mxmzpyJv78/I0eO1HWYKdKyZUuKFi2Kj48Ps2fP5unTp8ybN0/XYYkcSO5BExkiODiYtWvX0r9/fwA6d+5MVFSUer++vj5//vlntlnr6fTp09SpU4c2bdowfPhwPDw8UKlU3Lp1ixkzZrB9+3aOHj1K9erVdR3qRy1fvpwOHTp89D6a7GTJkiUMGjSIuLg4rKys0NPTIyQkBENDQ2bNmqV+H2YHdevWZevWrdnms5FSMTExyb7nfH19KVq0aCZHlHo57d7az82DBw8wMTHRGPYoMl7hwoWZPXs2rVq14vr165QqVYru3bvj5eWV7S56GBgYMHDgQPr3768xG6ihoSFXr16VHjSRbiRBExni999/559//mH16tVA4n0bjRs3Vl/NPHPmDB06dGD8+PE6jDJ1tm7dSt++fZOsg2Rra8uSJUto166djiJLnXv37jF58mT1/SUFChQgPDxcvV9fX5+TJ09mix/M73ry5AkbNmzAz88PlUqFu7s7//vf/8ifP7+uQ0uVqKgoDh8+rJ4tdPTo0cTExKj36+vrM2nSpGwzpPZdPj4+3LlzBwB3d/dsNxPl8ePHqV69unqh6rCwMI0eaYVCgYWFha7C+yTZvW1E1mVkZMSDBw9wcnICEm8ZOH/+PCVLltRxZKl39uxZvLy8WL9+PZ6ennz99dd06NCBfPnySYIm0pUkaCJDVK5cmcmTJ6sn1rC0tOTq1asULlwYSEx2Jk6cyD///KPLMFMtMjKS/fv3c/fuXSDxh0yjRo2y1exTQ4YMwcTEhF9//RVIbJuxY8eqewXWr19PgQIFWLx4sS7D/GwtXryY3bt3s3PnTiCxfYoXL64eMnz79m1GjBiRre7ZOH/+PL169eLmzZvqhEZPT4/ixYvj5eVFxYoVdRxhyly5coUff/yRPXv2AIltExkZqd6vp6fHmTNnsk19IOe0zX/i4+OZNWsWa9eu1Ug4O3XqxKBBgzA0NNRxhJ8ffX19AgIC1BMfWVpa4uPjg4uLi44jS7uIiAjWr1+Pt7c358+fJyEhgZkzZ9KzZ0+NYbVCpJUkaCJD2Nvbc/nyZZydnQGoUKEC27ZtU/dm3L9/n1KlSmn03IjMUbJkSby8vNQTZ7yfPB8/fpzevXurk9Cs7v1FUJOTHRZBBahRowYjR46kZcuWQNL2WbVqFQsWLODMmTO6DDPFbt68SeXKlfH09GTIkCF4enqqt8+aNQtfX1/Onj2bLa489+7dm8KFC/Pjjz8CiW2zZMkSnJycUKlUeHt7o1Kpss06WzmpbSCx97lhw4acOXOGBg0aqOtz69YtDh06RPXq1Tlw4EC27H3Ozt5fnmLnzp3Uq1cPc3NzjXLZZXmK9/n6+uLl5cXKlSsJDg6mYcOGKT4vCZEcSdBEhjAzM+P8+fPJLtZ47do1KleurHH1OSs7cuQIAwYM4OzZs1hZWWnsCwkJoVq1aixevJiaNWvqKMKUs7S05NatW+pkeciQIfz888/kypULSFyI08PDQ+Oewazs/UVQta0hlp3WDcuXLx9nzpyhUKFCQOLFjgsXLqif37lzh4oVKxISEqK7IFPhyy+/JD4+ns2bNye530SlUtG2bVsMDQ3ZsGGDjiJMOU9PT9asWUPZsmWBpMnzuXPn+PLLL7PNOls5qW0Axo0bx7Jly9i5c2eSIZpXr16lVatW9OjRI1sNrc8JevTokaJyS5cuzeBIMlZCQgI7d+7E29tbEjTx6VRCZIDixYurli9fnux+b29vVbFixTIxok/TsmVL1cyZM5PdP2fOHFWbNm0yMaK0s7KyUp07dy7Z/efOnVNZWlpmYkTpy8LCQnXv3j1dh5FmJiYmqtu3bye7/9atWypjY+NMjOjT5M6dW3XhwoVk958/f16VO3fuTIwo7UxNTVWPHz9WP585c6YqJCRE/fzhw4fSNjrk7u6u2rRpU7L7N2zYoHJzc8vEiIQQIm1kmn2RIb744gt+/vlnrYuCBgQEMG7cOL744gsdRJY2V69epUmTJsnub9SoEZcuXcrEiNKuePHiHDp0KNn9+/fvT7bnU2S8/Pnzc/369WT3+/j4ZKuJT8LCwnBwcEh2f968eQkLC8vEiNLOxMREo3dsyJAhGj3qjx8/zlb3o+aktoHE3v8PrXlYpUoVHj16lIkRCSFE2kiCJjLEiBEjsLCwwM3Nje+++445c+YwZ84cvv32W9zd3TE3N882655A4ppaH7q53MDAgJcvX2ZiRGnXo0cPJk+ezO7du5Ps27lzJ1OnTk3xkBSR/po1a8bYsWOJjo5Osi8qKooJEybQvHlzHUSWNgULFuT8+fPJ7j937hwFCxbMxIjSrmzZsmzbti3Z/Vu2bFEPf8wOclLbAFhZWfHixYtk9wcEBMgEDkKIbMFA1wGInMnS0pJTp04xevRo1q5dS3BwMAA2NjZ06tSJKVOmZKsTpZOTE9evX8fV1VXrfh8fn2yztk6fPn04cuQILVu2xMPDQz2dvq+vL76+vrRr144+ffroOMrP148//siGDRsoWrQoAwYMwN3dHUhsn/nz5xMfH6+epCI76NChA0OHDqVo0aJJemavXbvG8OHD6dq1q46iS51vv/2WDh06UKhQIfr376++/zEhIYGFCxcyb9481qxZo+MoUy4ntQ0kriE4ZcoUNm/erHX/1KlTqVu3biZHJYQQqSeThIgMp1Kp1L1L9vb22W5hSoDvv/+eY8eOceHChSQzgEVFRVGpUiXq1q3L3LlzdRRh6q1bt45169app6J2c3OjY8eOdOjQQceRfRorKyuuXr2aradwfvDgAf379+fgwYMaU583bNiQhQsXqielyA6io6OpX78+586do2HDhnh6eqoXeT906BCVKlXiyJEj2WZmvZEjR/Lbb79haWmpbof79+8THh7O0KFD+e2333QcYcrltLb5b1bK4sWLM3ToUDw8PNT1mTVrFjdv3uTs2bMUL15c16EKIcQHSYImRAo8f/6ccuXKoa+vz4ABA9S9Trdv32bBggUkJCRw+fLlD97PITKGra2tRtIfHByMlZVVktkd319gPDsIDAzEz88PAFdXV+zs7HQcUdrExsZqXZuqQ4cODBkyRD39dnZx9uxZ1q5dq16K4r+LG1WqVNFxZKmXE9umV69e3Lp1S/29oFKp8PDwwMvLi6pVq+o4QiGE+DhJ0ES6a9KkCePHj//oj5WwsDAWLlyIhYUF3333XSZFl3YPHz6kf//+7N+/X6NXo3HjxixYsCBb9NhEREQkWXsmPcvrwvLly1NUrlu3bhkciRAiq7hy5YpGwlmmTBndBiSEEKkgCZpId15eXowdOxZra2tatmxJhQoVcHR0xMTEhKCgIG7evMnJkyfZs2cPzZs357fffqNAgQK6DjvFgoKC8PPzQ6VS4ebmhq2tra5DSrF8+fIxaNAgunXrluw9cyqVikOHDjFz5kxq1arF6NGjMznKz9c333zDzz//nKJZGtevX098fDydO3fOhMjEo0ePUvU99eTJE5ycnDIwIiGEEDmVJGgiQ8TExLBx40bWr1/PyZMn1Yvq6unpUaxYMRo3bkyvXr3w9PTUcaSfF19fX3788Ud2795N6dKltSbPZ86cwcDAgNGjR9OvXz/09fV1HfZnY8yYMcydO5fq1at/8OLGunXrcHR05I8//kiyIG9W4+Li8tH7TvX09Lh3714mRZQ2Dg4OtGnTht69e1OxYkWtZUJCQtiwYQNz5syhb9++DBw4MJOjTJ2c0jaQOAHIoEGDMDU1/WjZc+fO8erVq2w1G6oQ4vMiCZrIFCEhIURFRZErV64PTlefFeXEXo1Hjx6xceNGTpw4wcOHD4mKiiJ37tyULVuWxo0b07Rp02yTmKV0woz79+9ncCTp4/nz5/z111+sW7eOmzdvauyztLSkQYMG9O7d+4Pr8mUlc+bMSXafv78/S5YsISYmhoSEhEyMKvVev37N5MmT8fb2xsTEhPLlyydJnm/cuEG5cuUYM2YMzZo103XIH5VT2gaga9eu7N27l/bt26svbtjb2wMQHx+vvrixatUqnj59yooVK6hVq5aOoxZCCO0kQRPiI3Jir0ZOolAoKFiwIJ06dSJPnjzJlhs0aFAmRpU+goKCePTokTqBLlKkSLacBfV9gYGBTJo0iUWLFlG5cmWmTZuWbSbYiIqKYvfu3Zw8eVLrxY3svsh7dm6bq1evMn/+fDZt2kRoaCj6+voYGxsTGRkJJK5j17t3b7p3755tZqYUQnyeJEETGWrjxo1JZgfr1KkT//vf/3QcWerktF6NnGTjxo14e3tz7NgxmjZtSs+ePWnWrFmSWRyF7kVFRTFz5kx+//13ChYsyJQpU7JFT9PnICe1jVKpxMfHRyOBLlOmDLlz59Z1aEIIkSKSoIkMoVQq6dixIxs3bsTd3R0PDw8Abt26hZ+fH+3bt2ft2rXZsjcgu/dqXL58GVtbW/WskytXrmTx4sU8evSIggULMmDAgGy5FtqTJ09YtmwZy5YtIzIykq+//ppevXrh5uam69DSZPz48YwdOzZJohkSEsI333zD2rVrdRRZ6iUkJPDnn38yYcIETExMmDhxIl26dMlWn5ucStpGCCGyIJUQGWDmzJkqOzs71c6dO5Ps2759u8rOzk41a9aszA9MqEqVKqU6ePCgSqVSqf7880+VqampauDAgapFixapBg8erLKwsFB5eXnpOMpPc+zYMVWdOnVUCoVCFRgYqOtw0iR//vyqqlWrqu7du6fedvToUZWzs7OqYsWKOowsddavX69yc3NT2dvbq2bPnq2KiYnRdUif5MqVK6pJkyapFixYoHr58qXGvpCQEFWPHj10FFnq5bS20SY2NlZ1584dVXBwsK5DEUKIFJMeNJEhSpUqxeDBg+nZs6fW/V5eXsyZMwcfH59MjixjREREcOnSpWxx07mZmRm3bt2iYMGClCtXjv79+9OnTx/1/jVr1jB58mRu3LihwyjTJjo6mk2bNuHt7c3Zs2dp1aoVy5cvz3aL7UJiT22/fv3Yt28fM2bM4M6dO8yZM4cffviBCRMmYGBgoOsQU0ShUGBqakrHjh2xsrJKttzMmTMzMaq0OXDgAC1btsTNzY2wsDAiIiLYuHEjdevWBRKHQjs6OmaLSTUgZ7UNwPTp0/n+++8xNTUlISGBkSNHMm/ePOLj41EoFHz99dcsWbIk201UJYT4/EiCJjKEqakpvr6+ya4b9PDhQzw8PIiKisrkyDLG1atXKVeuXLb4YZY7d272799P+fLlcXBw4MCBA5QuXVq9/969e5QsWVJ9Y312cO7cOby8vNiwYQOFCxemZ8+edO7cOVutUZecH3/8kalTp2JgYMDevXupX7++rkNKlTp16qRoKvcjR45kUkRpV61aNerWrcvkyZNRqVT89ttvTJo0iY0bN9KkSZNsl6DlpLYB0NfX59mzZ+TJk4fff/+dKVOmMGPGDCpXrsw///zD0KFDGTZsGCNGjNB1qEII8UHZ4xKsyHZMTU0JDg5ONkELDQ2VWbR0pGnTpixatIi//vqL2rVrs2nTJo0EbcOGDbi6uuowwtQpXrw4L168oFOnThw/flyjLtndvHnzmDNnDh07duTSpUsMHDiQNWvWZKs6Hjt2TNchpJsbN26wcuVKIDFxGTFiBPnz5+d///sf69atS3Z9tKwqJ7UNwLvXm9esWcPUqVPp0aMHAMWKFQPg119/lQRNCJHlSYImMkTVqlVZtGgRixYt0rp/wYIFVK1aNZOjSjs7O7sP7s8uV8wBpk2bRvXq1alduzYVKlRgxowZHDt2DE9PT3x9fTl79ixbt27VdZgpduvWLczNzVmxYoX6x7M2gYGBmRjVp2vSpAkXL15k+fLl/O9//yMqKoqhQ4dSpUoVJkyYID8ydcDY2Jjg4GCNbZ06dUKhUPDVV18xY8YM3QQm1P7rEXz06BHVqlXT2FetWjUePHigi7CEECJVJEETGeKnn36iTp06vH79muHDh+Ph4YFKpeLWrVvMmDGD7du3c/ToUV2HmWIxMTH079+fkiVLat3/8OFDJkyYkMlRpY2joyP//PMPU6dOZefOnahUKs6fP8/jx4+pXr06p06dokKFCroOM8WWLl2q6xAyREJCAj4+Pjg6OgKJvdKLFi2iRYsW9O7dO1skaFOnTmXgwIGYmZl9tOy5c+d49eoVzZs3z4TI0qZMmTIcPXqU8uXLa2zv0KEDKpWKbt266Siy1MtpbfOfP//8EwsLC4yMjJJclAkLC8uW96MKIT4/kqCJDFGtWjXWr19P37592bx5s8Y+W1tb1q5dS/Xq1XUUXeqVKVMGZ2fnZH+AXb16NdskaAA2NjZMnTqVqVOn6jqUT5adfhSnxsGDB7Vub968OdeuXcvkaNLm5s2bFCxYkPbt26sXebe3twcgPj5evcj7qlWrePr0KStWrNBxxB/Wv39//v77b637OnbsiEql4s8//8zkqNImp7UNQIECBdT//8bGxly+fFlj4qajR49StGhRXYUnhBApJpOEiAwVGRnJ/v37uXv3LpC4UHWjRo1SdNU2K5kyZQpxcXGMGzdO6/7Hjx8zduzYHNubIzJfVFQUBw8e1FjkvWHDhpiamuo4stS5evUq8+fPZ9OmTYSGhqKvr4+xsbF6EpqyZcvSu3dvunfvLvelZrLPrW3Onj2LsbExZcuW1XUoQgjxQZKgiQxx5MgRBgwYwNmzZ5NM3xwSEkK1atVYvHgxNWvW1FGEn6+2bdumqNyWLVsyOJL04eLikqKZ6O7du5dJEX26HTt20Lt3b169eqWxPXfu3Hh5edGyZUsdRZZ2SqUSHx8fHj58qF7kvUyZMuTOnVvXoaVaTkme/5OT2kYIIXICSdBEhmjVqhV169ZlyJAhWvfPnTuXo0ePZqvJKHKK/2Y1+8+aNWto2bIllpaWGtuzS2/gnDlzkt3n7+/PkiVLiImJyTYTuZw+fZo6derQqlUrhg0bhqenJ5A4JG3GjBns2rWL48ePU6VKFR1H+nnKiclzTnHnzh2Cg4OpVKmSetvhw4f55ZdfiIiIoE2bNvz44486jFAIIVIo89fGFp+DAgUKqG7evJns/lu3bqmcnZ0zMaJP4+vrqzp37pzGtkOHDqnq1Kmjqlixomry5Mk6iuzTWVhYqO7du6frMNLV69evVYMHD1YZGxuratWqpTpz5oyuQ0qxpk2bqvr27Zvs/r59+6qaNm2aiRGlj/j4eI3nZ8+eVR0/flwVGxuro4hS79SpUypDQ0NVu3btVKdPn1YFBQWpgoKCVKdOnVK1bdtWZWRklK3ea8np3r276smTJ7oOI9XatGmjGjNmjPr5/fv3VaampqpGjRqpBg4cqLKwsFDNmjVLdwEKIUQKSQ+ayBAmJiZcv3492fW0/Pz8KFmyZLZZqPqLL76gZMmSTJw4EYAHDx5QvHhxatasiYeHB97e3kyaNInBgwfrNtA0sLS05OrVqxQuXFjXoXyyqKgoZs6cye+//07BggWZMmUKzZo103VYqWJnZ8fx48eTnTHUx8eH2rVrExQUlMmRpc2zZ89o3749Z8+epXr16mzbto2vv/6aPXv2AODm5saxY8fIly+fjiP9uGbNmuHs7MySJUu07u/Xrx+PHz9W1y2r8/Hx0bq9QoUK6kXfAUqVKpWZYaWZs7MzGzZsUC/h8ssvv7Bp0yauXLkCgJeXF/PmzVM/F0KIrEpmcRQZwsnJ6YMJmo+PT7b4Qfafixcvakxrvnr1atzd3dm/fz+Q+ANm3rx52TJBywkSEhL4888/mTBhAiYmJsydO5cuXbp89N60rCgqKirJfZvvsra2Jjo6OhMj+jQjR45EpVKxdetWVq9eTYsWLdDX1+fx48ckJCTQqVMnJk+ezPz583Ud6kedPXuWadOmJbv/u+++o3bt2pkY0acpU6YMenp6aLtO265dO1QqFXp6etlmePCrV6/Inz+/+vnRo0c1hpzWqVOHYcOG6SI0IYRIFYWuAxA5U7NmzRgzZozWH5JRUVGMGzeOFi1a6CCytEnJid/f318HkYkNGzbg6enJ2LFjGTVqFL6+vnz99dfZMjmDxB6lI0eOJLv/8OHDuLm5ZWJEn+bQoUPMmDGDli1bsnDhQs6cOcO4ceNwcnKiQIECTJw4kb179+o6zBTJaclzqVKlaNq0KTdv3uTBgwc8ePCA+/fvo6+vz/79+9XPsws7OzuePXsGJE58cvHiRY17NWNjY7Umo0IIkdVID5rIED///DNbtmzB3d2dAQMGqNeeuX37NgsWLCAhIYGffvpJx1Gm3H8nfmdnZ/WJf+jQoer92enEv2PHDo3nSqWSw4cPc/36dY3trVq1ysyw0qxDhw6YmprSsWNHHj58yKhRo7SWmzlzZiZHljY9evRg+PDhODg4JBmeuXv3bkaMGJGtJjoICgrCyckJSPwcmZmZUbBgQfV+V1dX9Y/qrO6/5Pn9iXb+k92S5/PnzzNixAjatWvHqlWrNKafd3R01Gin7KBOnTpMmjSJhQsXsnHjRpRKJXXq1FHvv3nzJoUKFdJZfEIIkVKSoIkM4eDgwOnTp+nfvz+jR49WJy96eno0btyYBQsW4ODgoOMoUy4nnfjbtGmTZFu/fv00nmenYU21atX66DT62ak3bdCgQZw+fZoWLVpQtGhRPD09UalU3Lp1i7t379KmTZtsNZQ2T5486osbAAMGDMDOzk69PygoCHNzc12Flyo5LXk2MjJi9uzZ7N27l1atWvHtt98ycuRIXYeVZpMnT6Zhw4YULFgQfX195s6dq/HeWrlyJfXq1dNhhEIIkTIySYjIcEFBQfj5+aFSqXBzc8PW1lbXIaWav78/DRs25N69e+oTf//+/dX727Rpg4uLC7NmzdJhlCInWb9+PWvWrNFY5L1Dhw506NBBx5GlTuvWralXrx6DBg3Sun/BggVs2bKFw4cPZ3JkqadUKvnqq6/YvHlzssnzxo0bUSiy390Dz58/p0ePHoSHh3PmzBmuXr1KsWLFdB1WqsXHx3Pjxg3s7e1xdHTU2Hf16lXy589Prly5dBSdEEKkjCRoQqSQnPiFSH/nz5/HzMyMEiVK6DqUFMspybM2/61ROW/ePI37boUQQmQeSdCE+MzExMSgUCgwNDQE4N69e3h7e/Po0SMKFixIr169cHFx0XGUKTN16lQGDhyImZnZR8ueO3eOV69e0bx580yILO0UCsVHh2Tq6ekRHx+fSREJkTNs376dkJAQunbtqutQhBDigyRBEyIdZKcTf506dRgwYAD/+9//OHXqFPXr11cP17pz5w6+vr4cOnRIvZZQVta1a1f27t1L+/btadmyJRUqVMDe3h5I7PG8efMmJ0+eZNWqVTx9+pQVK1ZQq1YtHUf9Ydu3b09235kzZ5g7dy5KpTLbzBZ4584dgoODqVSpknrb4cOH+eWXX4iIiKBNmzbZ5r6tnJY856S2SQkPDw/u3r2bbe6vFUJ8viRBEyIdZKcTv7W1NRcvXsTNzY06depQrlw5jRkOx4wZw9GjRzl58qQOo0y5q1evMn/+fDZt2kRoaCj6+voYGxsTGRkJQNmyZenduzfdu3fHxMREx9Gmja+vL6NGjWLnzp107tyZiRMnZpsZ9nLSIu85LXnOSW0jhBA5iSRoQnxmLCwsuHjxIh4eHuTNm5f9+/dTunRp9f579+5RpkwZwsLCdBhl6imVSnx8fHj48CFRUVHkzp2bMmXKkDt3bl2HlmZPnz5l3LhxLF++nMaNG/Prr79mq3u1AJydndmwYYO6R/aXX35h06ZNXLlyBQAvLy/mzZunfp7dZOfkOae3jRBCZFfZb6opIcQnqVy5Mjt37gSgSJEiXL16VWP/lStXNKZBzy4UCgVlypShdevWdOjQgQYNGmTb5CwkJISRI0fi6urKjRs3OHz4MDt37sx2yRnk3EXenz59Sp8+fShZsiTx8fFcuXKF5cuXZ5vkDHJm27x48YIjR44QEhICJM5OOX36dKZOncq1a9d0HJ0QQqSMrIMmRAq9ePGC69evU758eaytrXn+/DnLly9HqVTSvHlzSpYsqesQU+SXX36hadOmRERE0LFjR4YNG8bdu3fx9PTE19eXuXPnMnr0aF2H+dmaPn0606ZNI2/evKxdu5bWrVvrOqRPkpMWeYfE5HnKlCnMmzePMmXKcPjwYWrWrKnrsNIkp7XNsWPHaNGiBZGRkTg4OLBv3z5atGiBqakpCoWC8ePHs2PHDho1aqTrUIUQ4oNkiKMQKfCxE7+/v3+2OvGfOXOGoUOHcu7cOY3tjo6O/PDDD8muWZUVxcXF8dNPP7Flyxbs7Oz45ptv6Nmzp3r/8+fPcXR0zBb3B0JiT6CpqSkNGjRAX18/2XJbtmzJxKjSrnPnzoSGhqoXeR83bhwBAQHqBYQ3b97MxIkTk/TkZkXvJs9TpkzJ9slzTmobgJo1a1K6dGmmTp3K4sWLmT17Nm3atGH+/PkA/PDDD5w+fZpTp07pOFIhhPgwSdCESIGceuJ/+fIl9+/fR6lUki9fPgoVKqTrkFJt/PjxLF68mOHDhxMcHEtJaFMAAA1tSURBVMz8+fP56quvWLJkCZCYoOXLlw+lUqnjSFOme/fuH50pEGDp0qWZEM2ny0mLvOe05DkntQ0kToB0+fJlihQpQnx8PKamply4cIEyZcoAcPfuXSpWrEhwcLBO4xRCiI+RBE2IFMhJJ/4jR44wYMAAzp49i5WVlca+kJAQqlWrxuLFi7PNsC03NzdmzZpFixYtAPDz86Np06bUqFEDb29vXrx4ka160HKinLLIe05LniHntA2Avb09x44do3jx4kRGRmJpacmpU6eoUqUKAD4+PtSvX5+XL1/qOFIhhPgwSdCESIGcdOJv1aoVdevWZciQIVr3z507l6NHj7J169ZMjixtzMzMuHnzpkbv35MnT6hXrx4VK1Zk+vTpODs7S4ImRA7Xpk0bEhISGDVqFCtWrODy5cs4ODiwfv169PT06NatG+Hh4ezdu1fXoQohxAfJLI5CpED16tUZNWoUp06dYsiQIZQrV069mGtkZCSTJk2iQoUKug4zRa5evUqTJk2S3d+oUSMuXbqUiRF9mrx583Lv3j2NbU5OThw9epQLFy7QvXt33QQmUmT79u2sWLFC12EILbJb2/z222/4+vpSs2ZNTpw4wbZt29DX18fGxgZra2uOHz/O5MmTdR2mEEJ8lPSgCZECd+/epXnz5vj5+eHh4cHBgwf59ttv2bNnDwC2trbs27ePcuXK6TjSjzMxMeH69eu4urpq3e/n50fJkiWJiorK5MjSpnfv3qhUKry8vJLse/LkCXXq1OH+/fvSg5ZFZadF3j832bVtXr9+rTEs8/Dhw0RFRVG1atVsM1xTCPF5kwRNiFTICSf+IkWKMGPGDNq0aaN1/5YtWxg+fDj379/P3MDS6OHDh9y+fZvGjRtr3f/06VMOHjxIt27dMjkyIYQQQojUkwRNiM/M999/z7Fjx7hw4QImJiYa+6KioqhUqRJ169Zl7ty5OopQCCHSJjY2lm3btnHmzBkCAgKAxGHQ1apVo3Xr1hgZGek4QiGE+DhJ0IRIoZxy4n/+/DnlypVDX1+fAQMGULRoUQBu377NggULSEhIUN9cnx3cuXOH4OBgKlWqpN52+PBh9T2Cbdq04ccff9RhhCKnLPKeE+WktvHz86Nx48Y8ffqUypUrq7/Dnj9/zrlz58ifPz979+5Ndni3EEJkFZKgCZECOe3E//DhQ/r378/+/fv57ytAT0+Pxo0bs2DBAlxcXHQcYcp98cUXlCxZkokTJwLw4MEDihcvTs2aNfHw8MDb25tJkyYxePBg3Qb6mcppi7znJDmtbRo2bIi5uTkrVqxIsoRIaGgoXbt2JSoqiv379+soQiGESBlJ0IRIgZx64g8KCsLPzw+VSoWbmxu2tra6DinVnJ2d2bBhA1WrVgXgl19+YdOmTVy5cgUALy8v5s2bp34uMldOXeQ9J8hpbWNmZsb58+cpUaKE1v3Xrl2jcuXKREZGZnJkQgiROpKgCZECcuLPukxNTblz5w7Ozs4A1K9fn2rVqjFp0iQA7t27R/ny5bPFIuI5UU5a5D2nyWlt4+joyB9//KFetP59O3fupF+/fjx9+jSTIxNCiNSRddCESAEbGxv8/f2T3e/v74+NjU2mxSPesrOz49mzZwAolUouXryoXkAcEu8dlOtQumNkZER0dDSQ2BZKpVL9HBInpjE0NNRVeJ+1nNY2vXv3pmvXrsyaNQsfHx+eP3/O8+fP8fHxYdasWXTv3p2+ffvqOkwhhPgo6UETIgXGjh3L/PnzGTNmDPXr19e4B+2/CSm+//57xo8fr9tAP0OdO3cmNDSUhQsXsnHjRsaNG0dAQADm5uYAbN68mYkTJ3L16lUdR/p5atOmDQkJCYwaNYoVK1aoJ6BZv349enp6dOvWjfDwcPbu3avrUD87ObFtpk2bxpw5cwgICEBPTw8AlUpF3rx5GTx4MCNGjNBxhEII8XGSoAmRQnLiz5r8/f1p2LAh9+7dQ19fn7lz59K/f3/1/jZt2uDi4sKsWbN0GOXnKyct8p7T5OS2efDggcZsu9lp4iMhhJAETYhUkhN/1hMfH8+NGzewt7fH0dFRY9/Vq1fJnz9/tllIPKfKCYu851Q5pW0KFy7MhQsXslXMQgihjSRoQqSAnPiFECJrUygUBAQEkCdPHl2HIoQQn8RA1wEIkR34+/uTkJCg6zBEGmzfvp2QkBC6du2q61A+WzllkfecSNpGCCGyHulBEyIF5Mps9uXh4cHdu3clwdaRnLbIe06S09pGoVCwfPlyrK2tP1iuVatWmRSREEKkjSRoQqSAnPiFSJucush7TpDT2kah+PjKQXp6enKxRgiR5UmCJkQKyIlfiLSRRd6zrpzWNjLSQQiRU8hC1UKkUEBAAEqlMtmHJGe68+LFC44cOUJISAiQOERr+vTpTJ06lWvXruk4us+bLPKedUnbCCFE1iSThAghsrVjx47RokULIiMjcXBwYN++fbRo0QJTU1MUCgXjx49nx44dNGrUSNehfpZ69+5N165dP7rIu8h80jZCCJE1yRBHIVJAhs5kXTVr1qR06dJMnTqVxYsXM3v2bNq0acP8+fMB+OGHHzh9+jSnTp3ScaSfL1nkPevKSW3z9ddf4+7uzr59+4iNjaV+/fqMGzcOU1NTXYcmhBCpIgmaECkgJ/6sy9ramsuXL1OkSBHi4+MxNTXlwoULlClTBoC7d+9SsWJFgoODdRqnkEXes7Kc0DaTJk1i/PjxNGjQAFNTU/bv30/Hjh3x9vbWdWhCCJEqMsRRiBRwd3fXOPHPmTOHFy9eyIk/CzAyMiI6OhpIXNNJqVSqnwNERUVhaGioq/A+e+8u8u7i4pItf/jnVDmtbVauXMnChQvp168fAIcOHaJ58+b89ddfKZroSQghsgrpQRMiBdzd3Rk2bFiSE39UVJSc+HWsTZs2JCQkMGrUKFasWMHly5dxcHBg/fr16Onp0a1bN8LDw9m7d6+uQ/0syfDgrCuntY2xsTF+fn44Ozurt5mYmODn50f+/Pl1GJkQQqSO/LIUIgUePnxIs2bN1M8bNGiAnp4eT58+1WFUAuC3337D19eXmjVrcuLECbZt24a+vj42NjZYW1tz/PhxJk+erOswhRAZLD4+HhMTE41thoaGxMXF6SgiIYRIGxniKEQKyIk/63Jzc+POnTu8fv2aXLlyAbB9+3YOHz5MVFQUVatWVW8XurF//35Z5D2Lyklto1Kp6N69O8bGxupt0dHRfPPNN5ibm6u3bdmyRRfhCSFEiskQRyFSQKFQ0LRpU40T/86dO6lXr56c+IX4AFnkPevKaW3To0ePFJVbunRpBkcihBCfRhI0IVJATvxZW2xsLNu2bePMmTMaM9FVq1aN1q1bY2RkpOMIP1857T6nnETaRgghsiZJ0IQQ2Zqfnx+NGzfm6dOnVK5cWWOx3XPnzpE/f3727t2Lq6urjiP9PEkSkHVJ2wghRNYkCZoQIltr2LAh5ubmrFixAisrK419oaGhdO3alaioKPbv36+jCD9vkgRkXdI2QgiRNUmCJoTI1szMzDh//jwlSpTQuv/atWtUrlyZyMjITI5MgCzynpVJ2wghRNYk0+wLIbI1Gxsb/P39k93v7++PjY1NpsUjNP23yLuFhQVOTk7MmTOH7777TtdhCaRthBAiq5IeNCFEtjZ27Fjmz5/PmDFjqF+/vsY9aIcPH+aXX37h+++/Z/z48boN9DMli7xnXdI2QgiRNUmCJoTI9qZNm8acOXMICAhAT08PSFwTKW/evAwePJgRI0boOMLPl7GxMX5+fjg7O6u3mZiY4OfnR/78+XUYmZC2EUKIrEkSNCFEjvHgwQONafZdXFx0HJHQ19cnICAAe3t79TZLS0t8fHykfXRM2kYIIbImSdCEENla4cKFuXDhArly5dJ1KEILWeQ965K2EUKIrMlA1wEIIcSn8Pf3JyEhQddhiGR069YtybYuXbroIBLxPmkbIYTImqQHTQiRrclaTkIIIYTISaQHTQiR7e3fvx9ra+sPlmnVqlUmRSOEEEIIkXbSgyaEyNZSMh24np6eDIMUQgghRLYgC50IIbK9gIAAlEplsg9JzoQQQgiRXUiCJoQQQgghhBBZhCRoQgghhBBCCJFFyD1oQohs7euvv8bd3Z19+/YRGxtL/fr1GTduHKamproOTQghhBAi1aQHTQiRrbm7uzN+/HgsLCxwcnJizpw5fPfdd7oOSwghhBAiTaQHTQiRrbm7uzNs2DD69esHwKFDh2jevDlRUVEpmuFRCCGEECIrkQRNCJGtGRsb4+fnh7Ozs3qbiYkJfn5+5M+fX4eRCSGEEEKknlxeFkJka/Hx8ZiYmGhsMzQ0JC4uTkcRCSGEEEKknYGuAxBCiE+hUqno3r07xsbG6m3R0dF88803mJubq7dt2bJFF+EJIYQQQqSKJGhCiGytW7duSbZ16dJFB5EIIYQQQnw6uQdNCCGEEEIIIbIIuQdNCCGEEEIIIbIISdCEEEIIIYQQIouQBE0IIYQQQgghsghJ0IQQQgghhBAii5AETQghhBBCCCGyCEnQhBBCCCGEECKLkARNCCGEEEIIIbKI/wMliwy2sFtP0wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap of features correlation\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=True)\n",
    "plt.title('Features Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58bd2d7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "58bd2d7b",
    "outputId": "f3a5eb0a-9c18-4757-d1c5-fd9206215519"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time series plot of CO concentration\n",
    "plt.figure(figsize=(10,6))\n",
    "df['CO(GT)'].plot()\n",
    "plt.title('Time series of CO concentration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89037cf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "89037cf5",
    "outputId": "c4962540-0fa3-40be-eda7-9b2a0ebf3c83"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-61649172-8760-4296-8f8d-cf894232437e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>18.00.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.7578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>19.00.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>0.7255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>20.00.00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>21.00.00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.7867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>22.00.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>0.7888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9352</th>\n",
       "      <td>04/04/2005</td>\n",
       "      <td>10.00.00</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.7568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9353</th>\n",
       "      <td>04/04/2005</td>\n",
       "      <td>11.00.00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.7119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9354</th>\n",
       "      <td>04/04/2005</td>\n",
       "      <td>12.00.00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.6406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9355</th>\n",
       "      <td>04/04/2005</td>\n",
       "      <td>13.00.00</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>961.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9356</th>\n",
       "      <td>04/04/2005</td>\n",
       "      <td>14.00.00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.5028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6941 rows Ã— 14 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61649172-8760-4296-8f8d-cf894232437e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-61649172-8760-4296-8f8d-cf894232437e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-61649172-8760-4296-8f8d-cf894232437e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            Date      Time  CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  \\\n",
       "0     10/03/2004  18.00.00     2.6       1360.0      11.9         1046.0   \n",
       "1     10/03/2004  19.00.00     2.0       1292.0       9.4          955.0   \n",
       "2     10/03/2004  20.00.00     2.2       1402.0       9.0          939.0   \n",
       "3     10/03/2004  21.00.00     2.2       1376.0       9.2          948.0   \n",
       "4     10/03/2004  22.00.00     1.6       1272.0       6.5          836.0   \n",
       "...          ...       ...     ...          ...       ...            ...   \n",
       "9352  04/04/2005  10.00.00     3.1       1314.0      13.5         1101.0   \n",
       "9353  04/04/2005  11.00.00     2.4       1163.0      11.4         1027.0   \n",
       "9354  04/04/2005  12.00.00     2.4       1142.0      12.4         1063.0   \n",
       "9355  04/04/2005  13.00.00     2.1       1003.0       9.5          961.0   \n",
       "9356  04/04/2005  14.00.00     2.2       1071.0      11.9         1047.0   \n",
       "\n",
       "      NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
       "0       166.0        1056.0    113.0        1692.0       1268.0  13.6  48.9   \n",
       "1       103.0        1174.0     92.0        1559.0        972.0  13.3  47.7   \n",
       "2       131.0        1140.0    114.0        1555.0       1074.0  11.9  54.0   \n",
       "3       172.0        1092.0    122.0        1584.0       1203.0  11.0  60.0   \n",
       "4       131.0        1205.0    116.0        1490.0       1110.0  11.2  59.6   \n",
       "...       ...           ...      ...           ...          ...   ...   ...   \n",
       "9352    472.0         539.0    190.0        1374.0       1729.0  21.9  29.3   \n",
       "9353    353.0         604.0    179.0        1264.0       1269.0  24.3  23.7   \n",
       "9354    293.0         603.0    175.0        1241.0       1092.0  26.9  18.3   \n",
       "9355    235.0         702.0    156.0        1041.0        770.0  28.3  13.5   \n",
       "9356    265.0         654.0    168.0        1129.0        816.0  28.5  13.1   \n",
       "\n",
       "          AH  \n",
       "0     0.7578  \n",
       "1     0.7255  \n",
       "2     0.7502  \n",
       "3     0.7867  \n",
       "4     0.7888  \n",
       "...      ...  \n",
       "9352  0.7568  \n",
       "9353  0.7119  \n",
       "9354  0.6406  \n",
       "9355  0.5139  \n",
       "9356  0.5028  \n",
       "\n",
       "[6941 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bedac",
   "metadata": {},
   "source": [
    "### Step 4: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd7d6fb",
   "metadata": {
    "id": "2bd7d6fb"
   },
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Select columns to normalize\n",
    "columns_to_normalize = df.columns.drop(['Date', 'Time'])\n",
    "# Normalize selected columns\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618724be",
   "metadata": {},
   "source": [
    "### Step 5: Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5fa5119",
   "metadata": {
    "id": "a5fa5119"
   },
   "outputs": [],
   "source": [
    "# split Datasets to training an testing\n",
    "\n",
    "features = df[['PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)']]\n",
    "target = df['CO(GT)']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float)\n",
    "\n",
    "X_train = torch.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = torch.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = torch.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P63-aiW1VYGe",
   "metadata": {
    "id": "P63-aiW1VYGe"
   },
   "source": [
    "### Step 6: Model Traning : **RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "THGs8ak39rBn",
   "metadata": {
    "id": "THGs8ak39rBn"
   },
   "outputs": [],
   "source": [
    "# Model Training: RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, i_size, h_size, n_layers, o_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=i_size,\n",
    "            hidden_size=h_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True)\n",
    "        self.out = nn.Linear(h_size, o_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        outs = []\n",
    "        for time_step in range(r_out.size(1)):\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return torch.stack(outs, dim=1), h_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAdlto5nOL22",
   "metadata": {
    "id": "FAdlto5nOL22"
   },
   "source": [
    "### Step 7: Hyperparameters setups  - 6 Setups (BY changing 3 parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61e072f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a61e072f",
    "outputId": "e37fe501-abea-43d1-b9da-215b898825b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.0433480404317379 | Validation Loss:  0.26243528723716736\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.25880900025367737 | Validation Loss:  0.01452941820025444\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.015961280092597008 | Validation Loss:  0.0458146333694458\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.05032956227660179 | Validation Loss:  0.053328413516283035\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.05792020261287689 | Validation Loss:  0.03121487982571125\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.03491869941353798 | Validation Loss:  0.014510874636471272\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.017255226150155067 | Validation Loss:  0.010608751326799393\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.012441395781934261 | Validation Loss:  0.01661963202059269\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.017707671970129013 | Validation Loss:  0.019651735201478004\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.020622117444872856 | Validation Loss:  0.014700624160468578\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.01633555442094803 | Validation Loss:  0.01337524689733982\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.016011513769626617 | Validation Loss:  0.014972819946706295\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.01794286258518696 | Validation Loss:  0.010724173858761787\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.012972859665751457 | Validation Loss:  0.008231237530708313\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.009314136579632759 | Validation Loss:  0.011232636868953705\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.011527211405336857 | Validation Loss:  0.008612338453531265\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.00889107771217823 | Validation Loss:  0.004025665577501059\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.004785092547535896 | Validation Loss:  0.0051856995560228825\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.006355526857078075 | Validation Loss:  0.004562913905829191\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.005559899844229221 | Validation Loss:  0.0022279333788901567\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.0026301888283342123 | Validation Loss:  0.0047372616827487946\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.004683493636548519 | Validation Loss:  0.004132373724132776\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.00409886846318841 | Validation Loss:  0.002242664573714137\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.0025405415799468756 | Validation Loss:  0.004240918438881636\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.0047685070894658566 | Validation Loss:  0.003211195347830653\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.003593367524445057 | Validation Loss:  0.0032665757462382317\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.0033490341156721115 | Validation Loss:  0.004645589739084244\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.004617094993591309 | Validation Loss:  0.0028426027856767178\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.002981643658131361 | Validation Loss:  0.0030552465468645096\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.0034494749270379543 | Validation Loss:  0.0031430223025381565\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.0035947952419519424 | Validation Loss:  0.0021857095416635275\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.002459316747263074 | Validation Loss:  0.0029557959642261267\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.0030457647517323494 | Validation Loss:  0.002872885437682271\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.002988782012835145 | Validation Loss:  0.002067301655188203\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.0024055272806435823 | Validation Loss:  0.002411116845905781\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.002958883997052908 | Validation Loss:  0.002385633997619152\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.002942032180726528 | Validation Loss:  0.002148405648767948\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.0025371606461703777 | Validation Loss:  0.002646861830726266\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.0028753674123436213 | Validation Loss:  0.0026073139160871506\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.0028314872179180384 | Validation Loss:  0.0021005079615861177\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.0024550785310566425 | Validation Loss:  0.0021540899761021137\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.0026252581737935543 | Validation Loss:  0.0021340379025787115\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.0025853519327938557 | Validation Loss:  0.001962142065167427\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.0022726417519152164 | Validation Loss:  0.0022095930762588978\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.0023814993910491467 | Validation Loss:  0.0022553238086402416\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.002397403120994568 | Validation Loss:  0.0019896423909813166\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.0022047804668545723 | Validation Loss:  0.0020421098452061415\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.0023344396613538265 | Validation Loss:  0.0020726905204355717\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.002357862889766693 | Validation Loss:  0.0020176281686872244\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.002216855064034462 | Validation Loss:  0.0021870366763323545\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.0023061649408191442 | Validation Loss:  0.002164401113986969\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.0022830097004771233 | Validation Loss:  0.001973650185391307\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.0021624222863465548 | Validation Loss:  0.001961427042260766\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.0022150941658765078 | Validation Loss:  0.0019199555972591043\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.002170924562960863 | Validation Loss:  0.0018970241071656346\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.0020898028742522 | Validation Loss:  0.0019926477689296007\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.0021392751950770617 | Validation Loss:  0.001946085598319769\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.002108950400725007 | Validation Loss:  0.001841632998548448\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.0020661395974457264 | Validation Loss:  0.0018396602245047688\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.0021088344510644674 | Validation Loss:  0.001826300285756588\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.0020814144518226385 | Validation Loss:  0.0018479542341083288\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0020489199087023735 | Validation Loss:  0.0019096768228337169\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.0020717778243124485 | Validation Loss:  0.00186337239574641\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.0020351267885416746 | Validation Loss:  0.0018003477016463876\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.002008956391364336 | Validation Loss:  0.0017957269446924329\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.002019333653151989 | Validation Loss:  0.0017900164239108562\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0019839839078485966 | Validation Loss:  0.0018265629187226295\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.001972402213141322 | Validation Loss:  0.0018551349639892578\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.001978490501642227 | Validation Loss:  0.0018111668759956956\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0019508814439177513 | Validation Loss:  0.0017877982463687658\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0019528432749211788 | Validation Loss:  0.0017852169694378972\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0019483864307403564 | Validation Loss:  0.0017925577703863382\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.0019257194362580776 | Validation Loss:  0.001819928060285747\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0019278659019619226 | Validation Loss:  0.0017974167130887508\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.001910993829369545 | Validation Loss:  0.0017574812518432736\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.0018963651964440942 | Validation Loss:  0.001743412809446454\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.0018943874165415764 | Validation Loss:  0.0017399461939930916\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.001875845598988235 | Validation Loss:  0.0017575835809111595\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0018714823527261615 | Validation Loss:  0.0017526996089145541\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0018647491233423352 | Validation Loss:  0.0017217152053490281\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0018517838325351477 | Validation Loss:  0.0017080979887396097\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.001850293017923832 | Validation Loss:  0.001707038376480341\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.001838124473579228 | Validation Loss:  0.0017212830716744065\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.001830489025451243 | Validation Loss:  0.0017232088139280677\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0018241438083350658 | Validation Loss:  0.0017014621989801526\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0018117285799235106 | Validation Loss:  0.0016902241623029113\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0018073144601657987 | Validation Loss:  0.0016912033315747976\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0017970410408452153 | Validation Loss:  0.0017037384677678347\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.0017902501858770847 | Validation Loss:  0.0017047859728336334\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0017844178946688771 | Validation Loss:  0.0016882922500371933\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.001775595243088901 | Validation Loss:  0.0016797976568341255\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.0017713841516524553 | Validation Loss:  0.0016810847446322441\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.0017625433392822742 | Validation Loss:  0.0016887690871953964\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.0017571077914908528 | Validation Loss:  0.0016819748561829329\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.0017498682718724012 | Validation Loss:  0.0016666104784235358\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.0017430064035579562 | Validation Loss:  0.0016602030955255032\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.0017375429160892963 | Validation Loss:  0.0016631947364658117\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.0017303231870755553 | Validation Loss:  0.001666192663833499\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0017258076695725322 | Validation Loss:  0.001655836240388453\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.001719085848890245 | Validation Loss:  0.0016466259257867932\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0017145380843430758 | Validation Loss:  0.0016463934443891048\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0017084635328501463 | Validation Loss:  0.0016519491327926517\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0017034035408869386 | Validation Loss:  0.0016492981230840087\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.001697968109510839 | Validation Loss:  0.0016406791983172297\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0016926885582506657 | Validation Loss:  0.0016385584603995085\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.001687905052676797 | Validation Loss:  0.0016431784024462104\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0016827291110530496 | Validation Loss:  0.0016445195069536567\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.001678453292697668 | Validation Loss:  0.0016376072308048606\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0016735338140279055 | Validation Loss:  0.001633883686736226\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0016695138765498996 | Validation Loss:  0.0016363458707928658\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0016647581942379475 | Validation Loss:  0.0016378414584323764\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0016608349978923798 | Validation Loss:  0.0016317198751494288\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0016563048120588064 | Validation Loss:  0.0016272133216261864\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0016525459941476583 | Validation Loss:  0.0016285581514239311\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0016483132494613528 | Validation Loss:  0.0016298627015203238\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.0016447548987343907 | Validation Loss:  0.0016249642940238118\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0016408114461228251 | Validation Loss:  0.0016215540235862136\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0016374002443626523 | Validation Loss:  0.0016234073555096984\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0016336431726813316 | Validation Loss:  0.0016247049206867814\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.001630305894650519 | Validation Loss:  0.0016209176974371076\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0016267279861494899 | Validation Loss:  0.0016191272297874093\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.0016234926879405975 | Validation Loss:  0.0016215558862313628\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.0016201222315430641 | Validation Loss:  0.0016219716053456068\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0016169960144907236 | Validation Loss:  0.0016186153516173363\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0016138385981321335 | Validation Loss:  0.001618008827790618\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0016107821138575673 | Validation Loss:  0.001619993825443089\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0016077759210020304 | Validation Loss:  0.001618538168258965\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0016047591343522072 | Validation Loss:  0.001615552813746035\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0016018847236409783 | Validation Loss:  0.00161594501696527\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0015989368548616767 | Validation Loss:  0.0016167531721293926\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.001596168614923954 | Validation Loss:  0.0016141472151502967\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.001593329361639917 | Validation Loss:  0.0016127551207318902\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.001590621075592935 | Validation Loss:  0.0016140886582434177\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.0015878877602517605 | Validation Loss:  0.001613415079191327\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0015851928619667888 | Validation Loss:  0.0016113321762531996\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0015825560549274087 | Validation Loss:  0.0016119255451485515\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0015798897948116064 | Validation Loss:  0.0016126622213050723\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.0015773208579048514 | Validation Loss:  0.0016108342679217458\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0015747294528409839 | Validation Loss:  0.001610331004485488\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.001572192762978375 | Validation Loss:  0.0016113040037453175\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0015696838963776827 | Validation Loss:  0.0016100211068987846\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0015671673463657498 | Validation Loss:  0.0016086653340607882\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0015647128457203507 | Validation Loss:  0.00160926952958107\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.0015622497303411365 | Validation Loss:  0.0016085464740172029\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0015598211903125048 | Validation Loss:  0.001606916543096304\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0015574321150779724 | Validation Loss:  0.0016072063008323312\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.001555040362291038 | Validation Loss:  0.0016069967532530427\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.001552692032419145 | Validation Loss:  0.0016055259620770812\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0015503618633374572 | Validation Loss:  0.0016056762542575598\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0015480400761589408 | Validation Loss:  0.001605781726539135\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.001545759616419673 | Validation Loss:  0.0016045267693698406\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0015434948727488518 | Validation Loss:  0.0016045693773776293\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0015412491047754884 | Validation Loss:  0.001604696735739708\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0015390407061204314 | Validation Loss:  0.0016035133739933372\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.0015368489548563957 | Validation Loss:  0.0016034463187679648\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.0015346769941970706 | Validation Loss:  0.0016034224536269903\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0015325387939810753 | Validation Loss:  0.0016022826312109828\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0015304210828617215 | Validation Loss:  0.0016022494528442621\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0015283249085769057 | Validation Loss:  0.0016021041665226221\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.0015262612141668797 | Validation Loss:  0.0016011439729481936\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.001524222083389759 | Validation Loss:  0.0016013090498745441\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0015222026268020272 | Validation Loss:  0.001601067720912397\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0015202120412141085 | Validation Loss:  0.0016003951895982027\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.0015182492788881063 | Validation Loss:  0.001600724644958973\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0015163080533966422 | Validation Loss:  0.0016003042692318559\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.00151439243927598 | Validation Loss:  0.0015999440802261233\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0015125052304938436 | Validation Loss:  0.0016002163756638765\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.0015106431674212217 | Validation Loss:  0.0015996051952242851\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0015088035725057125 | Validation Loss:  0.0015995792346075177\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.001506989006884396 | Validation Loss:  0.0015995759749785066\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0015052016824483871 | Validation Loss:  0.0015990352258086205\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.0015034388052299619 | Validation Loss:  0.0015992945991456509\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0015016988618299365 | Validation Loss:  0.0015989751555025578\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.0014999826671555638 | Validation Loss:  0.0015988764353096485\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.001498291501775384 | Validation Loss:  0.0015990904066711664\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.0014966243179515004 | Validation Loss:  0.0015987257938832045\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.0014949801843613386 | Validation Loss:  0.0015990594401955605\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.001493358053267002 | Validation Loss:  0.0015988701488822699\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0014917589724063873 | Validation Loss:  0.0015989202074706554\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0014901827089488506 | Validation Loss:  0.0015990727115422487\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.0014886290300637484 | Validation Loss:  0.001598841161467135\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.0014870973536744714 | Validation Loss:  0.0015991852851584554\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0014855873305350542 | Validation Loss:  0.0015989033272489905\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0014840980293229222 | Validation Loss:  0.0015992448898032308\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.001482629799284041 | Validation Loss:  0.0015990962274372578\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.0014811822911724448 | Validation Loss:  0.0015993451233953238\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.0014797551557421684 | Validation Loss:  0.0015993760898709297\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.00147834827657789 | Validation Loss:  0.0015995213761925697\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.0014769615372642875 | Validation Loss:  0.0015996971633285284\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.0014755942393094301 | Validation Loss:  0.001599739189259708\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0014742463827133179 | Validation Loss:  0.0016000336036086082\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0014729176182299852 | Validation Loss:  0.0015999417519196868\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0014716078294441104 | Validation Loss:  0.001600411138497293\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.0014703168999403715 | Validation Loss:  0.0016000656178221107\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.001469045295380056 | Validation Loss:  0.001600951887667179\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.00146779406350106 | Validation Loss:  0.0015999686438590288\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.001466568442992866 | Validation Loss:  0.0016020160401239991\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0014653813559561968 | Validation Loss:  0.0015992034459486604\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0014642768073827028 | Validation Loss:  0.001604962395504117\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0014633950777351856 | Validation Loss:  0.0015968193765729666\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0014632255770266056 | Validation Loss:  0.0016172449104487896\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.0014654920669272542 | Validation Loss:  0.0015983195044100285\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0014766918029636145 | Validation Loss:  0.0017036001663655043\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0015215736348181963 | Validation Loss:  0.0017660310259088874\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0016999335493892431 | Validation Loss:  0.0026647334452718496\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.002377524506300688 | Validation Loss:  0.004574201535433531\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.004741099663078785 | Validation Loss:  0.009333129972219467\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.008813775144517422 | Validation Loss:  0.008708962239325047\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.009129360318183899 | Validation Loss:  0.0016589546576142311\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0015558821614831686 | Validation Loss:  0.0066428459249436855\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.006214335095137358 | Validation Loss:  0.0021989187225699425\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.002353816060349345 | Validation Loss:  0.004302723798900843\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0046708351001143456 | Validation Loss:  0.0016777762211859226\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.0016543270321562886 | Validation Loss:  0.0043130554258823395\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.004019831772893667 | Validation Loss:  0.0019613010808825493\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.0018798663513734937 | Validation Loss:  0.00244062184356153\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.002681841840967536 | Validation Loss:  0.002565596019849181\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0028176377527415752 | Validation Loss:  0.0016754900570958853\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0016671649646013975 | Validation Loss:  0.003031158586964011\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.002839433029294014 | Validation Loss:  0.0021296460181474686\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.002038160338997841 | Validation Loss:  0.0017895200289785862\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0019375546835362911 | Validation Loss:  0.0022860507015138865\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.002536276588216424 | Validation Loss:  0.0016899614129215479\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.0018187797395512462 | Validation Loss:  0.0020441855303943157\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0019917204044759274 | Validation Loss:  0.002361447084695101\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0022703877184540033 | Validation Loss:  0.0016992167802527547\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.00174131675157696 | Validation Loss:  0.00178890663664788\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0019850360695272684 | Validation Loss:  0.0018781955586746335\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.002098194556310773 | Validation Loss:  0.0016279257833957672\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.001735355006530881 | Validation Loss:  0.0019174062181264162\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.001897622481919825 | Validation Loss:  0.002018317114561796\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.001976947532966733 | Validation Loss:  0.0016632655169814825\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0017097776290029287 | Validation Loss:  0.0016974457539618015\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.00184029305819422 | Validation Loss:  0.0017386783147230744\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0018916681874543428 | Validation Loss:  0.0016247713938355446\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0016992921009659767 | Validation Loss:  0.0017926373984664679\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.0017798723420128226 | Validation Loss:  0.0018524512415751815\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0018234350718557835 | Validation Loss:  0.0016481403727084398\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0016789808869361877 | Validation Loss:  0.0016344510950148106\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0017350990092381835 | Validation Loss:  0.0016546061960980296\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0017678161384537816 | Validation Loss:  0.0016022645868360996\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.001663643168285489 | Validation Loss:  0.001695385086350143\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0016927270917221904 | Validation Loss:  0.001745156361721456\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0017241250025108457 | Validation Loss:  0.0016294269589707255\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.0016448404639959335 | Validation Loss:  0.0015980019234120846\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.0016613712068647146 | Validation Loss:  0.0016121513908728957\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0016852308763191104 | Validation Loss:  0.0015915140975266695\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.0016266402089968324 | Validation Loss:  0.0016484064981341362\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0016336910193786025 | Validation Loss:  0.0016855750000104308\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.00165267544798553 | Validation Loss:  0.001617597066797316\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0016076213214546442 | Validation Loss:  0.0015900303842499852\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.001613613567315042 | Validation Loss:  0.001594503060914576\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.001624882104806602 | Validation Loss:  0.0015876404941082\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0015904601896181703 | Validation Loss:  0.0016275643138214946\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.001595427980646491 | Validation Loss:  0.0016437314916402102\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.0016013135900720954 | Validation Loss:  0.0015977779403328896\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0015746941789984703 | Validation Loss:  0.0015795069048181176\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0015803829301148653 | Validation Loss:  0.0015793554484844208\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.0015813023783266544 | Validation Loss:  0.0015834236983209848\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0015614686999469995 | Validation Loss:  0.00161466165445745\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.001567216357216239 | Validation Loss:  0.0016159606166183949\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0015642408980056643 | Validation Loss:  0.001585990423336625\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0015499555738642812 | Validation Loss:  0.0015780269168317318\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0015556075377389789 | Validation Loss:  0.0015785073628649116\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0015497066779062152 | Validation Loss:  0.0015908360946923494\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0015406751772388816 | Validation Loss:  0.0016108631389215589\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0015452188672497869 | Validation Loss:  0.001599834649823606\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0015375320799648762 | Validation Loss:  0.0015817377716302872\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0015331866452470422 | Validation Loss:  0.0015787214506417513\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0015353981871157885 | Validation Loss:  0.0015819586114957929\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.0015276510966941714 | Validation Loss:  0.0015970012173056602\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0015267643611878157 | Validation Loss:  0.0016014224383980036\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.0015262741362676024 | Validation Loss:  0.0015870010247454047\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.0015200633788481355 | Validation Loss:  0.0015796266961842775\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0015208825934678316 | Validation Loss:  0.0015806554583832622\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.0015179940965026617 | Validation Loss:  0.0015899743884801865\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.0015142647316679358 | Validation Loss:  0.0015993774868547916\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.0015148930251598358 | Validation Loss:  0.0015926833730190992\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.001510904054157436 | Validation Loss:  0.0015842189313843846\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.0015095972921699286 | Validation Loss:  0.0015835072845220566\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0015088096261024475 | Validation Loss:  0.0015891618095338345\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.0015053593087941408 | Validation Loss:  0.0015976104186847806\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0015052161179482937 | Validation Loss:  0.001595203415490687\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0015029780333861709 | Validation Loss:  0.0015875049866735935\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0015010531060397625 | Validation Loss:  0.0015854116063565016\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0015005709137767553 | Validation Loss:  0.0015890251379460096\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0014980046544224024 | Validation Loss:  0.0015957128489390016\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0014973082579672337 | Validation Loss:  0.0015952640678733587\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.001495828153565526 | Validation Loss:  0.001589426421560347\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0014940572436898947 | Validation Loss:  0.0015874115051701665\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0014934538630768657 | Validation Loss:  0.001590376952663064\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0014915223000571132 | Validation Loss:  0.0015957951545715332\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.00149065803270787 | Validation Loss:  0.0015958870062604547\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0014894255436956882 | Validation Loss:  0.0015915264375507832\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0014879693044349551 | Validation Loss:  0.001589898718520999\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0014872392639517784 | Validation Loss:  0.0015923746395856142\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.0014856993220746517 | Validation Loss:  0.0015963712939992547\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.0014848561258986592 | Validation Loss:  0.001595937181264162\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.0014837011694908142 | Validation Loss:  0.001592420507222414\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.001482530264183879 | Validation Loss:  0.0015913230599835515\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Test Loss:  0.0012869234196841717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.14617960155010223 | Validation Loss:  0.02272380143404007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.0233427993953228 | Validation Loss:  0.013945214450359344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.015303298830986023 | Validation Loss:  0.03373272716999054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.03758110851049423 | Validation Loss:  0.008164940401911736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.009431888349354267 | Validation Loss:  0.029092442244291306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.028672587126493454 | Validation Loss:  0.009617118164896965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.009880874305963516 | Validation Loss:  0.008262399584054947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.009668737649917603 | Validation Loss:  0.013508597388863564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.015052525326609612 | Validation Loss:  0.004181974567472935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.004950988106429577 | Validation Loss:  0.004664357751607895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.0046353526413440704 | Validation Loss:  0.01023368164896965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.00990432221442461 | Validation Loss:  0.005338564515113831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.005217238795012236 | Validation Loss:  0.0020447978749871254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.002384610939770937 | Validation Loss:  0.004765799269080162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.0054909405298531055 | Validation Loss:  0.005922522395849228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.006723733153194189 | Validation Loss:  0.0034118255134671926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.003982260823249817 | Validation Loss:  0.0019157432252541184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.0021193192806094885 | Validation Loss:  0.004096405114978552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.004014231730252504 | Validation Loss:  0.00511223915964365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.0049704923294484615 | Validation Loss:  0.0029002102091908455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.002939257537946105 | Validation Loss:  0.0019622433464974165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.0023201957810670137 | Validation Loss:  0.002982994308695197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.0036051287315785885 | Validation Loss:  0.003361021401360631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.004031831864267588 | Validation Loss:  0.002428950509056449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.002925841836258769 | Validation Loss:  0.0019256850937381387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.0021552920807152987 | Validation Loss:  0.002891296287998557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.0029185086023062468 | Validation Loss:  0.0034368291962891817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.0034127417020499706 | Validation Loss:  0.0025111401919275522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.002584664849564433 | Validation Loss:  0.001915407832711935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.002175625879317522 | Validation Loss:  0.0022455831058323383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.0026781167834997177 | Validation Loss:  0.002448704792186618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.0029411804862320423 | Validation Loss:  0.002103255596011877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.0025170377921313047 | Validation Loss:  0.001874543959274888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.0021279831416904926 | Validation Loss:  0.00225143413990736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.002358036581426859 | Validation Loss:  0.0025899894535541534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.0026361909694969654 | Validation Loss:  0.0022835414856672287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.0023733098059892654 | Validation Loss:  0.0018850202905014157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.002086991909891367 | Validation Loss:  0.001896981499157846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.0022118978668004274 | Validation Loss:  0.0020296890288591385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.00239419168792665 | Validation Loss:  0.001955498941242695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.0022831487003713846 | Validation Loss:  0.0018324300181120634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.0020634448155760765 | Validation Loss:  0.0019472590647637844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.0020762826316058636 | Validation Loss:  0.0021505814511328936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.0022212902549654245 | Validation Loss:  0.0020925523713231087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.0021696919575333595 | Validation Loss:  0.0018729452276602387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.0020098877139389515 | Validation Loss:  0.0017936688382178545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.002006844850257039 | Validation Loss:  0.0018348577432334423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.0020962406415492296 | Validation Loss:  0.0018242707010358572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.0020792356226593256 | Validation Loss:  0.0017738027963787317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.0019737810362130404 | Validation Loss:  0.0018150745891034603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.0019438688177615404 | Validation Loss:  0.0019207852892577648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.0019993868190795183 | Validation Loss:  0.001923494623042643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.0019936389289796352 | Validation Loss:  0.001819609198719263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.0019192257896065712 | Validation Loss:  0.0017560651758685708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.001900157192721963 | Validation Loss:  0.001759420381858945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.0019342901650816202 | Validation Loss:  0.0017507816664874554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.0019240833353251219 | Validation Loss:  0.0017304272623732686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.0018712320597842336 | Validation Loss:  0.0017597476253286004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.00185651786159724 | Validation Loss:  0.0018128445371985435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.0018785647116601467 | Validation Loss:  0.0018037427216768265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.0018658617045730352 | Validation Loss:  0.0017430089646950364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0018256108742207289 | Validation Loss:  0.0017086968291550875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.0018181975465267897 | Validation Loss:  0.0017079024109989405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.0018305324483662844 | Validation Loss:  0.0017022715182974935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.0018142902990803123 | Validation Loss:  0.001702129957266152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.0017856515478342772 | Validation Loss:  0.0017292940756306052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0017832795856520534 | Validation Loss:  0.0017468332080170512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.00178658333607018 | Validation Loss:  0.0017207254422828555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0017670163651928306 | Validation Loss:  0.0016841950127854943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0017494269413873553 | Validation Loss:  0.001670419704169035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0017506822478026152 | Validation Loss:  0.0016668421449139714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.00174537836574018 | Validation Loss:  0.001666652737185359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.0017262652982026339 | Validation Loss:  0.0016821363242343068\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0017171716317534447 | Validation Loss:  0.0016976826591417193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.0017169195925816894 | Validation Loss:  0.0016857725568115711\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.0017051032045856118 | Validation Loss:  0.0016600006492808461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.0016908845864236355 | Validation Loss:  0.0016460033366456628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.001687615062110126 | Validation Loss:  0.0016411688411608338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0016817247960716486 | Validation Loss:  0.0016417265869677067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0016681579872965813 | Validation Loss:  0.0016524982638657093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0016606501303613186 | Validation Loss:  0.0016595355700701475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.0016565615078434348 | Validation Loss:  0.0016478956677019596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.001645573298446834 | Validation Loss:  0.001631111023016274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.0016360841691493988 | Validation Loss:  0.0016230768524110317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.001631784369237721 | Validation Loss:  0.0016207542503252625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0016230863984674215 | Validation Loss:  0.0016245115548372269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0016129827126860619 | Validation Loss:  0.001631487044505775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0016075618332251906 | Validation Loss:  0.001627909718081355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.0016000224277377129 | Validation Loss:  0.0016149418661370873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0015905306208878756 | Validation Loss:  0.0016059314366430044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.001584693556651473 | Validation Loss:  0.0016030686674639583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.001577584887854755 | Validation Loss:  0.001605362631380558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.0015686145052313805 | Validation Loss:  0.001610489678569138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.001562592457048595 | Validation Loss:  0.0016081954818218946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.0015556503785774112 | Validation Loss:  0.0015986396465450525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.0015474838437512517 | Validation Loss:  0.001591913285665214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.0015417308313772082 | Validation Loss:  0.0015901103615760803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.001534864422865212 | Validation Loss:  0.0015926682390272617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0015275549376383424 | Validation Loss:  0.001595535664819181\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0015221007633954287 | Validation Loss:  0.0015915436670184135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0015154287684708834 | Validation Loss:  0.00158495653886348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0015093338442966342 | Validation Loss:  0.0015820900443941355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0015042758313938975 | Validation Loss:  0.001583040226250887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.0014981550630182028 | Validation Loss:  0.0015861716819927096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0014932663179934025 | Validation Loss:  0.001585487276315689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.0014884108677506447 | Validation Loss:  0.0015810425393283367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0014834117610007524 | Validation Loss:  0.0015787690645083785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.0014795674942433834 | Validation Loss:  0.001579763600602746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0014751413837075233 | Validation Loss:  0.0015827311435714364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0014715156285092235 | Validation Loss:  0.0015829292824491858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0014681103639304638 | Validation Loss:  0.0015801945701241493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0014646865893155336 | Validation Loss:  0.0015791960759088397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.001462040119804442 | Validation Loss:  0.0015807733871042728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0014590286882594228 | Validation Loss:  0.0015833154320716858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0014567020116373897 | Validation Loss:  0.0015829985495656729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.001454267418012023 | Validation Loss:  0.0015812577912583947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0014521029079332948 | Validation Loss:  0.0015812736237421632\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0014501286204904318 | Validation Loss:  0.0015830459306016564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0014480557292699814 | Validation Loss:  0.0015841100830584764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.0014463391853496432 | Validation Loss:  0.0015826369635760784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0014443910913541913 | Validation Loss:  0.0015814682701602578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.0014427714049816132 | Validation Loss:  0.0015819452237337828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.0014409255236387253 | Validation Loss:  0.0015829299809411168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0014393040910363197 | Validation Loss:  0.0015819113468751311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0014375512255355716 | Validation Loss:  0.0015800684923306108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0014359155902639031 | Validation Loss:  0.001579475705511868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0014342356007546186 | Validation Loss:  0.0015798152890056372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0014325965894386172 | Validation Loss:  0.0015789472963660955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0014309908729046583 | Validation Loss:  0.0015768788289278746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0014293915592133999 | Validation Loss:  0.0015757075743749738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.001427871990017593 | Validation Loss:  0.0015756296925246716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.0014263472985476255 | Validation Loss:  0.0015749161830171943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.0014249319210648537 | Validation Loss:  0.0015730076702311635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.0014235175913199782 | Validation Loss:  0.001571711152791977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.001422222936525941 | Validation Loss:  0.0015714936889708042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0014209361979737878 | Validation Loss:  0.0015709379222244024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0014197693672031164 | Validation Loss:  0.0015693469904363155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.0014186191838234663 | Validation Loss:  0.0015682376688346267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.001417580060660839 | Validation Loss:  0.0015681360382586718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.0014165608445182443 | Validation Loss:  0.001567807630635798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0014156397664919496 | Validation Loss:  0.0015666127437725663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0014147410402074456 | Validation Loss:  0.0015658462652936578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0014139246195554733 | Validation Loss:  0.0015659474302083254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.0014131293864920735 | Validation Loss:  0.001565792947076261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0014124008594080806 | Validation Loss:  0.0015649431152269244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0014116927050054073 | Validation Loss:  0.0015645493986085057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0014110368210822344 | Validation Loss:  0.001564858597703278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0014104003785178065 | Validation Loss:  0.0015648239059373736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0014098024694249034 | Validation Loss:  0.0015642816433683038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0014092251658439636 | Validation Loss:  0.0015642426442354918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.0014086756855249405 | Validation Loss:  0.0015646908432245255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0014081476256251335 | Validation Loss:  0.0015646894462406635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0014076385414227843 | Validation Loss:  0.0015644041122868657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0014071529731154442 | Validation Loss:  0.001564654754474759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.0014066798612475395 | Validation Loss:  0.0015651367139071226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.0014062313130125403 | Validation Loss:  0.001565128332003951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0014057914959266782 | Validation Loss:  0.0015650872373953462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0014053761260583997 | Validation Loss:  0.0015655368333682418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0014049685560166836 | Validation Loss:  0.0015659339260309935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.0014045829884707928 | Validation Loss:  0.0015659126220270991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.0014042062684893608 | Validation Loss:  0.001566088292747736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0014038477092981339 | Validation Loss:  0.001566592836752534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0014035003259778023 | Validation Loss:  0.0015668215928599238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.001403166214004159 | Validation Loss:  0.0015668413834646344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0014028452569618821 | Validation Loss:  0.0015671777073293924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.0014025336131453514 | Validation Loss:  0.0015675805043429136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0014022354735061526 | Validation Loss:  0.0015676451148465276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.001401944668032229 | Validation Loss:  0.0015677601331844926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0014016658533364534 | Validation Loss:  0.0015681309159845114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0014013933250680566 | Validation Loss:  0.001568320207297802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0014011302264407277 | Validation Loss:  0.00156831881031394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.001400874461978674 | Validation Loss:  0.0015685319667682052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0014006247511133552 | Validation Loss:  0.0015687862178310752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.001400382723659277 | Validation Loss:  0.0015687852865085006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.0014001451199874282 | Validation Loss:  0.0015688330167904496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.0013999140355736017 | Validation Loss:  0.0015690495492890477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.001399687840603292 | Validation Loss:  0.0015691008884459734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.0013994663022458553 | Validation Loss:  0.0015690481523051858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0013992500025779009 | Validation Loss:  0.001569168409332633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.001399037311784923 | Validation Loss:  0.0015692620072513819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.0013988298596814275 | Validation Loss:  0.0015691837761551142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.0013986255507916212 | Validation Loss:  0.0015692021697759628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0013984257820993662 | Validation Loss:  0.0015692999586462975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0013982298551127315 | Validation Loss:  0.001569243730045855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.001398037071339786 | Validation Loss:  0.001569190644659102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.0013978485949337482 | Validation Loss:  0.0015692624729126692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.0013976632617413998 | Validation Loss:  0.001569245709106326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.001397481421008706 | Validation Loss:  0.0015691624721512198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.001397303189150989 | Validation Loss:  0.001569196698255837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.0013971277512609959 | Validation Loss:  0.001569212879985571\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0013969558058306575 | Validation Loss:  0.0015691317385062575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.001396786654368043 | Validation Loss:  0.0015691317385062575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0013966202968731523 | Validation Loss:  0.0015691673615947366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.001396456966176629 | Validation Loss:  0.0015691075241193175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.001396296196617186 | Validation Loss:  0.0015690852887928486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.0013961379881948233 | Validation Loss:  0.0015691276639699936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.0013959822244942188 | Validation Loss:  0.001569094485603273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.001395828789100051 | Validation Loss:  0.0015690611908212304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0013956776820123196 | Validation Loss:  0.0015691013541072607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0013955290196463466 | Validation Loss:  0.0015690919244661927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.001395382103510201 | Validation Loss:  0.001569056767039001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.0013952371664345264 | Validation Loss:  0.0015690898289903998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.001395094208419323 | Validation Loss:  0.001569097046740353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0013949532294645905 | Validation Loss:  0.0015690645668655634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0013948139967396855 | Validation Loss:  0.0015690886648371816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.001394676510244608 | Validation Loss:  0.00156910449732095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0013945407699793577 | Validation Loss:  0.0015690752770751715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.0013944064266979694 | Validation Loss:  0.0015690891304984689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.0013942739460617304 | Validation Loss:  0.0015691068256273866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0013941426295787096 | Validation Loss:  0.001569079700857401\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.001394013175740838 | Validation Loss:  0.0015690840082243085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.0013938848860561848 | Validation Loss:  0.0015690983273088932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0013937578769400716 | Validation Loss:  0.0015690706204622984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.0013936321483924985 | Validation Loss:  0.0015690649161115289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0013935079332441092 | Validation Loss:  0.0015690724831074476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.0013933846494182944 | Validation Loss:  0.001569041982293129\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.001393262529745698 | Validation Loss:  0.0015690269647166133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0013931415742263198 | Validation Loss:  0.0015690254513174295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0013930217828601599 | Validation Loss:  0.0015689902938902378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0013929028064012527 | Validation Loss:  0.001568966661579907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.001392784877680242 | Validation Loss:  0.001568955252878368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0013926677638664842 | Validation Loss:  0.0015689138090237975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.0013925516977906227 | Validation Loss:  0.001568882493302226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.0013924363302066922 | Validation Loss:  0.0015688614221289754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0013923218939453363 | Validation Loss:  0.0015688138082623482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0013922081561759114 | Validation Loss:  0.0015687758568674326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.0013920953497290611 | Validation Loss:  0.0015687452396377921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0013919828925281763 | Validation Loss:  0.0015686919214203954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0013918715994805098 | Validation Loss:  0.0015686480328440666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0013917605392634869 | Validation Loss:  0.0015686089172959328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.0013916504103690386 | Validation Loss:  0.0015685503603890538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.0013915408635511994 | Validation Loss:  0.0015685020480304956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0013914318988099694 | Validation Loss:  0.0015684551326557994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0013913233997300267 | Validation Loss:  0.0015683919191360474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0013912154827266932 | Validation Loss:  0.001568339765071869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.001391108031384647 | Validation Loss:  0.0015682856319472194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.001391001045703888 | Validation Loss:  0.0015682190423831344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0013908949913457036 | Validation Loss:  0.0015681632794439793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.001390789053402841 | Validation Loss:  0.0015681025106459856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0013906836975365877 | Validation Loss:  0.0015680333599448204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0013905786909162998 | Validation Loss:  0.0015679739881306887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.0013904741499572992 | Validation Loss:  0.00156790716573596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0013903698418289423 | Validation Loss:  0.0015678361523896456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0013902663486078382 | Validation Loss:  0.0015677729388698936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.001390162855386734 | Validation Loss:  0.0015677007613703609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.0013900598278269172 | Validation Loss:  0.0015676278853788972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0013899571495130658 | Validation Loss:  0.0015675602480769157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.0013898549368605018 | Validation Loss:  0.0015674832975491881\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0013897529570385814 | Validation Loss:  0.0015674085589125752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0013896514428779483 | Validation Loss:  0.001567335450090468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.001389550045132637 | Validation Loss:  0.0015672545414417982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.001389449113048613 | Validation Loss:  0.0015671775909140706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.0013893485302105546 | Validation Loss:  0.0015670985449105501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.001389248063787818 | Validation Loss:  0.0015670140273869038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.0013891480630263686 | Validation Loss:  0.0015669339336454868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.001389048295095563 | Validation Loss:  0.0015668487176299095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0013889488764107227 | Validation Loss:  0.001566761638969183\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0013888494577258825 | Validation Loss:  0.0015666770050302148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.0013887503882870078 | Validation Loss:  0.001566586666740477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0013886517845094204 | Validation Loss:  0.0015664967941120267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0013885534135624766 | Validation Loss:  0.0015664069214835763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0013884550426155329 | Validation Loss:  0.0015663121594116092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0013883571373298764 | Validation Loss:  0.0015662190271541476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0013882593484595418 | Validation Loss:  0.001566123915836215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0013881619088351727 | Validation Loss:  0.001566025661304593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0013880647020414472 | Validation Loss:  0.001565928803756833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0013879676116630435 | Validation Loss:  0.0015658282209187746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0013878707541152835 | Validation Loss:  0.001565727056004107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0013877743622288108 | Validation Loss:  0.0015656257746741176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.001387677970342338 | Validation Loss:  0.0015655211172997952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.0013875816948711872 | Validation Loss:  0.0015654170420020819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0013874858850613236 | Validation Loss:  0.0015653114533051848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.0013873903080821037 | Validation Loss:  0.001565203769132495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.0013872948475182056 | Validation Loss:  0.0015650962013751268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0013871993869543076 | Validation Loss:  0.0015649863053113222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.001387104275636375 | Validation Loss:  0.0015648762928321958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.001387009397149086 | Validation Loss:  0.0015647652326151729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.0013869146350771189 | Validation Loss:  0.0015646525425836444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.0013868202222511172 | Validation Loss:  0.0015645399689674377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.0013867259258404374 | Validation Loss:  0.00156442541629076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0013866318622604012 | Validation Loss:  0.0015643102815374732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.001386537915095687 | Validation Loss:  0.0015641945647075772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.001386444317176938 | Validation Loss:  0.0015640778001397848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0013863507192581892 | Validation Loss:  0.0015639602206647396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0013862574705854058 | Validation Loss:  0.0015638417098671198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0013861643383279443 | Validation Loss:  0.001563722500577569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0013860714389011264 | Validation Loss:  0.0015636025927960873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0013859786558896303 | Validation Loss:  0.001563481753692031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.001385886105708778 | Validation Loss:  0.0015633604489266872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0013857937883585691 | Validation Loss:  0.0015632384456694126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0013857015874236822 | Validation Loss:  0.0015631149290129542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0013856093864887953 | Validation Loss:  0.0015629917616024613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.0013855176512151957 | Validation Loss:  0.0015628673136234283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0013854260323569179 | Validation Loss:  0.0015627422835677862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0013853346463292837 | Validation Loss:  0.0015626164386048913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0013852434931322932 | Validation Loss:  0.0015624895459041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.0013851523399353027 | Validation Loss:  0.001562363002449274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.001385061303153634 | Validation Loss:  0.001562234596349299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.0013849706156179309 | Validation Loss:  0.0015621061902493238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0013848802773281932 | Validation Loss:  0.001561976969242096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.0013847899390384555 | Validation Loss:  0.0015618468169122934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.0013846996007487178 | Validation Loss:  0.001561716548167169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.0013846096117049456 | Validation Loss:  0.0015615849988535047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.0013845197390764952 | Validation Loss:  0.0015614537987858057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.0013844302156940103 | Validation Loss:  0.0015613212017342448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.0013843406923115253 | Validation Loss:  0.0015611882554367185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.001384251518175006 | Validation Loss:  0.0015610546106472611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.0013841624604538083 | Validation Loss:  0.0015609199181199074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.0013840734027326107 | Validation Loss:  0.0015607852255925536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.0013839845778420568 | Validation Loss:  0.0015606494853273034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.0013838961021974683 | Validation Loss:  0.0015605136286467314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.001383807510137558 | Validation Loss:  0.001560376724228263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.001383719383738935 | Validation Loss:  0.001560240052640438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.0013836313737556338 | Validation Loss:  0.0015601019840687513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.0013835434801876545 | Validation Loss:  0.0015599641483277082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.001383455703034997 | Validation Loss:  0.0015598249156028032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.0013833681587129831 | Validation Loss:  0.0015596859157085419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.001383280730806291 | Validation Loss:  0.0015595461009070277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.0013831936521455646 | Validation Loss:  0.0015594055876135826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.0013831063406541944 | Validation Loss:  0.0015592651907354593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.001383019844070077 | Validation Loss:  0.001559123513288796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.0013829331146553159 | Validation Loss:  0.001558982185088098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.0013828465016558766 | Validation Loss:  0.0015588394599035382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.001382760121487081 | Validation Loss:  0.0015586973167955875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.001382674090564251 | Validation Loss:  0.0015585535438731313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.001382587943226099 | Validation Loss:  0.001558410469442606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.0013825021451339126 | Validation Loss:  0.0015582656487822533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.0013824165798723698 | Validation Loss:  0.001558121875859797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.0013823311310261488 | Validation Loss:  0.0015579764731228352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.0013822456821799278 | Validation Loss:  0.0015578316524624825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.0013821605825796723 | Validation Loss:  0.001557685318402946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.001382075366564095 | Validation Loss:  0.0015575394500046968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.0013819907326251268 | Validation Loss:  0.0015573924174532294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.001381905865855515 | Validation Loss:  0.0015572455013170838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.0013818215811625123 | Validation Loss:  0.0015570976538583636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.0013817371800541878 | Validation Loss:  0.001556950155645609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.0013816531281918287 | Validation Loss:  0.001556801376864314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.0013815690763294697 | Validation Loss:  0.0015566532965749502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.0013814852572977543 | Validation Loss:  0.0015565030043944716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.0013814015546813607 | Validation Loss:  0.0015563545748591423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.0013813180848956108 | Validation Loss:  0.0015562032349407673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.0013812347315251827 | Validation Loss:  0.0015560546889901161\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.0013811514945700765 | Validation Loss:  0.0015559016028419137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.0013810684904456139 | Validation Loss:  0.0015557537553831935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.0013809858355671167 | Validation Loss:  0.0015555975260213017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.001380902947857976 | Validation Loss:  0.0015554518904536963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.0013808205258101225 | Validation Loss:  0.001555291353724897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.001380738103762269 | Validation Loss:  0.0015551504911854863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.001380656030960381 | Validation Loss:  0.0015549807576462626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.0013805737253278494 | Validation Loss:  0.001554851420223713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.0013804921181872487 | Validation Loss:  0.0015546628274023533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.0013804102782160044 | Validation Loss:  0.0015545602655038238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.0013803287874907255 | Validation Loss:  0.0015543288318440318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.0013802475295960903 | Validation Loss:  0.0015542901819571853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.0013801666209474206 | Validation Loss:  0.001553959446027875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.0013800859451293945 | Validation Loss:  0.0015540763270109892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.0013800066662952304 | Validation Loss:  0.0015535032143816352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.0013799292501062155 | Validation Loss:  0.00155401520896703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.0013798577710986137 | Validation Loss:  0.0015528379008173943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.0013798002619296312 | Validation Loss:  0.0015544038033112884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.0013797791907563806 | Validation Loss:  0.001551719498820603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.0013798561412841082 | Validation Loss:  0.0015563246561214328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.0013801964232698083 | Validation Loss:  0.0015501584857702255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.0013812663964927197 | Validation Loss:  0.0015648672124370933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.0013843592023476958 | Validation Loss:  0.0015538458246737719\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.001393230282701552 | Validation Loss:  0.0016109300777316093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.001418211031705141 | Validation Loss:  0.0016252619680017233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.0014895213535055518 | Validation Loss:  0.001898307236842811\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.0016753155505284667 | Validation Loss:  0.0021955997217446566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.0021355757489800453 | Validation Loss:  0.003047846956178546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.0027786248829215765 | Validation Loss:  0.0032613861840218306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.003317947732284665 | Validation Loss:  0.0023047756403684616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.0020575402304530144 | Validation Loss:  0.0016074408777058125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.0014325205702334642 | Validation Loss:  0.0021822305861860514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.002160152420401573 | Validation Loss:  0.0018702977104112506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.0016731772338971496 | Validation Loss:  0.0017040292732417583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.0015266704140231013 | Validation Loss:  0.0018616083543747663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.0018304854165762663 | Validation Loss:  0.0015565092908218503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.001437689526937902 | Validation Loss:  0.001967820804566145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.0017671240493655205 | Validation Loss:  0.001558260410092771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.0014575902605429292 | Validation Loss:  0.0017153657972812653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.0016589091392233968 | Validation Loss:  0.001638359623029828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.0014839171199128032 | Validation Loss:  0.001748634735122323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.001574016991071403 | Validation Loss:  0.0015856415266171098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.001500329584814608 | Validation Loss:  0.001612160005606711\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.0015335147036239505 | Validation Loss:  0.0016449358081445098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.001482711173593998 | Validation Loss:  0.0016652909107506275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.0014977650716900826 | Validation Loss:  0.001583937439136207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.001479344442486763 | Validation Loss:  0.0015792001504451036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.0014704102650284767 | Validation Loss:  0.0016373718390241265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.001465291017666459 | Validation Loss:  0.0016214806819334626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.0014497796073555946 | Validation Loss:  0.0015784869901835918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.0014540445990860462 | Validation Loss:  0.001569324522279203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.0014340069610625505 | Validation Loss:  0.0016283139120787382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.0014490318717435002 | Validation Loss:  0.0015894630923867226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.0014153802767395973 | Validation Loss:  0.001585823018103838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.0014447409193962812 | Validation Loss:  0.0015653457958251238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.0014068924356251955 | Validation Loss:  0.0016266702441498637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.0014399801148101687 | Validation Loss:  0.0015726594720035791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.001399929285980761 | Validation Loss:  0.0015856829704716802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.001432941760867834 | Validation Loss:  0.0015712343156337738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.001396830310113728 | Validation Loss:  0.001615962595678866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.0014256199356168509 | Validation Loss:  0.0015684600220993161\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.0013977462658658624 | Validation Loss:  0.0015754197956994176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.001414108439348638 | Validation Loss:  0.0015813821228221059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.0014001406962051988 | Validation Loss:  0.0015869487542659044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.0014032964827492833 | Validation Loss:  0.00156701123341918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.0014021500246599317 | Validation Loss:  0.0015633156290277839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.001394977211020887 | Validation Loss:  0.0015862956643104553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.001402272959239781 | Validation Loss:  0.0015634724404662848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.001388954697176814 | Validation Loss:  0.001560341683216393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.0013997050700709224 | Validation Loss:  0.001559216296300292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.001387063879519701 | Validation Loss:  0.0015742717077955604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.0013949288986623287 | Validation Loss:  0.001552661880850792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.0013877326855435967 | Validation Loss:  0.0015512086683884263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.0013896272284910083 | Validation Loss:  0.0015629124827682972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.0013885395601391792 | Validation Loss:  0.0015561985783278942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.0013853067066520452 | Validation Loss:  0.0015475895488634706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.0013887523673474789 | Validation Loss:  0.0015486530028283596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.0013829421950504184 | Validation Loss:  0.0015608302783221006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.0013874114956706762 | Validation Loss:  0.001546212355606258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.0013827666407451034 | Validation Loss:  0.0015445258468389511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.001384733128361404 | Validation Loss:  0.0015534284757450223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.0013835219433531165 | Validation Loss:  0.0015508022624999285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.0013822823530063033 | Validation Loss:  0.0015437293332070112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.001383845810778439 | Validation Loss:  0.0015464707976207137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.0013810080708935857 | Validation Loss:  0.0015542424516752362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.0013831253163516521 | Validation Loss:  0.0015447476180270314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.0013810115633532405 | Validation Loss:  0.0015440856805071235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.0013815091224387288 | Validation Loss:  0.0015517809661105275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.0013815263519063592 | Validation Loss:  0.0015480546280741692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.0013801435707136989 | Validation Loss:  0.0015441845171153545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.001381342182867229 | Validation Loss:  0.0015482943272218108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.001379829947836697 | Validation Loss:  0.0015509153017774224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.0013803568435832858 | Validation Loss:  0.001544923521578312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.001380077563226223 | Validation Loss:  0.0015460795257240534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.0013793769758194685 | Validation Loss:  0.0015512756071984768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.0013800157466903329 | Validation Loss:  0.0015464558964595199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.0013790637021884322 | Validation Loss:  0.0015454660169780254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.0013793711550533772 | Validation Loss:  0.0015500100562348962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.0013792200479656458 | Validation Loss:  0.001547999563626945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.0013787137577310205 | Validation Loss:  0.0015453157247975469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.0013791293604299426 | Validation Loss:  0.0015482689486816525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.001378574874252081 | Validation Loss:  0.001548989675939083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.0013786270283162594 | Validation Loss:  0.0015455234097316861\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.0013786585768684745 | Validation Loss:  0.0015468827914446592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.0013782365713268518 | Validation Loss:  0.0015489979414269328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.0013784506591036916 | Validation Loss:  0.0015457008266821504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.0013781854650005698 | Validation Loss:  0.0015457893023267388\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.0013780458830296993 | Validation Loss:  0.0015483187744393945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.0013781313318759203 | Validation Loss:  0.0015459220157936215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.0013778213178738952 | Validation Loss:  0.0015450256178155541\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.0013778505381196737 | Validation Loss:  0.0015472411178052425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.0013777652056887746 | Validation Loss:  0.0015459046699106693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.0013775562401860952 | Validation Loss:  0.0015443924348801374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.001377607462927699 | Validation Loss:  0.0015461025759577751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.0013774348190054297 | Validation Loss:  0.0015457450645044446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.001377339125610888 | Validation Loss:  0.0015439426060765982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.0013773381942883134 | Validation Loss:  0.0015450466889888048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.0013771620579063892 | Validation Loss:  0.0015453413361683488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.001377130625769496 | Validation Loss:  0.0015435490058735013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.0013770709047093987 | Validation Loss:  0.001544149243272841\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.0013769313227385283 | Validation Loss:  0.0015448527410626411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.0013769144425168633 | Validation Loss:  0.0015432604122906923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.0013768187491223216 | Validation Loss:  0.0015434224624186754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.0013767192140221596 | Validation Loss:  0.0015442768344655633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.0013766901101917028 | Validation Loss:  0.0015429842751473188\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.0013765834737569094 | Validation Loss:  0.001542849582619965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.0013765112962573767 | Validation Loss:  0.0015437392285093665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.001376464031636715 | Validation Loss:  0.001542765530757606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.0013763618189841509 | Validation Loss:  0.0015424175653606653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.0013763033784925938 | Validation Loss:  0.0015432321233674884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.0013762414455413818 | Validation Loss:  0.0015425411984324455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.0013761495938524604 | Validation Loss:  0.001542085548862815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.0013760954607278109 | Validation Loss:  0.0015428062761202455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.0013760252622887492 | Validation Loss:  0.0015423449221998453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.0013759434223175049 | Validation Loss:  0.001541829900816083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.0013758887071162462 | Validation Loss:  0.0015424233861267567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.0013758152490481734 | Validation Loss:  0.001542123151011765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.0013757406268268824 | Validation Loss:  0.0015416021924465895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.0013756834669038653 | Validation Loss:  0.0015420877607539296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.0013756100088357925 | Validation Loss:  0.0015418959083035588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.0013755400432273746 | Validation Loss:  0.0015413822839036584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.0013754804385825992 | Validation Loss:  0.0015417553950101137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.0013754082610830665 | Validation Loss:  0.001541621284559369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.0013753410894423723 | Validation Loss:  0.0015411358326673508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.0013752796221524477 | Validation Loss:  0.0015414234949275851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.001375208841636777 | Validation Loss:  0.001541318604722619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.0013751432998105884 | Validation Loss:  0.0015408588806167245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.0013750809011980891 | Validation Loss:  0.0015410669147968292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.0013750118669122458 | Validation Loss:  0.001540966099128127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.0013749467907473445 | Validation Loss:  0.0015405419981107116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.001374883926473558 | Validation Loss:  0.001540695782750845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.0013748160563409328 | Validation Loss:  0.0015405904268845916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.0013747515622526407 | Validation Loss:  0.0015401982236653566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.0013746883487328887 | Validation Loss:  0.0015403053257614374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.0013746216427534819 | Validation Loss:  0.0015401876298710704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.0013745572650805116 | Validation Loss:  0.0015398339601233602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.0013744940515607595 | Validation Loss:  0.0015399137046188116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.001374428509734571 | Validation Loss:  0.0015397843671962619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.0013743643648922443 | Validation Loss:  0.0015394663205370307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.001374301384203136 | Validation Loss:  0.0015395228983834386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.0013742364244535565 | Validation Loss:  0.0015393799403682351\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.0013741726288571954 | Validation Loss:  0.001539101474918425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.0013741097645834088 | Validation Loss:  0.0015391451306641102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.0013740458525717258 | Validation Loss:  0.0015389914624392986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.0013739821733906865 | Validation Loss:  0.0015387485036626458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.0013739195419475436 | Validation Loss:  0.0015387769090011716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.001373856095597148 | Validation Loss:  0.0015386114828288555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.001373792882077396 | Validation Loss:  0.0015384048456326127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.0013737303670495749 | Validation Loss:  0.001538420794531703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.0013736673863604665 | Validation Loss:  0.001538245240226388\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.0013736042892560363 | Validation Loss:  0.0015380688710138202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.0013735421234741807 | Validation Loss:  0.0015380665427073836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.0013734794920310378 | Validation Loss:  0.0015378812095150352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.0013734169770032167 | Validation Loss:  0.0015377338277176023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.001373354927636683 | Validation Loss:  0.0015377119416370988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.0013732928782701492 | Validation Loss:  0.0015375199727714062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.0013732305960729718 | Validation Loss:  0.0015373948263004422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.0013731686631217599 | Validation Loss:  0.0015373480273410678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.0013731070794165134 | Validation Loss:  0.0015371529152616858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.0013730451464653015 | Validation Loss:  0.0015370470937341452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.001372983562760055 | Validation Loss:  0.0015369750326499343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.0013729220954701304 | Validation Loss:  0.0015367818996310234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.0013728606281802058 | Validation Loss:  0.00153668865095824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.001372799277305603 | Validation Loss:  0.0015365896979346871\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.0013727382756769657 | Validation Loss:  0.0015364036662504077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.001372676924802363 | Validation Loss:  0.0015363191487267613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.0013726160395890474 | Validation Loss:  0.001536196214146912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.0013725550379604101 | Validation Loss:  0.0015360228717327118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.0013724943855777383 | Validation Loss:  0.0015359390527009964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.001372433383949101 | Validation Loss:  0.0015357957454398274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.0013723729643970728 | Validation Loss:  0.0015356389340013266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.0013723125448450446 | Validation Loss:  0.0015355510404333472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.0013722520088776946 | Validation Loss:  0.0015353939961642027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.00137219182215631 | Validation Loss:  0.001535254530608654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.00137213128618896 | Validation Loss:  0.0015351561596617103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.001372071448713541 | Validation Loss:  0.001534992945380509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.0013720112619921565 | Validation Loss:  0.0015348695451393723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.0013719514245167375 | Validation Loss:  0.0015347572043538094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.0013718915870413184 | Validation Loss:  0.0015345956198871136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.0013718318659812212 | Validation Loss:  0.0015344832791015506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.0013717722613364458 | Validation Loss:  0.0015343553386628628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.0013717126566916704 | Validation Loss:  0.0015342015540227294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.0013716532848775387 | Validation Loss:  0.001534093520604074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.0013715937966480851 | Validation Loss:  0.001533952890895307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.0013715344248339534 | Validation Loss:  0.001533810980618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.0013714752858504653 | Validation Loss:  0.0015336998039856553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.001371416263282299 | Validation Loss:  0.0015335509087890387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.0013713573571294546 | Validation Loss:  0.0015334209892898798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.0013712982181459665 | Validation Loss:  0.0015333013143390417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.0013712396612390876 | Validation Loss:  0.0015331513714045286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.0013711809879168868 | Validation Loss:  0.001533030066639185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.001371122314594686 | Validation Loss:  0.001532898866571486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.001371063757687807 | Validation Loss:  0.001532753580249846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.00137100531719625 | Validation Loss:  0.0015326347202062607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.0013709471095353365 | Validation Loss:  0.001532494556158781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.0013708887854591012 | Validation Loss:  0.001532357418909669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.0013708306942135096 | Validation Loss:  0.001532234949991107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.001370772602967918 | Validation Loss:  0.0015320897800847888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.0013707143953070045 | Validation Loss:  0.0015319607919082046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.0013706567697227001 | Validation Loss:  0.001531830057501793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.001370599027723074 | Validation Loss:  0.0015316866338253021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.001370541169308126 | Validation Loss:  0.00153156160376966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.0013704837765544653 | Validation Loss:  0.0015314226038753986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.0013704262673854828 | Validation Loss:  0.001531285117380321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.0013703688746318221 | Validation Loss:  0.0015311577590182424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.0013703114818781614 | Validation Loss:  0.0015310150338336825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.0013702542055398226 | Validation Loss:  0.0015308832516893744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.0013701970456168056 | Validation Loss:  0.001530750305391848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.0013701400021091104 | Validation Loss:  0.0015306087443605065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.001370083075016737 | Validation Loss:  0.0015304796397686005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.0013700262643396854 | Validation Loss:  0.0015303405234590173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.0013699695700779557 | Validation Loss:  0.0015302037354558706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.0013699127594009042 | Validation Loss:  0.0015300726518034935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.00136985641438514 | Validation Loss:  0.0015299313236027956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.0013697997201234102 | Validation Loss:  0.0015297988429665565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.0013697431422770023 | Validation Loss:  0.0015296625206246972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.00136968691367656 | Validation Loss:  0.0015295231714844704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.0013696306850761175 | Validation Loss:  0.001529391622170806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.0013695746893063188 | Validation Loss:  0.00152925169095397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.0013695184607058764 | Validation Loss:  0.001529116416350007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.0013694626977667212 | Validation Loss:  0.0015289818402379751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.0013694069348275661 | Validation Loss:  0.0015288415597751737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.0013693509390577674 | Validation Loss:  0.0015287089627236128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.0013692955253645778 | Validation Loss:  0.0015285704284906387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.0013692398788407445 | Validation Loss:  0.0015284335240721703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.001369184348732233 | Validation Loss:  0.0015282992972061038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.0013691289350390434 | Validation Loss:  0.0015281593659892678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.0013690736377611756 | Validation Loss:  0.001528025371953845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.0013690184568986297 | Validation Loss:  0.0015278876526281238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.0013689633924514055 | Validation Loss:  0.0015277497004717588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.0013689082115888596 | Validation Loss:  0.001527615124359727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.001368853379972279 | Validation Loss:  0.001527475891634822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.0013687986647710204 | Validation Loss:  0.0015273407334461808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.0013687436003237963 | Validation Loss:  0.0015272031305357814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.001368689234368503 | Validation Loss:  0.001527065527625382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.0013686346355825663 | Validation Loss:  0.0015269300201907754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.0013685801532119513 | Validation Loss:  0.001526791020296514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.001368525787256658 | Validation Loss:  0.0015266549307852983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.001368471421301365 | Validation Loss:  0.001526517327874899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.0013684172881767154 | Validation Loss:  0.0015263792593032122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.0013683632714673877 | Validation Loss:  0.0015262437518686056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.0013683091383427382 | Validation Loss:  0.0015261046355590224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.001368255354464054 | Validation Loss:  0.001525968429632485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.0013682014541700482 | Validation Loss:  0.0015258305938914418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.001368147786706686 | Validation Loss:  0.0015256924089044333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.0013680941192433238 | Validation Loss:  0.0015255563193932176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.001368040801025927 | Validation Loss:  0.0015254172030836344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.0013679872499778867 | Validation Loss:  0.0015252807643264532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.00136793393176049 | Validation Loss:  0.0015251428121700883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.0013678808463737369 | Validation Loss:  0.001525005092844367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.0013678275281563401 | Validation Loss:  0.0015248681884258986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.0013677745591849089 | Validation Loss:  0.0015247297706082463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.0013677215902134776 | Validation Loss:  0.0015245932154357433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.00136766885407269 | Validation Loss:  0.0015244551468640566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.0013676158851012588 | Validation Loss:  0.0015243175439536572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.0013675633817911148 | Validation Loss:  0.0015241806395351887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.0013675108784809709 | Validation Loss:  0.0015240421053022146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.001367458375170827 | Validation Loss:  0.0015239057829603553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.0013674059882760048 | Validation Loss:  0.001523767365142703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.0013673538342118263 | Validation Loss:  0.0015236308099702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.001367301563732326 | Validation Loss:  0.0015234928578138351\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.0013672496424987912 | Validation Loss:  0.0015233553713187575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.0013671976048499346 | Validation Loss:  0.001523218466900289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.0013671456836163998 | Validation Loss:  0.0015230803983286023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.0013670939952135086 | Validation Loss:  0.0015229437267407775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.0013670423068106174 | Validation Loss:  0.0015228058909997344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.0013669907348230481 | Validation Loss:  0.0015226693358272314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.0013669393956661224 | Validation Loss:  0.0015225313836708665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.0013668880565091968 | Validation Loss:  0.0015223950613290071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.001366836717352271 | Validation Loss:  0.001522257225587964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.001366785611025989 | Validation Loss:  0.0015221205540001392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.001366734504699707 | Validation Loss:  0.001521983533166349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.0013666835147887468 | Validation Loss:  0.0015218459302559495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.0013666327577084303 | Validation Loss:  0.001521709724329412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.0013665820006281137 | Validation Loss:  0.001521571772173047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.0013665312435477972 | Validation Loss:  0.0015214360319077969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.0013664807192981243 | Validation Loss:  0.0015212979633361101\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.0013664301950484514 | Validation Loss:  0.00152116222307086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.001366380020044744 | Validation Loss:  0.0015210246201604605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.0013663297286257148 | Validation Loss:  0.0015208887634798884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.0013662796700373292 | Validation Loss:  0.00152075185906142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.0013662294950336218 | Validation Loss:  0.001520615303888917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.001366179552860558 | Validation Loss:  0.0015204793307930231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.001366129843518138 | Validation Loss:  0.001520342193543911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.0013660802505910397 | Validation Loss:  0.0015202066861093044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.0013660304248332977 | Validation Loss:  0.001520069781690836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.0013659810647368431 | Validation Loss:  0.0015199347399175167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.0013659315882250667 | Validation Loss:  0.0015197974862530828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.001365882228128612 | Validation Loss:  0.001519662793725729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.001365833100862801 | Validation Loss:  0.0015195256564766169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.0013657839735969901 | Validation Loss:  0.001519391080364585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.001365734962746501 | Validation Loss:  0.0015192540595307946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.0013656861847266555 | Validation Loss:  0.0015191200654953718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.00136563740670681 | Validation Loss:  0.0015189831610769033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.0013655887451022863 | Validation Loss:  0.001518848817795515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.0013655400834977627 | Validation Loss:  0.0015187126118689775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.0013654916547238827 | Validation Loss:  0.0015185782685875893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.0013654434587806463 | Validation Loss:  0.0015184427611529827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.0013653951464220881 | Validation Loss:  0.001518308068625629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.00136534683406353 | Validation Loss:  0.0015181729104369879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.001365298987366259 | Validation Loss:  0.0015180387999862432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.0013652511406689882 | Validation Loss:  0.0015179038746282458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.0013652031775563955 | Validation Loss:  0.0015177695313468575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.0013651555636897683 | Validation Loss:  0.001517634722404182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.001365108066238463 | Validation Loss:  0.0015175013104453683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.0013650606852024794 | Validation Loss:  0.0015173666179180145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.0013650130713358521 | Validation Loss:  0.0015172333223745227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.0013649659231305122 | Validation Loss:  0.001517098513431847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.0013649188913404942 | Validation Loss:  0.001516966032795608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.001364871859550476 | Validation Loss:  0.001516830874606967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.001364824827760458 | Validation Loss:  0.0015166994417086244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.0013647779123857617 | Validation Loss:  0.0015165637014433742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.001364731346257031 | Validation Loss:  0.0015164335491135716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.0013646847801283002 | Validation Loss:  0.001516296761110425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.001364638446830213 | Validation Loss:  0.0015161690535023808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.0013645918807014823 | Validation Loss:  0.001516029704362154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.0013645455474033952 | Validation Loss:  0.0015159060712903738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.0013644996797665954 | Validation Loss:  0.0015157615998759866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.00136445346288383 | Validation Loss:  0.0015156461158767343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.001364407711662352 | Validation Loss:  0.0015154904685914516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.0013643618440255523 | Validation Loss:  0.001515391399152577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.001364316325634718 | Validation Loss:  0.0015152139822021127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.0013642706908285618 | Validation Loss:  0.0015151472762227058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.0013642252888530493 | Validation Loss:  0.0015149235259741545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.0013641800032928586 | Validation Loss:  0.0015149260871112347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.001364135299809277 | Validation Loss:  0.001514601637609303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.0013640907127410173 | Validation Loss:  0.001514756353572011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.001364047289825976 | Validation Loss:  0.001514210132881999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.0013640058459714055 | Validation Loss:  0.0015147096710279584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.0013639688258990645 | Validation Loss:  0.0015136649599298835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.0013639418175444007 | Validation Loss:  0.001514978939667344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.001363938208669424 | Validation Loss:  0.0015128091908991337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.0013639898970723152 | Validation Loss:  0.0015161623014137149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.0013641732512041926 | Validation Loss:  0.0015115448040887713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.0013646804727613926 | Validation Loss:  0.0015204966766759753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.0013659711694344878 | Validation Loss:  0.001511453534476459\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.001369261764921248 | Validation Loss:  0.0015383430290967226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.0013773972168564796 | Validation Loss:  0.0015286094276234508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.0013985184486955404 | Validation Loss:  0.0016259719850495458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.00144959578756243 | Validation Loss:  0.0016815264243632555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.0015855696983635426 | Validation Loss:  0.0020795294549316168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.00187081063631922 | Validation Loss:  0.0025620099622756243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.0025657047517597675 | Validation Loss:  0.003360874718055129\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.0031045014038681984 | Validation Loss:  0.003336547175422311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.0034481885377317667 | Validation Loss:  0.001981383189558983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.0017823519883677363 | Validation Loss:  0.001827088650316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.0016511710127815604 | Validation Loss:  0.0027409617323428392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.0027754493057727814 | Validation Loss:  0.002010900527238846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.0018228035187348723 | Validation Loss:  0.0018923756433650851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.0017298077000305057 | Validation Loss:  0.002099188044667244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.0021279240027070045 | Validation Loss:  0.0015376339433714747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.0014346456155180931 | Validation Loss:  0.0022223081905394793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.002031581709161401 | Validation Loss:  0.0016596912173554301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.0016261631390079856 | Validation Loss:  0.0017590674106031656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.0017430993029847741 | Validation Loss:  0.0018409512704238296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.00166953238658607 | Validation Loss:  0.001664799521677196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.001515114912763238 | Validation Loss:  0.0017648974899202585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.0017200205475091934 | Validation Loss:  0.001545289414934814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.0014415839686989784 | Validation Loss:  0.001893389504402876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.0017040325328707695 | Validation Loss:  0.001533638103865087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.0013984708348289132 | Validation Loss:  0.001707910792902112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.0016233486821874976 | Validation Loss:  0.001561391749419272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.001410380587913096 | Validation Loss:  0.0017297975718975067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.0015460416907444596 | Validation Loss:  0.001602509175427258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.0014719654573127627 | Validation Loss:  0.0015933680115267634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.001465426292270422 | Validation Loss:  0.001673608785495162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.0015128239756450057 | Validation Loss:  0.0015606576343998313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.0013983643148094416 | Validation Loss:  0.001632938627153635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.0015025262255221605 | Validation Loss:  0.0015557726146653295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.0013937685871496797 | Validation Loss:  0.0016401666216552258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.0014737648889422417 | Validation Loss:  0.0015564447967335582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.0014192118542268872 | Validation Loss:  0.001558171585202217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.001422086264938116 | Validation Loss:  0.0016066551906988025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.0014342691283673048 | Validation Loss:  0.0015496403211727738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.0013906249077990651 | Validation Loss:  0.0015562865883111954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.0014319340698421001 | Validation Loss:  0.0015267007984220982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.0013834950514137745 | Validation Loss:  0.001583445118740201\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.0014210970839485526 | Validation Loss:  0.001526167499832809\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.0013889753026887774 | Validation Loss:  0.0015290891751646996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.0014021765673533082 | Validation Loss:  0.0015457753324881196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.001394195482134819 | Validation Loss:  0.001533375820145011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.0013863962376490235 | Validation Loss:  0.001522210892289877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.001397793530486524 | Validation Loss:  0.0015177528839558363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.0013782099122181535 | Validation Loss:  0.0015523683978244662\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.0013963794335722923 | Validation Loss:  0.0015124041819944978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.0013766672927886248 | Validation Loss:  0.00151602562982589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.0013875263975933194 | Validation Loss:  0.0015314806951209903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.001380697125568986 | Validation Loss:  0.001527157612144947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.0013771062949672341 | Validation Loss:  0.0015166091034188867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.0013840683968737721 | Validation Loss:  0.0015155586879700422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.0013719340786337852 | Validation Loss:  0.0015360694378614426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.0013823594199493527 | Validation Loss:  0.0015142967458814383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.0013737557455897331 | Validation Loss:  0.001516016316600144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.0013754036044701934 | Validation Loss:  0.0015333470655605197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.0013774376129731536 | Validation Loss:  0.0015194965526461601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.001370613812468946 | Validation Loss:  0.0015165816294029355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.001376985921524465 | Validation Loss:  0.00152303883805871\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.001371378661133349 | Validation Loss:  0.0015269789146259427\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.0013727110344916582 | Validation Loss:  0.0015170663828030229\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.001373983221128583 | Validation Loss:  0.001517958240583539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.0013697834219783545 | Validation Loss:  0.0015280005754902959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.0013736644759774208 | Validation Loss:  0.0015160511247813702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.0013706276658922434 | Validation Loss:  0.0015161401825025678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.0013706551399081945 | Validation Loss:  0.0015262361848726869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.0013721163850277662 | Validation Loss:  0.0015174396103248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.0013691738713532686 | Validation Loss:  0.0015145540237426758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.0013711994979530573 | Validation Loss:  0.0015208078548312187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.0013700226554647088 | Validation Loss:  0.0015191154088824987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.0013692558277398348 | Validation Loss:  0.001514163101091981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.0013705625897273421 | Validation Loss:  0.0015171929262578487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.0013687560567632318 | Validation Loss:  0.0015196743188425899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.001369567820802331 | Validation Loss:  0.001513295341283083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.0013693933142349124 | Validation Loss:  0.0015146912774071097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.0013684338191524148 | Validation Loss:  0.0015197920147329569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.0013693679356947541 | Validation Loss:  0.0015138965100049973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.0013684184523299336 | Validation Loss:  0.00151330407243222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.0013684763107448816 | Validation Loss:  0.0015180843183770776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.0013687409227713943 | Validation Loss:  0.001514691044576466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.0013679256662726402 | Validation Loss:  0.0015131249092519283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.0013683853903785348 | Validation Loss:  0.001516826217994094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.0013680438278242946 | Validation Loss:  0.001515528536401689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.0013677437091246247 | Validation Loss:  0.0015128070954233408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.0013680460397154093 | Validation Loss:  0.001515399431809783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.0013675320660695434 | Validation Loss:  0.0015163128264248371\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.001367604942061007 | Validation Loss:  0.0015131874242797494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.0013675806112587452 | Validation Loss:  0.00151436950545758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.0013672177447006106 | Validation Loss:  0.001516131218522787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.0013673788635060191 | Validation Loss:  0.0015134107088670135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.0013671505730599165 | Validation Loss:  0.0015138703165575862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.0013670048210769892 | Validation Loss:  0.0015159682370722294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.001367073622532189 | Validation Loss:  0.0015136994188651443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.0013668161118403077 | Validation Loss:  0.0015132497064769268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.0013668002793565392 | Validation Loss:  0.001515302574262023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.0013667508028447628 | Validation Loss:  0.0015140363248065114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.0013665560400113463 | Validation Loss:  0.0015130569227039814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.0013665680307894945 | Validation Loss:  0.0015145951183512807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.0013664484722539783 | Validation Loss:  0.001513952505774796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.0013663264689967036 | Validation Loss:  0.001512745046056807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.0013663150602951646 | Validation Loss:  0.0015139965107664466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.0013661769917234778 | Validation Loss:  0.0015139366732910275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.0013661031844094396 | Validation Loss:  0.0015125094214454293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.0013660597614943981 | Validation Loss:  0.0015132346889004111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.001365930074825883 | Validation Loss:  0.001513619557954371\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.0013658818788826466 | Validation Loss:  0.0015123753109946847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.0013658112147822976 | Validation Loss:  0.0015127251390367746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.0013657019007951021 | Validation Loss:  0.0015132254920899868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.0013656610390171409 | Validation Loss:  0.0015120903262868524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.0013655726797878742 | Validation Loss:  0.001512186136096716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.0013654861832037568 | Validation Loss:  0.0015128555241972208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.0013654395006597042 | Validation Loss:  0.0015119403833523393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.001365344855003059 | Validation Loss:  0.001511737471446395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.0013652767520397902 | Validation Loss:  0.0015123519115149975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.0013652180787175894 | Validation Loss:  0.0015117164002731442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.001365128206089139 | Validation Loss:  0.0015114318812265992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.0013650688342750072 | Validation Loss:  0.0015119781019166112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.0013650007313117385 | Validation Loss:  0.0015114904381334782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.0013649201719090343 | Validation Loss:  0.0015110940439626575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.0013648617314174771 | Validation Loss:  0.0015115930000320077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.0013647902524098754 | Validation Loss:  0.0015113211702555418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.001364717143587768 | Validation Loss:  0.0015108744846656919\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.0013646569568663836 | Validation Loss:  0.001511228154413402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.0013645858271047473 | Validation Loss:  0.0015110629610717297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.001364517374895513 | Validation Loss:  0.0015106460778042674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.0013644564896821976 | Validation Loss:  0.0015109358355402946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.0013643864076584578 | Validation Loss:  0.0015108350198715925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.0013643207494169474 | Validation Loss:  0.0015103922924026847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.0013642595149576664 | Validation Loss:  0.0015105869388207793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.0013641907135024667 | Validation Loss:  0.0015105641214177012\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.0013641277328133583 | Validation Loss:  0.0015101630706340075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.00136406603269279 | Validation Loss:  0.0015102638863027096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.0013639990938827395 | Validation Loss:  0.0015102365287020802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.0013639379758387804 | Validation Loss:  0.0015098636504262686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.0013638759264722466 | Validation Loss:  0.001509929308667779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.0013638113159686327 | Validation Loss:  0.0015099217416718602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.0013637508964166045 | Validation Loss:  0.0015095609705895185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.001363689312711358 | Validation Loss:  0.0015095628332346678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.0013636266812682152 | Validation Loss:  0.0015095624839887023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.0013635670766234398 | Validation Loss:  0.0015092536341398954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.0013635063078254461 | Validation Loss:  0.0015092293033376336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.0013634450733661652 | Validation Loss:  0.0015092104440554976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.0013633861672133207 | Validation Loss:  0.0015089212683960795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.0013633263297379017 | Validation Loss:  0.001508887275122106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.0013632664922624826 | Validation Loss:  0.0015088783111423254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.001363208401016891 | Validation Loss:  0.0015086177736520767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.0013631496112793684 | Validation Loss:  0.0015085592167451978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.0013630908215418458 | Validation Loss:  0.0015085386112332344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.0013630333123728633 | Validation Loss:  0.001508312881924212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.001362975686788559 | Validation Loss:  0.0015082579338923097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.001362917828373611 | Validation Loss:  0.0015082242898643017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.0013628611341118813 | Validation Loss:  0.0015080084558576345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.0013628044398501515 | Validation Loss:  0.0015079479198902845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.0013627477455884218 | Validation Loss:  0.001507912646047771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.001362691866233945 | Validation Loss:  0.0015077171847224236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.0013626358704641461 | Validation Loss:  0.0015076467534527183\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.0013625802239403129 | Validation Loss:  0.0015075922710821033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.0013625251594930887 | Validation Loss:  0.001507410197518766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.0013624700950458646 | Validation Loss:  0.0015073437243700027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.001362415263429284 | Validation Loss:  0.0015072778332978487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.0013623612467199564 | Validation Loss:  0.001507099368609488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.0013623068807646632 | Validation Loss:  0.001507026026956737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.0013622529804706573 | Validation Loss:  0.0015069517539814115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.0013621996622532606 | Validation Loss:  0.0015067866770550609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.0013621463440358639 | Validation Loss:  0.0015067106578499079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.0013620932586491108 | Validation Loss:  0.0015066212508827448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.0013620405225083232 | Validation Loss:  0.001506463042460382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.001361988135613501 | Validation Loss:  0.001506390399299562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.001361935748718679 | Validation Loss:  0.0015062956372275949\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.0013618838274851441 | Validation Loss:  0.0015061442973092198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.001361832139082253 | Validation Loss:  0.0015060678124427795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.0013617806835100055 | Validation Loss:  0.0015059674624353647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.0013617294607684016 | Validation Loss:  0.0015058289282023907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.001361678703688085 | Validation Loss:  0.0015057536074891686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.0013616279466077685 | Validation Loss:  0.0015056452248245478\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.0013615776551887393 | Validation Loss:  0.001505513908341527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.0013615274801850319 | Validation Loss:  0.001505439868196845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.00136147765442729 | Validation Loss:  0.0015053297393023968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.0013614279450848699 | Validation Loss:  0.0015052062226459384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.0013613784685730934 | Validation Loss:  0.0015051278751343489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.0013613292248919606 | Validation Loss:  0.0015050144866108894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.0013612804468721151 | Validation Loss:  0.0015049005160108209\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.0013612316688522696 | Validation Loss:  0.001504820422269404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.0013611833564937115 | Validation Loss:  0.001504703308455646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.0013611351605504751 | Validation Loss:  0.001504594343714416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.0013610869646072388 | Validation Loss:  0.0015045111067593098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.0013610392343252897 | Validation Loss:  0.0015043942257761955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.0013609917368739843 | Validation Loss:  0.0015042911982163787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.0013609444722533226 | Validation Loss:  0.0015042020240798593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.0013608974404633045 | Validation Loss:  0.0015040846774354577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.00136085064150393 | Validation Loss:  0.001503987004980445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.0013608040753751993 | Validation Loss:  0.0015038944547995925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.0013607576256617904 | Validation Loss:  0.0015037781558930874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.0013607116416096687 | Validation Loss:  0.0015036831609904766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.0013606655411422253 | Validation Loss:  0.0015035863034427166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.001360620022751391 | Validation Loss:  0.0015034740790724754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.0013605743879452348 | Validation Loss:  0.0015033812960609794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.001360529218800366 | Validation Loss:  0.0015032803639769554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.001360484166070819 | Validation Loss:  0.0015031713992357254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.0013604392297565937 | Validation Loss:  0.0015030803624540567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.0013603945262730122 | Validation Loss:  0.0015029776841402054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.001360350288450718 | Validation Loss:  0.0015028726775199175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.0013603060506284237 | Validation Loss:  0.001502780825830996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.001360262162052095 | Validation Loss:  0.0015026770997792482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.001360218389891088 | Validation Loss:  0.0015025766333565116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.001360174734145403 | Validation Loss:  0.001502484199590981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.0013601315440610051 | Validation Loss:  0.0015023794258013368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.0013600882375612855 | Validation Loss:  0.001502282335422933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.0013600453967228532 | Validation Loss:  0.001502188853919506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.0013600025558844209 | Validation Loss:  0.001502085360698402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.001359960064291954 | Validation Loss:  0.0015019910642877221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.0013599178055301309 | Validation Loss:  0.0015018950216472149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.0013598755467683077 | Validation Loss:  0.0015017936239019036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.00135983363725245 | Validation Loss:  0.0015017014229670167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.001359791960567236 | Validation Loss:  0.0015016039833426476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.0013597504002973437 | Validation Loss:  0.0015015050303190947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.001359709189273417 | Validation Loss:  0.001501413295045495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.0013596678618341684 | Validation Loss:  0.0015013152733445168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.0013596270000562072 | Validation Loss:  0.0015012194635346532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.001359586021862924 | Validation Loss:  0.0015011271461844444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.0013595456257462502 | Validation Loss:  0.001501029240898788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.0013595052296295762 | Validation Loss:  0.0015009358758106828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.001359465066343546 | Validation Loss:  0.0015008432092145085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.0013594251358881593 | Validation Loss:  0.0015007463516667485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.0013593850890174508 | Validation Loss:  0.0015006548492237926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.0013593456242233515 | Validation Loss:  0.0015005611348897219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.0013593061594292521 | Validation Loss:  0.0015004663728177547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.0013592670438811183 | Validation Loss:  0.0015003756852820516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.0013592278119176626 | Validation Loss:  0.001500281854532659\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.0013591889292001724 | Validation Loss:  0.0015001889551058412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.0013591502793133259 | Validation Loss:  0.001500098966062069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.0013591117458418012 | Validation Loss:  0.001500005484558642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.00135907344520092 | Validation Loss:  0.001499914680607617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.0013590352609753609 | Validation Loss:  0.001499824458733201\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.0013589971931651235 | Validation Loss:  0.0014997320249676704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.0013589595910161734 | Validation Loss:  0.0014996432000771165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.0013589216396212578 | Validation Loss:  0.0014995525125414133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.0013588843867182732 | Validation Loss:  0.0014994618250057101\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.0013588469009846449 | Validation Loss:  0.0014993736986070871\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.001358809880912304 | Validation Loss:  0.001499283709563315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.001358772860839963 | Validation Loss:  0.001499194884672761\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.0013587363064289093 | Validation Loss:  0.0014991073403507471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.0013586996356025338 | Validation Loss:  0.0014990178169682622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.001358663197606802 | Validation Loss:  0.0014989308547228575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.0013586269924417138 | Validation Loss:  0.0014988434268161654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.0013585907872766256 | Validation Loss:  0.0014987551840022206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.001358554931357503 | Validation Loss:  0.0014986696187406778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.001358519191853702 | Validation Loss:  0.0014985821908339858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.0013584836851805449 | Validation Loss:  0.001498495927080512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.0013584482949227095 | Validation Loss:  0.0014984109438955784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.001358412904664874 | Validation Loss:  0.0014983243308961391\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.0013583777472376823 | Validation Loss:  0.001498239696957171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.001358342939056456 | Validation Loss:  0.0014981547137722373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.0013583082472905517 | Validation Loss:  0.0014980696141719818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.0013582735555246472 | Validation Loss:  0.001497986144386232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.0013582390965893865 | Validation Loss:  0.0014979016268625855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.0013582047540694475 | Validation Loss:  0.0014978183899074793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.0013581705279648304 | Validation Loss:  0.001497735152952373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.0013581366511061788 | Validation Loss:  0.0014976516831666231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.001358102890662849 | Validation Loss:  0.0014975696103647351\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.0013580691302195191 | Validation Loss:  0.0014974873047322035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.0013580357190221548 | Validation Loss:  0.0014974052319303155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.0013580021914094687 | Validation Loss:  0.001497324206866324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.0013579692458733916 | Validation Loss:  0.0014972425997257233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.001357936067506671 | Validation Loss:  0.001497162040323019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.001357903005555272 | Validation Loss:  0.0014970815973356366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.0013578702928498387 | Validation Loss:  0.0014970009215176105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.001357837812975049 | Validation Loss:  0.0014969216426834464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.0013578053331002593 | Validation Loss:  0.0014968420146033168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.001357773202471435 | Validation Loss:  0.0014967626193538308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.001357740955427289 | Validation Loss:  0.0014966846210882068\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.0013577090576291084 | Validation Loss:  0.001496605691500008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.0013576771598309278 | Validation Loss:  0.001496527693234384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.001357645378448069 | Validation Loss:  0.0014964501606300473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.0013576139463111758 | Validation Loss:  0.0014963725116103888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.0013575823977589607 | Validation Loss:  0.0014962959103286266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.001357551314868033 | Validation Loss:  0.001496218959800899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.0013575201155617833 | Validation Loss:  0.0014961425913497806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.0013574891490861773 | Validation Loss:  0.0014960671542212367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.0013574581826105714 | Validation Loss:  0.0014959910186007619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.001357427448965609 | Validation Loss:  0.0014959159307181835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.0013573969481512904 | Validation Loss:  0.0014958410756662488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.0013573665637522936 | Validation Loss:  0.001495766337029636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.0013573362957686186 | Validation Loss:  0.0014956925297155976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.0013573060277849436 | Validation Loss:  0.0014956181403249502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.001357276109047234 | Validation Loss:  0.0014955451479181647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.0013572463067248464 | Validation Loss:  0.0014954718062654138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.0013572165044024587 | Validation Loss:  0.0014953988138586283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.0013571868184953928 | Validation Loss:  0.0014953266363590956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.0013571573654189706 | Validation Loss:  0.0014952541096135974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.001357128145173192 | Validation Loss:  0.0014951826306059957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.0013570988085120916 | Validation Loss:  0.0014951112680137157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.0013570698210969567 | Validation Loss:  0.001495039789006114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.0013570408336818218 | Validation Loss:  0.0014949694741517305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.0013570120790973306 | Validation Loss:  0.0014948986936360598\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.0013569833245128393 | Validation Loss:  0.0014948289608582854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.0013569548027589917 | Validation Loss:  0.0014947594609111547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.001356926397420466 | Validation Loss:  0.001494690077379346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.0013568979920819402 | Validation Loss:  0.0014946210430935025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.0013568699359893799 | Validation Loss:  0.0014945522416383028\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Test Loss:  0.0011501767439767718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.02425028197467327 | Validation Loss:  19.881811141967773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  19.835182189941406 | Validation Loss:  8.01113224029541\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  7.996798038482666 | Validation Loss:  18.77802276611328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  18.731494903564453 | Validation Loss:  17.72003746032715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  17.674909591674805 | Validation Loss:  4.451910972595215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  4.430567741394043 | Validation Loss:  0.33504945039749146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.34397485852241516 | Validation Loss:  7.02725076675415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  7.059736251831055 | Validation Loss:  11.03111457824707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  11.071191787719727 | Validation Loss:  7.081521987915039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  7.113712310791016 | Validation Loss:  1.3596680164337158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  1.3521482944488525 | Validation Loss:  0.3690909743309021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.36952805519104004 | Validation Loss:  1.300299882888794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  1.3627392053604126 | Validation Loss:  2.6571433544158936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  2.669193983078003 | Validation Loss:  1.836341142654419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  1.8724887371063232 | Validation Loss:  0.0494270883500576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.05635572597384453 | Validation Loss:  0.07214178144931793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.07190345972776413 | Validation Loss:  0.31605979800224304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.3123375475406647 | Validation Loss:  0.4257087707519531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.4209454655647278 | Validation Loss:  0.2686273753643036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.26541391015052795 | Validation Loss:  0.056000031530857086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.05616879090666771 | Validation Loss:  0.031951624900102615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.03599913418292999 | Validation Loss:  0.17136061191558838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.17836503684520721 | Validation Loss:  0.2531825304031372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.26123103499412537 | Validation Loss:  0.16786400973796844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.17481836676597595 | Validation Loss:  0.03839525952935219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.04269055649638176 | Validation Loss:  0.027066873386502266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.02824849635362625 | Validation Loss:  0.11934702843427658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.1181677058339119 | Validation Loss:  0.16801416873931885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.16606836020946503 | Validation Loss:  0.10646577179431915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.10551769286394119 | Validation Loss:  0.0249971691519022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.026283076032996178 | Validation Loss:  0.026040121912956238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.029818560928106308 | Validation Loss:  0.08400128036737442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.0895145833492279 | Validation Loss:  0.10058154165744781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.10642939060926437 | Validation Loss:  0.0529128797352314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.05766861140727997 | Validation Loss:  0.014074984937906265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.0168762169778347 | Validation Loss:  0.03425593674182892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.035126227885484695 | Validation Loss:  0.07138630747795105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.07116565108299255 | Validation Loss:  0.06517552584409714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.06510496139526367 | Validation Loss:  0.027587758377194405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.02874440886080265 | Validation Loss:  0.01425318792462349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.017090775072574615 | Validation Loss:  0.0355759896337986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.03976735845208168 | Validation Loss:  0.04887361451983452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.05351123586297035 | Validation Loss:  0.031948287039995193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.03599565476179123 | Validation Loss:  0.013956399634480476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.01673073135316372 | Validation Loss:  0.021883657202124596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.023345891386270523 | Validation Loss:  0.038064297288656235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.038791608065366745 | Validation Loss:  0.03427646681666374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.03514595329761505 | Validation Loss:  0.01785358227789402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.019602825865149498 | Validation Loss:  0.014502635225653648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.017385276034474373 | Validation Loss:  0.024488279595971107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.028186067938804626 | Validation Loss:  0.026894107460975647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.030714822933077812 | Validation Loss:  0.017693428322672844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.020941218361258507 | Validation Loss:  0.01365048997104168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.01597379706799984 | Validation Loss:  0.020616255700588226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.022159475833177567 | Validation Loss:  0.02486347407102585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.02615642175078392 | Validation Loss:  0.019011173397302628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.02066797763109207 | Validation Loss:  0.013487642630934715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.015882201492786407 | Validation Loss:  0.01604851335287094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.019137335941195488 | Validation Loss:  0.019319791346788406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.022696703672409058 | Validation Loss:  0.01653386279940605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.019673746079206467 | Validation Loss:  0.013406995683908463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.015956450253725052 | Validation Loss:  0.01575486548244953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.017709963023662567 | Validation Loss:  0.01869100332260132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.020372336730360985 | Validation Loss:  0.01673034392297268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.018581653013825417 | Validation Loss:  0.013624145649373531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.015956971794366837 | Validation Loss:  0.014215439558029175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.01704566739499569 | Validation Loss:  0.015811514109373093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.018873704597353935 | Validation Loss:  0.014733799733221531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.01765383966267109 | Validation Loss:  0.013392939232289791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.015910256654024124 | Validation Loss:  0.014621738344430923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.016730114817619324 | Validation Loss:  0.015967469662427902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.01789834350347519 | Validation Loss:  0.01488213986158371\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.01695050671696663 | Validation Loss:  0.0134590407833457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.015871597453951836 | Validation Loss:  0.013827157206833363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.016568705439567566 | Validation Loss:  0.014375738799571991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.017236173152923584 | Validation Loss:  0.013738790526986122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.01645500957965851 | Validation Loss:  0.013449819758534431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.015868976712226868 | Validation Loss:  0.014303875155746937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.01646737940609455 | Validation Loss:  0.01465997938066721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.016762230545282364 | Validation Loss:  0.013876901008188725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.016132954508066177 | Validation Loss:  0.013392026536166668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.015902474522590637 | Validation Loss:  0.013680333271622658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.016377968713641167 | Validation Loss:  0.013705591671168804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.016411468386650085 | Validation Loss:  0.01340460404753685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.015950433909893036 | Validation Loss:  0.013620170764625072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.015954477712512016 | Validation Loss:  0.014066465198993683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.01627780869603157 | Validation Loss:  0.01390762347728014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.016155900433659554 | Validation Loss:  0.013467464596033096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.015874378383159637 | Validation Loss:  0.013427485711872578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.016000449657440186 | Validation Loss:  0.013526784256100655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.016163652762770653 | Validation Loss:  0.01342016365379095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.01598578691482544 | Validation Loss:  0.01345057412981987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.015869174152612686 | Validation Loss:  0.013718786649405956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.016019465401768684 | Validation Loss:  0.013759331777691841\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.01604769006371498 | Validation Loss:  0.013513579033315182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.01589403674006462 | Validation Loss:  0.013391824439167976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.015896925702691078 | Validation Loss:  0.01342895720154047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.01600329950451851 | Validation Loss:  0.013404520228505135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.015950219705700874 | Validation Loss:  0.013417721726000309\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.01586548052728176 | Validation Loss:  0.01356906071305275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.015923723578453064 | Validation Loss:  0.013629546388983727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.01596037857234478 | Validation Loss:  0.013501747511327267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.015888430178165436 | Validation Loss:  0.013398703187704086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.01587437465786934 | Validation Loss:  0.013397292234003544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.015929125249385834 | Validation Loss:  0.01339311245828867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.015911325812339783 | Validation Loss:  0.013414980843663216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0158658716827631 | Validation Loss:  0.013508478179574013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.015891579911112785 | Validation Loss:  0.013548178598284721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.015911998227238655 | Validation Loss:  0.01347423903644085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.015876837074756622 | Validation Loss:  0.01340398471802473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.015869872644543648 | Validation Loss:  0.013391824439167976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.015896925702691078 | Validation Loss:  0.01339290477335453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.015886494889855385 | Validation Loss:  0.013420735485851765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.015865232795476913 | Validation Loss:  0.0134822316467762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.01587996445596218 | Validation Loss:  0.013499378226697445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.015887346118688583 | Validation Loss:  0.013449856080114841\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.015868986025452614 | Validation Loss:  0.01340517494827509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.015869177877902985 | Validation Loss:  0.013394344598054886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0158817321062088 | Validation Loss:  0.013399754650890827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.015873240306973457 | Validation Loss:  0.01342900563031435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.015865344554185867 | Validation Loss:  0.013468010351061821\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.01587456837296486 | Validation Loss:  0.013468259014189243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.015874655917286873 | Validation Loss:  0.01343240775167942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.015865659341216087 | Validation Loss:  0.013405326753854752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.01586909405887127 | Validation Loss:  0.01339958980679512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.015873407945036888 | Validation Loss:  0.013409764505922794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.01586718112230301 | Validation Loss:  0.01343625970184803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.015866165980696678 | Validation Loss:  0.013456904329359531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.015870947390794754 | Validation Loss:  0.013446934521198273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.015868263319134712 | Validation Loss:  0.013421379961073399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.015865201130509377 | Validation Loss:  0.013406412675976753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.015868540853261948 | Validation Loss:  0.013406544923782349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.015868477523326874 | Validation Loss:  0.013420157134532928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.01586526818573475 | Validation Loss:  0.013439967297017574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.01586678996682167 | Validation Loss:  0.013445980846881866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.015868039801716805 | Validation Loss:  0.013432171195745468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.01586563140153885 | Validation Loss:  0.013415590859949589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.015865769237279892 | Validation Loss:  0.013409346342086792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.01586732640862465 | Validation Loss:  0.013414680026471615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.015865925699472427 | Validation Loss:  0.013428390957415104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.015865301713347435 | Validation Loss:  0.013439002446830273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.015866616740822792 | Validation Loss:  0.013435403816401958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.01586604118347168 | Validation Loss:  0.01342285331338644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.015865158289670944 | Validation Loss:  0.013414050452411175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.015866048634052277 | Validation Loss:  0.013414289802312851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.015866000205278397 | Validation Loss:  0.013422544114291668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.01586516574025154 | Validation Loss:  0.01343237329274416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.01586565561592579 | Validation Loss:  0.013434188440442085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.015865875408053398 | Validation Loss:  0.013426726683974266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.01586521603167057 | Validation Loss:  0.013418368995189667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.01586541160941124 | Validation Loss:  0.013415912166237831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.015865718945860863 | Validation Loss:  0.013420326635241508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.015865257009863853 | Validation Loss:  0.013427904807031155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.015865271911025047 | Validation Loss:  0.01343161053955555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.015865573659539223 | Validation Loss:  0.013427969068288803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.015865275636315346 | Validation Loss:  0.013421371579170227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.015865202993154526 | Validation Loss:  0.013417965732514858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.015865452587604523 | Validation Loss:  0.01341996993869543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.015865277498960495 | Validation Loss:  0.013425390236079693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.01586516946554184 | Validation Loss:  0.013429226353764534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.01586536131799221 | Validation Loss:  0.013427815400063992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.015865270048379898 | Validation Loss:  0.013423067517578602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.015865158289670944 | Validation Loss:  0.013419735245406628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.015865296125411987 | Validation Loss:  0.013420408591628075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.015865253284573555 | Validation Loss:  0.013424172066152096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013427471742033958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.015865251421928406 | Validation Loss:  0.01342713925987482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.015865232795476913 | Validation Loss:  0.013423849828541279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013421021401882172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.01586521789431572 | Validation Loss:  0.013421078212559223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.01586521603167057 | Validation Loss:  0.013423697091639042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013426310382783413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.015865199267864227 | Validation Loss:  0.013426396064460278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.015865201130509377 | Validation Loss:  0.013424104079604149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013421879149973392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.015865184366703033 | Validation Loss:  0.013421732932329178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.015865188091993332 | Validation Loss:  0.013423596508800983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013425581157207489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.015865175053477287 | Validation Loss:  0.01342574693262577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.015865178778767586 | Validation Loss:  0.013424096629023552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013422424905002117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.01586516946554184 | Validation Loss:  0.013422287069261074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.015865173190832138 | Validation Loss:  0.013423667289316654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013425125740468502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.015865163877606392 | Validation Loss:  0.013425225391983986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.01586516760289669 | Validation Loss:  0.013423994183540344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013422769494354725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.015865160152316093 | Validation Loss:  0.013422726653516293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.015865162014961243 | Validation Loss:  0.013423789292573929\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424835167825222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.015865160152316093 | Validation Loss:  0.013424819335341454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.015865160152316093 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423004187643528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.015865160152316093 | Validation Loss:  0.013423077762126923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.015865158289670944 | Validation Loss:  0.013423919677734375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424628414213657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.015865158289670944 | Validation Loss:  0.013424498029053211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423740863800049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342318020761013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.015865158289670944 | Validation Loss:  0.013423359021544456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013424023985862732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424459844827652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424243777990341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423657044768333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423336669802666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.015865156427025795 | Validation Loss:  0.01342358160763979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424086384475231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424312695860863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013424043543636799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423614203929901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423478230834007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423752039670944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424109667539597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424170203506947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423899188637733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423611409962177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423611409962177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423871248960495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424091041088104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424047268927097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423801399767399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423643074929714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423729687929153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423949480056763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013424047268927097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423933647572994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342375110834837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423705473542213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423826545476913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423973694443703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423980213701725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423852622509003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423748314380646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423766009509563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423888012766838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423965312540531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423916883766651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423807919025421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423766009509563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423827476799488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423919677734375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423936441540718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423864729702473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423793017864227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423801399767399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423874042928219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342392060905695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342389639467001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423827476799488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423806987702847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423839583992958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342389639467001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423900119960308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423818163573742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423833064734936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423874042928219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342389639467001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423871248960495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423833064734936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423827476799488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423885218799114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423879630863667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423849828541279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423833064734936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423847034573555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423876836895943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423876836895943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423839583992958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.015865156427025795 | Validation Loss:  0.013423849828541279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423864729702473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423873111605644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423849828541279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423849828541279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423868454992771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423863798379898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423852622509003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423852622509003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423863798379898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423863798379898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.015865152701735497 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423856347799301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.015865152701735497 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.015865154564380646 | Validation Loss:  0.01342385821044445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423859141767025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.015865154564380646 | Validation Loss:  0.013423861004412174\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Test Loss:  0.01356145367026329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.04851577803492546 | Validation Loss:  0.0266716331243515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.030548375099897385 | Validation Loss:  0.016498297452926636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.019654177129268646 | Validation Loss:  0.012732419185340405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.015210689045488834 | Validation Loss:  0.013887622393667698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.01577211357653141 | Validation Loss:  0.017041156068444252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.01848544366657734 | Validation Loss:  0.01913326047360897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.020329229533672333 | Validation Loss:  0.0190269835293293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.020142149180173874 | Validation Loss:  0.017208553850650787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.018362971022725105 | Validation Loss:  0.014664224348962307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.015934085473418236 | Validation Loss:  0.01225302740931511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.0136791430413723 | Validation Loss:  0.010497966781258583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.012092521414160728 | Validation Loss:  0.009564526379108429\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.0113155422732234 | Validation Loss:  0.00932108424603939\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.011196437291800976 | Validation Loss:  0.009450913406908512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.01140332967042923 | Validation Loss:  0.009591612033545971\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.011564564891159534 | Validation Loss:  0.009466808289289474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.011400840245187283 | Validation Loss:  0.008964726701378822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.010803100652992725 | Validation Loss:  0.008143256418406963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.009836191311478615 | Validation Loss:  0.00718143954873085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.00868910364806652 | Validation Loss:  0.006306203082203865\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.0076010823249816895 | Validation Loss:  0.005711750127375126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.00678093358874321 | Validation Loss:  0.005481186788529158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.006328708957880735 | Validation Loss:  0.0055301012471318245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.006178512237966061 | Validation Loss:  0.0056213513016700745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.006110280752182007 | Validation Loss:  0.00548974797129631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.0058695594780147076 | Validation Loss:  0.005003990139812231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.005326482467353344 | Validation Loss:  0.00423766952008009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.004547602962702513 | Validation Loss:  0.0034127698745578527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.0037420568987727165 | Validation Loss:  0.0027696113102138042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.003134005470201373 | Validation Loss:  0.002437104471027851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.002834924263879657 | Validation Loss:  0.0023667877539992332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.002780129900202155 | Validation Loss:  0.0023773489519953728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.002776903100311756 | Validation Loss:  0.002297867089509964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.0026511498726904392 | Validation Loss:  0.002102035330608487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.0023823759984225035 | Validation Loss:  0.0019195422064512968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.0021128919906914234 | Validation Loss:  0.0019171653548255563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.00202550133690238 | Validation Loss:  0.0021354127675294876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.0021763057447969913 | Validation Loss:  0.0024203117936849594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.0024221588391810656 | Validation Loss:  0.0025586728006601334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.0025526226963847876 | Validation Loss:  0.002492450876161456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.0025048325769603252 | Validation Loss:  0.0023518719244748354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.00239978707395494 | Validation Loss:  0.002293285448104143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.002382598351687193 | Validation Loss:  0.0023420846555382013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.00246704020537436 | Validation Loss:  0.0023953388445079327\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.0025407085195183754 | Validation Loss:  0.0023570326156914234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.002503161784261465 | Validation Loss:  0.002238673157989979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.0023676049895584583 | Validation Loss:  0.002128237159922719\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.002228625351563096 | Validation Loss:  0.002090130001306534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.002159804804250598 | Validation Loss:  0.0021065985783934593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.002152562141418457 | Validation Loss:  0.002109172521159053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.0021452237851917744 | Validation Loss:  0.0020542817655950785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.0020970385521650314 | Validation Loss:  0.001959759509190917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.0020242570899426937 | Validation Loss:  0.001876038033515215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.0019720089621841908 | Validation Loss:  0.0018358184024691582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.001965498784556985 | Validation Loss:  0.0018326881108805537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.0019907017704099417 | Validation Loss:  0.0018389621982350945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.002014296827837825 | Validation Loss:  0.0018363099079579115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.0020156786777079105 | Validation Loss:  0.0018290813313797116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.002000534674152732 | Validation Loss:  0.0018331852043047547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.001988933654502034 | Validation Loss:  0.0018559041200205684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.0019937267061322927 | Validation Loss:  0.00188646640162915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0020095291547477245 | Validation Loss:  0.001904733944684267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.002020027954131365 | Validation Loss:  0.0018983667250722647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.0020142935682088137 | Validation Loss:  0.001872073276899755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.0019958848133683205 | Validation Loss:  0.0018416506936773658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.001977403648197651 | Validation Loss:  0.0018202142091467977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0019677325617522 | Validation Loss:  0.0018097921274602413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.0019648480229079723 | Validation Loss:  0.0018043878953903913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0019600156228989363 | Validation Loss:  0.0017992033390328288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0019477339228615165 | Validation Loss:  0.0017959243850782514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0019311218056827784 | Validation Loss:  0.0017993965884670615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0019180730450898409 | Validation Loss:  0.0018103201873600483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.0019129932625219226 | Validation Loss:  0.0018224390223622322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.001912908861413598 | Validation Loss:  0.001827502972446382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.0019115351606160402 | Validation Loss:  0.0018226761603727937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.0019062365172430873 | Validation Loss:  0.0018125100759789348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.0019000901374965906 | Validation Loss:  0.0018038969719782472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.0018974056001752615 | Validation Loss:  0.0017999781994149089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0018984890775755048 | Validation Loss:  0.0017991986824199557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0018995861755684018 | Validation Loss:  0.001799161545932293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0018973842961713672 | Validation Loss:  0.0017998431576415896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.0018924223259091377 | Validation Loss:  0.0018025487661361694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.0018877481343224645 | Validation Loss:  0.001806693966500461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.0018850364722311497 | Validation Loss:  0.0018089471850544214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0018829521723091602 | Validation Loss:  0.0018060035072267056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0018792888149619102 | Validation Loss:  0.0017977190436795354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0018738053040578961 | Validation Loss:  0.001787098590284586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0018683215603232384 | Validation Loss:  0.00177751702722162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.001864394173026085 | Validation Loss:  0.0017704443307593465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0018616769229993224 | Validation Loss:  0.0017656756099313498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.0018587446538731456 | Validation Loss:  0.0017628647619858384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.0018549865344539285 | Validation Loss:  0.0017620886210352182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.001851172186434269 | Validation Loss:  0.0017629923531785607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.0018482684390619397 | Validation Loss:  0.0017640703590586782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.0018462054431438446 | Validation Loss:  0.0017633691895753145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.001844066777266562 | Validation Loss:  0.001759986043907702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.0018412534845992923 | Validation Loss:  0.0017547173192724586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.0018380816327407956 | Validation Loss:  0.0017492446349933743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0018351926701143384 | Validation Loss:  0.0017448634607717395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0018326891586184502 | Validation Loss:  0.0017419998766854405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0018300871597602963 | Validation Loss:  0.0017405964899808168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0018270120490342379 | Validation Loss:  0.001740497536957264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0018236578907817602 | Validation Loss:  0.0017413118621334434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.0018204798689112067 | Validation Loss:  0.0017421719385311007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0018176157027482986 | Validation Loss:  0.001742013031616807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.0018147960072383285 | Validation Loss:  0.0017402764642611146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0018117806175723672 | Validation Loss:  0.0017373013542965055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.001808680361136794 | Validation Loss:  0.001733998185954988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0018057678826153278 | Validation Loss:  0.0017311922274529934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.001803097897209227 | Validation Loss:  0.0017292702104896307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0018004643497988582 | Validation Loss:  0.0017282620538026094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0017977012321352959 | Validation Loss:  0.0017279874300584197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0017948785098269582 | Validation Loss:  0.0017280362080782652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0017921535763889551 | Validation Loss:  0.0017277735751122236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0017895306227728724 | Validation Loss:  0.0017266032518818974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.001786869834177196 | Validation Loss:  0.0017243439797312021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.001784094492904842 | Validation Loss:  0.0017213355749845505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0017812836449593306 | Validation Loss:  0.0017181800212711096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.001778538920916617 | Validation Loss:  0.0017153831431642175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.0017758485628291965 | Validation Loss:  0.0017131948843598366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0017731284024193883 | Validation Loss:  0.0017116257222369313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.0017703617922961712 | Validation Loss:  0.0017104856669902802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.001767617417499423 | Validation Loss:  0.0017094124341383576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0017649439396336675 | Validation Loss:  0.001707997522316873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0017623081803321838 | Validation Loss:  0.0017060076352208853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.001759655773639679 | Validation Loss:  0.0017035268247127533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0017569921910762787 | Validation Loss:  0.001700887456536293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0017543603898957372 | Validation Loss:  0.0016984588000923395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0017517695669084787 | Validation Loss:  0.0016964786918833852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0017491828184574842 | Validation Loss:  0.0016950004501268268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.00174657569732517 | Validation Loss:  0.0016939050983637571\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.001743967761285603 | Validation Loss:  0.001692941295914352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.001741384039632976 | Validation Loss:  0.0016918190522119403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.001738818595185876 | Validation Loss:  0.001690350123681128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0017362483777105808 | Validation Loss:  0.001688548014499247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0017336736200377345 | Validation Loss:  0.0016866048099473119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0017311153933405876 | Validation Loss:  0.0016847640508785844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.0017285823123529553 | Validation Loss:  0.001683197682723403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0017260608728975058 | Validation Loss:  0.0016819462180137634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.0017235414125025272 | Validation Loss:  0.0016809195512905717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0017210330115631223 | Validation Loss:  0.0016799344448372722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0017185453325510025 | Validation Loss:  0.00167879369109869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0017160737188532948 | Validation Loss:  0.0016773842507973313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.0017136079259216785 | Validation Loss:  0.0016757349949330091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.00171114772092551 | Validation Loss:  0.0016739870188757777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.001708701835013926 | Validation Loss:  0.0016723090084269643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0017062713159248233 | Validation Loss:  0.001670811790972948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0017038488294929266 | Validation Loss:  0.0016695085214450955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0017014326294884086 | Validation Loss:  0.0016683198045939207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0016990284202620387 | Validation Loss:  0.001667114207521081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.0016966392286121845 | Validation Loss:  0.0016657760133966804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0016942626098170877 | Validation Loss:  0.0016642690170556307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0016918954206630588 | Validation Loss:  0.0016626528231427073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0016895401058718562 | Validation Loss:  0.0016610448947176337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.0016871999250724912 | Validation Loss:  0.001659552683122456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.0016848735976964235 | Validation Loss:  0.0016582249663770199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0016825580969452858 | Validation Loss:  0.0016570341540500522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.001680254121311009 | Validation Loss:  0.0016558971256017685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0016779642319306731 | Validation Loss:  0.0016547194682061672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.0016756876138970256 | Validation Loss:  0.001653447630815208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.001673422520980239 | Validation Loss:  0.0016520966310054064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0016711693024262786 | Validation Loss:  0.0016507355030626059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0016689295880496502 | Validation Loss:  0.0016494439914822578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.0016667032614350319 | Validation Loss:  0.0016482674982398748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0016644891584292054 | Validation Loss:  0.0016471970593556762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.001662287744693458 | Validation Loss:  0.001646177377551794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0016601001843810081 | Validation Loss:  0.001645137439481914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.0016579267103224993 | Validation Loss:  0.0016440314939245582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0016557667404413223 | Validation Loss:  0.001642861170694232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0016536195762455463 | Validation Loss:  0.0016416723374277353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0016514863818883896 | Validation Loss:  0.0016405221540480852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.0016493673902004957 | Validation Loss:  0.0016394462436437607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0016472616698592901 | Validation Loss:  0.0016384398331865668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.0016451694536954165 | Validation Loss:  0.0016374652041122317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.0016430915566161275 | Validation Loss:  0.0016364726470783353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.001641027512960136 | Validation Loss:  0.0016354334075003862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.0016389770898967981 | Validation Loss:  0.001634354586713016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.0016369406366720796 | Validation Loss:  0.0016332720406353474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0016349183861166239 | Validation Loss:  0.0016322287265211344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0016329106874763966 | Validation Loss:  0.0016312465304508805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.0016309167258441448 | Validation Loss:  0.0016303189331665635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.001628936966881156 | Validation Loss:  0.0016294143861159682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0016269718762487173 | Validation Loss:  0.001628500409424305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0016250209882855415 | Validation Loss:  0.0016275623347610235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.0016230845358222723 | Validation Loss:  0.0016266126185655594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.0016211620531976223 | Validation Loss:  0.001625680597499013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.001619254588149488 | Validation Loss:  0.0016247930470854044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.0016173613257706165 | Validation Loss:  0.0016239570686593652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.0016154823824763298 | Validation Loss:  0.0016231577610597014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.0016136177582666278 | Validation Loss:  0.001622369047254324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0016117679188027978 | Validation Loss:  0.0016215700889006257\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0016099321655929089 | Validation Loss:  0.001620759372599423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0016081110807135701 | Validation Loss:  0.0016199531964957714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.0016063046641647816 | Validation Loss:  0.0016191727481782436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.0016045126831158996 | Validation Loss:  0.0016184303676709533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.001602735137566924 | Validation Loss:  0.0016177205834537745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.0016009720275178552 | Validation Loss:  0.0016170259332284331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0015992237022146583 | Validation Loss:  0.001616328489035368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0015974894631654024 | Validation Loss:  0.0016156226629391313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0015957695432007313 | Validation Loss:  0.0016149170696735382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0015940644079819322 | Validation Loss:  0.0016142282402142882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.001592373359017074 | Validation Loss:  0.001613568514585495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0015906969783827662 | Validation Loss:  0.0016129370778799057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0015890346840023994 | Validation Loss:  0.0016123232198879123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0015873865922912955 | Validation Loss:  0.0016117127379402518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.0015857528196647763 | Validation Loss:  0.0016111008590087295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0015841331332921982 | Validation Loss:  0.0016104928217828274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.0015825277660042048 | Validation Loss:  0.0016099006170406938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.0015809363685548306 | Validation Loss:  0.0016093344893306494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0015793588245287538 | Validation Loss:  0.001608793972991407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.001577795366756618 | Validation Loss:  0.0016082706861197948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.0015762457624077797 | Validation Loss:  0.001607754034921527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0015747100114822388 | Validation Loss:  0.0016072402941063046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.0015731878811493516 | Validation Loss:  0.0016067334217950702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0015716797206550837 | Validation Loss:  0.0016062421491369605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.001570184831507504 | Validation Loss:  0.0016057728789746761\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.0015687034465372562 | Validation Loss:  0.0016053238650783896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0015672354493290186 | Validation Loss:  0.001604888355359435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0015657811891287565 | Validation Loss:  0.0016044584335759282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0015643397346138954 | Validation Loss:  0.0016040322370827198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.0015629115514457226 | Validation Loss:  0.0016036139568313956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0015614962903782725 | Validation Loss:  0.0016032103449106216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.001560094184242189 | Validation Loss:  0.0016028244281187654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.0015587047673761845 | Validation Loss:  0.0016024538781493902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0015573282726109028 | Validation Loss:  0.001602092757821083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0015559643507003784 | Validation Loss:  0.0016017368761822581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.0015546127688139677 | Validation Loss:  0.0016013861168175936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0015532737597823143 | Validation Loss:  0.0016010457184165716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0015519472071900964 | Validation Loss:  0.0016007195226848125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0015506327617913485 | Validation Loss:  0.0016004088101908565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.0015493301907554269 | Validation Loss:  0.0016001102048903704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.0015480398433282971 | Validation Loss:  0.0015998189337551594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0015467614866793156 | Validation Loss:  0.0015995330177247524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0015454946551471949 | Validation Loss:  0.001599255483597517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0015442394651472569 | Validation Loss:  0.0015989895910024643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0015429958002641797 | Validation Loss:  0.0015987372025847435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.0015417635440826416 | Validation Loss:  0.0015984971541911364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0015405425801873207 | Validation Loss:  0.0015982656041160226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0015393327921628952 | Validation Loss:  0.001598039292730391\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0015381339471787214 | Validation Loss:  0.001597819966264069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0015369461616501212 | Validation Loss:  0.0015976099530234933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.0015357689699158072 | Validation Loss:  0.001597410999238491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0015346026048064232 | Validation Loss:  0.0015972224064171314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0015334467170760036 | Validation Loss:  0.0015970420790836215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.001532301539555192 | Validation Loss:  0.0015968672232702374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.0015311663737520576 | Validation Loss:  0.0015966978389769793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.001530041336081922 | Validation Loss:  0.0015965360216796398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.0015289266593754292 | Validation Loss:  0.0015963836340233684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0015278218779712915 | Validation Loss:  0.001596240559592843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0015267267590388656 | Validation Loss:  0.0015961048193275928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0015256415354087949 | Validation Loss:  0.0015959746669977903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.001524565857835114 | Validation Loss:  0.00159584975335747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.001523499726317823 | Validation Loss:  0.0015957315918058157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0015224431408569217 | Validation Loss:  0.0015956214629113674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.001521395519375801 | Validation Loss:  0.0015955195995047688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.0015203574439510703 | Validation Loss:  0.001595424604602158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0015193279832601547 | Validation Loss:  0.001595335197634995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0015183078357949853 | Validation Loss:  0.001595250447280705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.001517296303063631 | Validation Loss:  0.00159517175052315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0015162936178967357 | Validation Loss:  0.00159510038793087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0015152996638789773 | Validation Loss:  0.0015950361266732216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0015143143245950341 | Validation Loss:  0.001594977336935699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0015133372507989407 | Validation Loss:  0.0015949234366416931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0015123685589060187 | Validation Loss:  0.0015948739601299167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0015114078996703029 | Validation Loss:  0.0015948296058923006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0015104555059224367 | Validation Loss:  0.001594791654497385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0015095113776624203 | Validation Loss:  0.0015947595238685608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.001508575165644288 | Validation Loss:  0.0015947322826832533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.00150764686986804 | Validation Loss:  0.001594708883203566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0015067261410877109 | Validation Loss:  0.0015946896746754646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.001505813212133944 | Validation Loss:  0.00159467535559088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0015049079665914178 | Validation Loss:  0.0015946663916110992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.0015040101716294885 | Validation Loss:  0.001594662549905479\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.0015031197108328342 | Validation Loss:  0.0015946627827361226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0015022368170320988 | Validation Loss:  0.0015946663916110992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.0015013606753200293 | Validation Loss:  0.0015946745406836271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.001500492449849844 | Validation Loss:  0.0015946868807077408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.0014996310928836465 | Validation Loss:  0.001594704226590693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.0014987767208367586 | Validation Loss:  0.0015947251813486218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.0014979293337091804 | Validation Loss:  0.001594749977812171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0014970891643315554 | Validation Loss:  0.001594778150320053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.001496255281381309 | Validation Loss:  0.0015948100481182337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0014954283833503723 | Validation Loss:  0.001594846136868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0014946081209927797 | Validation Loss:  0.001594885834492743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.001493794727139175 | Validation Loss:  0.0015949291409924626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0014929876197129488 | Validation Loss:  0.001594975241459906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0014921873807907104 | Validation Loss:  0.001595025067217648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0014913934282958508 | Validation Loss:  0.0015950778033584356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.0014906057622283697 | Validation Loss:  0.0015951341483741999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0014898244990035892 | Validation Loss:  0.001595193985849619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0014890494057908654 | Validation Loss:  0.0015952562680467963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0014882805990055203 | Validation Loss:  0.0015953212277963758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.001487517962232232 | Validation Loss:  0.0015953894471749663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0014867611462250352 | Validation Loss:  0.00159546104259789\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.001486010616645217 | Validation Loss:  0.0015955353155732155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0014852661406621337 | Validation Loss:  0.0015956119168549776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.00148452736902982 | Validation Loss:  0.0015956914285197854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.0014837946509942412 | Validation Loss:  0.0015957735013216734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.0014830677537247539 | Validation Loss:  0.001595858484506607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0014823463279753923 | Validation Loss:  0.001595945912413299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.001481630839407444 | Validation Loss:  0.0015960360178723931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.001480921171605587 | Validation Loss:  0.00159612821880728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.001480216858908534 | Validation Loss:  0.0015962227480486035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.001479518017731607 | Validation Loss:  0.001596319954842329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.0014788252301514149 | Validation Loss:  0.0015964192571118474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.0014781375648453832 | Validation Loss:  0.0015965207712724805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.0014774553710594773 | Validation Loss:  0.0015966244973242283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.0014767785323783755 | Validation Loss:  0.0015967300860211253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.0014761072816327214 | Validation Loss:  0.0015968376537784934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.001475440920330584 | Validation Loss:  0.001596947549842298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.0014747799141332507 | Validation Loss:  0.0015970595413818955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.0014741242630407214 | Validation Loss:  0.0015971732791513205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.0014734737342223525 | Validation Loss:  0.0015972885303199291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.001472828327678144 | Validation Loss:  0.001597405644133687\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.0014721880434080958 | Validation Loss:  0.0015975250862538815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.00147155299782753 | Validation Loss:  0.0015976461581885815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.001470922608859837 | Validation Loss:  0.0015977686271071434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.0014702973421663046 | Validation Loss:  0.0015978924930095673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.0014696770813316107 | Validation Loss:  0.0015980183379724622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.0014690615935251117 | Validation Loss:  0.0015981454635038972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.0014684509951621294 | Validation Loss:  0.001598274684511125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.001467845169827342 | Validation Loss:  0.0015984044875949621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.0014672440011054277 | Validation Loss:  0.0015985361533239484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.0014666476054117084 | Validation Loss:  0.0015986690996214747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.0014660559827461839 | Validation Loss:  0.001598803442902863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.0014654690166935325 | Validation Loss:  0.0015989390667527914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.0014648865908384323 | Validation Loss:  0.001599075854755938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.0014643087051808834 | Validation Loss:  0.0015992136904969811\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.0014637353597208858 | Validation Loss:  0.001599352341145277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.0014631666708737612 | Validation Loss:  0.0015994925051927567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.001462602405808866 | Validation Loss:  0.0015996340662240982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.0014620423316955566 | Validation Loss:  0.0015997759765014052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.0014614869141951203 | Validation Loss:  0.0015999190509319305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.001460935571230948 | Validation Loss:  0.0016000629402697086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.001460388652049005 | Validation Loss:  0.001600207993760705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.0014598460402339697 | Validation Loss:  0.0016003536293283105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.0014593076193705201 | Validation Loss:  0.0016005001962184906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.0014587736222893 | Validation Loss:  0.001600647228769958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.001458243583329022 | Validation Loss:  0.001600795192644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.0014577177353203297 | Validation Loss:  0.0016009435057640076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.0014571959618479013 | Validation Loss:  0.0016010927502065897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.0014566781464964151 | Validation Loss:  0.001601242576725781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.0014561646385118365 | Validation Loss:  0.0016013928689062595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.0014556548558175564 | Validation Loss:  0.00160154327750206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.001455149264074862 | Validation Loss:  0.0016016945010051131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.0014546472812071443 | Validation Loss:  0.0016018461901694536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.0014541494892910123 | Validation Loss:  0.001601997995749116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.0014536554226651788 | Validation Loss:  0.0016021501505747437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.0014531651977449656 | Validation Loss:  0.001602302654646337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.0014526788145303726 | Validation Loss:  0.0016024552751332521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.0014521959237754345 | Validation Loss:  0.0016026082448661327\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.0014517167583107948 | Validation Loss:  0.0016027615638449788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.0014512415509670973 | Validation Loss:  0.001602914766408503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.0014507698360830545 | Validation Loss:  0.0016030679689720273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.0014503018464893103 | Validation Loss:  0.0016032212879508734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.0014498373493552208 | Validation Loss:  0.0016033746069297194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.001449376461096108 | Validation Loss:  0.001603528275154531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.001448918948881328 | Validation Loss:  0.0016036814777180552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.0014484651619568467 | Validation Loss:  0.0016038345638662577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.0014480144018307328 | Validation Loss:  0.001603987766429782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.0014475674834102392 | Validation Loss:  0.0016041405033320189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.0014471238246187568 | Validation Loss:  0.0016042932402342558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.0014466834254562855 | Validation Loss:  0.001604445744305849\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.0014462466351687908 | Validation Loss:  0.001604597782716155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.0014458128716796637 | Validation Loss:  0.001604749821126461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.0014453823678195477 | Validation Loss:  0.0016049013938754797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.0014449551235884428 | Validation Loss:  0.0016050526173785329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.001444531255401671 | Validation Loss:  0.001605203258804977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.0014441104140132666 | Validation Loss:  0.0016053536674007773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.0014436927158385515 | Validation Loss:  0.0016055037267506123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.0014432781608775258 | Validation Loss:  0.0016056533204391599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.0014428666327148676 | Validation Loss:  0.0016058019828051329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.0014424582477658987 | Validation Loss:  0.0016059504123404622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.0014420528896152973 | Validation Loss:  0.0016060983762145042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.0014416505582630634 | Validation Loss:  0.0016062457580119371\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.0014412511372938752 | Validation Loss:  0.0016063922084867954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.0014408546267077327 | Validation Loss:  0.0016065379604697227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.0014404610265046358 | Validation Loss:  0.0016066833632066846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.0014400702202692628 | Validation Loss:  0.0016068281838670373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.001439682557247579 | Validation Loss:  0.0016069721896201372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.0014392975717782974 | Validation Loss:  0.0016071150312200189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.0014389153802767396 | Validation Loss:  0.0016072577564045787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.0014385360991582274 | Validation Loss:  0.0016073996666818857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.0014381592627614737 | Validation Loss:  0.0016075404128059745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.001437785685993731 | Validation Loss:  0.0016076802276074886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.001437414437532425 | Validation Loss:  0.0016078196931630373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.0014370459830388427 | Validation Loss:  0.001607958460226655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.0014366799732670188 | Validation Loss:  0.001608095713891089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.0014363169902935624 | Validation Loss:  0.0016082319198176265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.0014359564520418644 | Validation Loss:  0.0016083680093288422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.0014355984749272466 | Validation Loss:  0.001608503283932805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.0014352429425343871 | Validation Loss:  0.0016086369287222624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.0014348902041092515 | Validation Loss:  0.0016087698750197887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.0014345399104058743 | Validation Loss:  0.0016089020064100623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.0014341920614242554 | Validation Loss:  0.001609033439308405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.0014338467735797167 | Validation Loss:  0.0016091635916382074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.0014335039304569364 | Validation Loss:  0.0016092924633994699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.0014331636484712362 | Validation Loss:  0.0016094207530841231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.0014328256947919726 | Validation Loss:  0.0016095483442768455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.0014324900694191456 | Validation Loss:  0.0016096746549010277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.001432156772352755 | Validation Loss:  0.0016097994521260262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.0014318260364234447 | Validation Loss:  0.0016099239001050591\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.001431497628800571 | Validation Loss:  0.0016100474167615175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.0014311715494841337 | Validation Loss:  0.001610169536434114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.001430847798474133 | Validation Loss:  0.0016102903755381703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.0014305263757705688 | Validation Loss:  0.0016104108653962612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.0014302071649581194 | Validation Loss:  0.0016105305403470993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.0014298902824521065 | Validation Loss:  0.0016106483526527882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.0014295754954218864 | Validation Loss:  0.0016107652336359024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.0014292632695287466 | Validation Loss:  0.0016108812997117639\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.0014289531391113997 | Validation Loss:  0.0016109967837110162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.001428644871339202 | Validation Loss:  0.001611110637895763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.0014283392811194062 | Validation Loss:  0.0016112234443426132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.0014280357863754034 | Validation Loss:  0.0016113354358822107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.0014277343871071935 | Validation Loss:  0.0016114467289298773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.00142743531614542 | Validation Loss:  0.0016115567414090037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.0014271378749981523 | Validation Loss:  0.0016116658225655556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.0014268428785726428 | Validation Loss:  0.001611773855984211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.0014265502104535699 | Validation Loss:  0.001611880725249648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.0014262594049796462 | Validation Loss:  0.0016119866631925106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.0014259706949815154 | Validation Loss:  0.0016120917862281203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.0014256839640438557 | Validation Loss:  0.0016121958615258336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.0014253996778279543 | Validation Loss:  0.0016122987726703286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.001425117370672524 | Validation Loss:  0.0016124011017382145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.0014248369261622429 | Validation Loss:  0.0016125021502375603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.0014245586935430765 | Validation Loss:  0.001612602500244975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.0014242823235690594 | Validation Loss:  0.0016127015696838498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.0014240080490708351 | Validation Loss:  0.0016128001734614372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.0014237358700484037 | Validation Loss:  0.0016128976130858064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.0014234656700864434 | Validation Loss:  0.001612994121387601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.0014231976820155978 | Validation Loss:  0.001613089581951499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.0014229313237592578 | Validation Loss:  0.001613184460438788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.0014226670609787107 | Validation Loss:  0.0016132784076035023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.0014224047772586346 | Validation Loss:  0.0016133713070303202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.0014221447054296732 | Validation Loss:  0.0016134637407958508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.0014218863798305392 | Validation Loss:  0.001613555126823485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.0014216301497071981 | Validation Loss:  0.0016136455815285444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.0014213755493983626 | Validation Loss:  0.001613735337741673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.0014211232773959637 | Validation Loss:  0.0016138242790475488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.0014208725187927485 | Validation Loss:  0.0016139124054461718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.0014206240884959698 | Validation Loss:  0.0016139996005222201\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.0014203774044290185 | Validation Loss:  0.0016140865627676249\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.0014201325830072165 | Validation Loss:  0.0016141724772751331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.0014198898570612073 | Validation Loss:  0.0016142574604600668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.0014196491101756692 | Validation Loss:  0.0016143418615683913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.0014194101095199585 | Validation Loss:  0.0016144256806001067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.0014191728550940752 | Validation Loss:  0.0016145085683092475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.0014189376961439848 | Validation Loss:  0.0016145911067724228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.0014187043998390436 | Validation Loss:  0.0016146728303283453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.0014184728497639298 | Validation Loss:  0.0016147538553923368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.001418243395164609 | Validation Loss:  0.001614834414795041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.0014180156867951155 | Validation Loss:  0.0016149142757058144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.0014177898410707712 | Validation Loss:  0.0016149933217093349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.0014175658579915762 | Validation Loss:  0.0016150721348822117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.0014173438539728522 | Validation Loss:  0.0016151503659784794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.0014171234797686338 | Validation Loss:  0.0016152277821674943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.0014169050846248865 | Validation Loss:  0.0016153046162799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.0014166885521262884 | Validation Loss:  0.0016153812175616622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.0014164739986881614 | Validation Loss:  0.001615457353182137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.001416260958649218 | Validation Loss:  0.001615532673895359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.0014160498976707458 | Validation Loss:  0.0016156077617779374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.0014158408157527447 | Validation Loss:  0.0016156821511685848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.001415633363649249 | Validation Loss:  0.0016157563077285886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.0014154277741909027 | Validation Loss:  0.0016158298822119832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.0014152240473777056 | Validation Loss:  0.0016159029910340905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.0014150220667943358 | Validation Loss:  0.0016159760998561978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.0014148219488561153 | Validation Loss:  0.0016160485101863742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.001414623809978366 | Validation Loss:  0.0016161203384399414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.0014144271844998002 | Validation Loss:  0.0016161921666935086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.0014142324216663837 | Validation Loss:  0.0016162636457011104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.0014140395214781165 | Validation Loss:  0.001616334542632103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.0014138482511043549 | Validation Loss:  0.0016164049739018083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.0014136589597910643 | Validation Loss:  0.0016164755215868354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.0014134712982922792 | Validation Loss:  0.0016165456036105752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.0014132856158539653 | Validation Loss:  0.001616614987142384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.001413101446814835 | Validation Loss:  0.0016166848363354802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.001412919140420854 | Validation Loss:  0.001616754219867289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.0014127386966720223 | Validation Loss:  0.001616822904907167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.0014125598827376962 | Validation Loss:  0.0016168915899470448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.0014123828150331974 | Validation Loss:  0.001616960158571601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.001412207493558526 | Validation Loss:  0.0016170286107808352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.001412033918313682 | Validation Loss:  0.0016170963644981384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.0014118620892986655 | Validation Loss:  0.001617164583876729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.0014116920065134764 | Validation Loss:  0.0016172319883480668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.0014115236699581146 | Validation Loss:  0.0016172993928194046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.0014113569632172585 | Validation Loss:  0.0016173666808754206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.0014111922355368733 | Validation Loss:  0.0016174340853467584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.0014110289048403502 | Validation Loss:  0.0016175010241568089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.0014108672039583325 | Validation Loss:  0.0016175680793821812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.0014107075985521078 | Validation Loss:  0.0016176350181922317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.001410549390129745 | Validation Loss:  0.001617701374925673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.0014103929279372096 | Validation Loss:  0.001617767964489758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.0014102380955591798 | Validation Loss:  0.0016178344376385212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.0014100851258262992 | Validation Loss:  0.0016179007943719625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.0014099336694926023 | Validation Loss:  0.0016179669182747602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.001409783959388733 | Validation Loss:  0.0016180331585928798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.0014096357626840472 | Validation Loss:  0.0016180992824956775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.0014094891957938671 | Validation Loss:  0.0016181654063984752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.0014093444915488362 | Validation Loss:  0.001618231413885951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.0014092011842876673 | Validation Loss:  0.0016182971885427833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.001409059506841004 | Validation Loss:  0.001618363312445581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.0014089196920394897 | Validation Loss:  0.0016184290871024132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.0014087812742218375 | Validation Loss:  0.0016184948617592454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.0014086446026340127 | Validation Loss:  0.0016185605200007558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.0014085094444453716 | Validation Loss:  0.0016186265274882317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.0014083759160712361 | Validation Loss:  0.0016186920693144202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.0014082439010962844 | Validation Loss:  0.0016187578439712524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.0014081133995205164 | Validation Loss:  0.0016188236186280847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.0014079844113439322 | Validation Loss:  0.001618889276869595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.001407857402227819 | Validation Loss:  0.0016189550515264273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.0014077314408496022 | Validation Loss:  0.0016190207097679377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.0014076072257012129 | Validation Loss:  0.001619086368009448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.001407484756782651 | Validation Loss:  0.0016191521426662803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.001407363684847951 | Validation Loss:  0.001619217568077147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.0014072440098971128 | Validation Loss:  0.0016192833427339792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.0014071258483454585 | Validation Loss:  0.001619349350221455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.001407009200192988 | Validation Loss:  0.0016194148920476437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.0014068939490243793 | Validation Loss:  0.0016194808995351195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.001406780444085598 | Validation Loss:  0.001619546441361308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.0014066683361306787 | Validation Loss:  0.0016196120996028185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.0014065573923289776 | Validation Loss:  0.0016196781070902944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.0014064483111724257 | Validation Loss:  0.0016197439981624484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.0014063403941690922 | Validation Loss:  0.0016198094235733151\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.001406234223395586 | Validation Loss:  0.001619875431060791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.0014061289839446545 | Validation Loss:  0.0016199415549635887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.0014060256071388721 | Validation Loss:  0.001620007213205099\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.0014059232780709863 | Validation Loss:  0.0016200731042772532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.001405822578817606 | Validation Loss:  0.001620139111764729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.0014057231601327658 | Validation Loss:  0.0016202048864215612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.0014056251384317875 | Validation Loss:  0.0016202708939090371\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.001405528630129993 | Validation Loss:  0.0016203366685658693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.0014054332859814167 | Validation Loss:  0.0016204029088839889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.0014053393388167024 | Validation Loss:  0.0016204685671254992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.00140524678863585 | Validation Loss:  0.0016205342253670096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.0014051554026082158 | Validation Loss:  0.0016206008149310946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.0014050655299797654 | Validation Loss:  0.0016206662403419614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.0014049768215045333 | Validation Loss:  0.001620732364244759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.001404889510013163 | Validation Loss:  0.0016207984881475568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.0014048033626750112 | Validation Loss:  0.0016208641463890672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.0014047182630747557 | Validation Loss:  0.001620930153876543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.001404634676873684 | Validation Loss:  0.0016209962777793407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.0014045523712411523 | Validation Loss:  0.0016210617031902075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.001404471229761839 | Validation Loss:  0.001621127943508327\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.0014043913688510656 | Validation Loss:  0.0016211936017498374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.0014043126720935106 | Validation Loss:  0.001621259725652635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.001404235023073852 | Validation Loss:  0.0016213255003094673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.0014041588874533772 | Validation Loss:  0.001621390925720334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.0014040837995707989 | Validation Loss:  0.0016214567003771663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.001404009759426117 | Validation Loss:  0.0016215225914493203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.001403937116265297 | Validation Loss:  0.0016215877840295434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.0014038654044270515 | Validation Loss:  0.0016216535586863756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.0014037948567420244 | Validation Loss:  0.0016217191005125642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.0014037253567948937 | Validation Loss:  0.0016217841766774654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.0014036571374163032 | Validation Loss:  0.0016218499513342977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.0014035898493602872 | Validation Loss:  0.0016219152603298426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.0014035239582881331 | Validation Loss:  0.0016219799872487783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.00140345876570791 | Validation Loss:  0.0016220456454902887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.0014033947372809052 | Validation Loss:  0.0016221104888245463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.0014033317565917969 | Validation Loss:  0.0016221749829128385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.001403269823640585 | Validation Loss:  0.001622240524739027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.0014032090548425913 | Validation Loss:  0.0016223047859966755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.0014031491009518504 | Validation Loss:  0.0016223693965002894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.001403090194799006 | Validation Loss:  0.0016224343562498689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.0014030324527993798 | Validation Loss:  0.00162249815184623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.0014029755257070065 | Validation Loss:  0.001622562762349844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.0014029196463525295 | Validation Loss:  0.0016226270236074924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.001402864814735949 | Validation Loss:  0.0016226908192038536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.0014028105651959777 | Validation Loss:  0.0016227549640461802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.0014027574798092246 | Validation Loss:  0.001622818410396576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.0014027052093297243 | Validation Loss:  0.0016228819731622934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.0014026541030034423 | Validation Loss:  0.001622945535928011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.0014026036951690912 | Validation Loss:  0.0016230084002017975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.0014025542186573148 | Validation Loss:  0.0016230716137215495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.001402505673468113 | Validation Loss:  0.0016231348272413015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.001402457943186164 | Validation Loss:  0.0016231973422691226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.0014024111442267895 | Validation Loss:  0.0016232598572969437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.0014023652765899897 | Validation Loss:  0.0016233223723247647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.001402319991029799 | Validation Loss:  0.0016233845381066203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.001402275636792183 | Validation Loss:  0.0016234463546425104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.0014022319810464978 | Validation Loss:  0.0016235080547630787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.0014021890237927437 | Validation Loss:  0.001623569754883647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.0014021472306922078 | Validation Loss:  0.001623630989342928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.001402106019668281 | Validation Loss:  0.0016236919909715652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.0014020655071362853 | Validation Loss:  0.0016237529926002026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.0014020258095115423 | Validation Loss:  0.0016238136449828744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.0014019869267940521 | Validation Loss:  0.0016238740645349026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.001401948626153171 | Validation Loss:  0.001623934251256287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.0014019112568348646 | Validation Loss:  0.001623994205147028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.0014018743531778455 | Validation Loss:  0.0016240538097918034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.001401838380843401 | Validation Loss:  0.0016241134144365788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.0014018029905855656 | Validation Loss:  0.0016241726698353887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.0014017682988196611 | Validation Loss:  0.0016242313431575894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.0014017343055456877 | Validation Loss:  0.001624290132895112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.0014017007779330015 | Validation Loss:  0.0016243485733866692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.0014016680652275681 | Validation Loss:  0.0016244064318016171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.001401635934598744 | Validation Loss:  0.0016244642902165651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.0014016043860465288 | Validation Loss:  0.0016245216829702258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.0014015736524015665 | Validation Loss:  0.0016245791921392083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.0014015432680025697 | Validation Loss:  0.0016246357699856162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.0014015136985108256 | Validation Loss:  0.0016246928134933114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.001401484594680369 | Validation Loss:  0.0016247488092631102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.0014014561893418431 | Validation Loss:  0.0016248052706941962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.0014014282496646047 | Validation Loss:  0.0016248608008027077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.0014014008920639753 | Validation Loss:  0.0016249158652499318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.0014013740001246333 | Validation Loss:  0.0016249711625277996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.0014013478066772223 | Validation Loss:  0.0016250258777290583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.0014013220788910985 | Validation Loss:  0.0016250802436843514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.001401296816766262 | Validation Loss:  0.0016251347260549664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.0014012721367180347 | Validation Loss:  0.001625188160687685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.0014012480387464166 | Validation Loss:  0.0016252420609816909\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.001401224173605442 | Validation Loss:  0.0016252946807071567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.0014012010069563985 | Validation Loss:  0.0016253481153398752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.0014011783059686422 | Validation Loss:  0.0016253999201580882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.0014011559542268515 | Validation Loss:  0.0016254523070529103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.0014011343009769917 | Validation Loss:  0.0016255038790404797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.0014011129969730973 | Validation Loss:  0.0016255555674433708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.0014010921586304903 | Validation Loss:  0.0016256064409390092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.001401071553118527 | Validation Loss:  0.001625657081604004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.0014010515296831727 | Validation Loss:  0.0016257076058536768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.001401031855493784 | Validation Loss:  0.001625757198780775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.0014010126469656825 | Validation Loss:  0.0016258073737844825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.0014009940205141902 | Validation Loss:  0.0016258560353890061\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.0014009756268933415 | Validation Loss:  0.001625905279070139\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.0014009574661031365 | Validation Loss:  0.0016259539406746626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.0014009400038048625 | Validation Loss:  0.0016260017873719335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.0014009226579219103 | Validation Loss:  0.0016260497504845262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.0014009057777002454 | Validation Loss:  0.0016260967822745442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.001400889246724546 | Validation Loss:  0.0016261445125564933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.0014008731814101338 | Validation Loss:  0.001626190380193293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.0014008572325110435 | Validation Loss:  0.0016262372955679893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.0014008417492732406 | Validation Loss:  0.0016262830467894673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.001400826615281403 | Validation Loss:  0.0016263282159343362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.0014008117141202092 | Validation Loss:  0.0016263738507404923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.0014007972786203027 | Validation Loss:  0.001626417855732143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.0014007830759510398 | Validation Loss:  0.0016264633741229773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.0014007692225277424 | Validation Loss:  0.001626505982130766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.0014007557183504105 | Validation Loss:  0.0016265512676909566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.0014007424470037222 | Validation Loss:  0.0016265923622995615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.0014007292920723557 | Validation Loss:  0.0016266375314444304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.0014007167192175984 | Validation Loss:  0.0016266776947304606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.001400704262778163 | Validation Loss:  0.0016267220489680767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.001400692155584693 | Validation Loss:  0.0016267617465928197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.0014006802812218666 | Validation Loss:  0.0016268048202618957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.0014006687561050057 | Validation Loss:  0.0016268444014713168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.0014006572309881449 | Validation Loss:  0.001626885961741209\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.0014006461715325713 | Validation Loss:  0.0016269250772893429\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.0014006352284923196 | Validation Loss:  0.0016269658226519823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.0014006246346980333 | Validation Loss:  0.0016270044725388288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.001400614157319069 | Validation Loss:  0.0016270439373329282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.0014006039127707481 | Validation Loss:  0.001627082470804453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.0014005940174683928 | Validation Loss:  0.001627120771445334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.0014005843549966812 | Validation Loss:  0.0016271588392555714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.001400574459694326 | Validation Loss:  0.0016271955100819468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.0014005653792992234 | Validation Loss:  0.001627233810722828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.0014005560660734773 | Validation Loss:  0.0016272689681500196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.0014005472185090184 | Validation Loss:  0.0016273069195449352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.0014005384873598814 | Validation Loss:  0.0016273407964035869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.001400529989041388 | Validation Loss:  0.0016273792134597898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.0014005216071382165 | Validation Loss:  0.0016274109948426485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.0014005134580656886 | Validation Loss:  0.0016274499939754605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.0014005055418238044 | Validation Loss:  0.001627479214221239\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.0014004976255819201 | Validation Loss:  0.0016275200759992003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.0014004898257553577 | Validation Loss:  0.0016275449888780713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.0014004824915900826 | Validation Loss:  0.0016275896923616529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.0014004750410094857 | Validation Loss:  0.0016276080859825015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.0014004679396748543 | Validation Loss:  0.0016276601236313581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.0014004608383402228 | Validation Loss:  0.0016276660608127713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.001400453969836235 | Validation Loss:  0.001627733581699431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.001400447334162891 | Validation Loss:  0.0016277162358164787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.0014004408149048686 | Validation Loss:  0.0016278144903481007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.0014004344120621681 | Validation Loss:  0.001627752324566245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.0014004282420501113 | Validation Loss:  0.001627913792617619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.0014004223048686981 | Validation Loss:  0.0016277587274089456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.0014004164841026068 | Validation Loss:  0.0016280536074191332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.0014004113618284464 | Validation Loss:  0.001627704594284296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.0014004070544615388 | Validation Loss:  0.0016282869037240744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.001400404842570424 | Validation Loss:  0.0016275213565677404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.0014004063559696078 | Validation Loss:  0.0016287406906485558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.0014004167169332504 | Validation Loss:  0.0016270666383206844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.0014004461700096726 | Validation Loss:  0.0016297521069645882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.0014005189295858145 | Validation Loss:  0.0016260964330285788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.001400690758600831 | Validation Loss:  0.0016323173185810447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.0014010888990014791 | Validation Loss:  0.001624524244107306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.0014020033413544297 | Validation Loss:  0.001639711786992848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.001404055510647595 | Validation Loss:  0.001624549156986177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.0014084299327805638 | Validation Loss:  0.0016607646830379963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.001416561659425497 | Validation Loss:  0.0016339364228770137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.0014278644230216742 | Validation Loss:  0.0016859903698787093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.0014345254749059677 | Validation Loss:  0.0016326323384419084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.0014260245952755213 | Validation Loss:  0.0016459316248074174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.0014074892969802022 | Validation Loss:  0.00163009122479707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.0014005966950207949 | Validation Loss:  0.0016246436862275004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.0014103433350101113 | Validation Loss:  0.0016628435114398599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.0014182549202814698 | Validation Loss:  0.0016248781466856599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.001411234145052731 | Validation Loss:  0.0016318862326443195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.001401091692969203 | Validation Loss:  0.0016377657884731889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.0014034104533493519 | Validation Loss:  0.0016245495062321424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.0014107893221080303 | Validation Loss:  0.0016475573647767305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.0014086969895288348 | Validation Loss:  0.0016240852419286966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.0014014338376000524 | Validation Loss:  0.0016237655654549599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.001401726040057838 | Validation Loss:  0.001644046395085752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.0014067984884604812 | Validation Loss:  0.001622971729375422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.0014059569220989943 | Validation Loss:  0.00163154280744493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.0014011181192472577 | Validation Loss:  0.0016319098649546504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.0014012318570166826 | Validation Loss:  0.001622864045202732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.0014046060387045145 | Validation Loss:  0.001638519810512662\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.0014039353700354695 | Validation Loss:  0.0016248526517301798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.0014007645659148693 | Validation Loss:  0.0016244364669546485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.0014010071754455566 | Validation Loss:  0.0016370204975828528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.001403208589181304 | Validation Loss:  0.00162323541007936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.0014026081189513206 | Validation Loss:  0.0016293739899992943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.001400540815666318 | Validation Loss:  0.0016307206824421883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.001400841516442597 | Validation Loss:  0.0016234108479693532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.0014022696996107697 | Validation Loss:  0.00163364689797163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.0014017613139003515 | Validation Loss:  0.0016259734984487295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.0014004209078848362 | Validation Loss:  0.0016251427587121725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.0014006884302943945 | Validation Loss:  0.0016333911335095763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.0014016125351190567 | Validation Loss:  0.001624364871531725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.0014012412866577506 | Validation Loss:  0.0016286619938910007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.0014003620017319918 | Validation Loss:  0.0016298829577863216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.001400547451339662 | Validation Loss:  0.001624595490284264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.0014011573512107134 | Validation Loss:  0.001631527324207127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.0014009197475388646 | Validation Loss:  0.0016267176251858473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.0014003330143168569 | Validation Loss:  0.001626217388547957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.0014004288241267204 | Validation Loss:  0.0016313866944983602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.0014008384896442294 | Validation Loss:  0.0016254662768915296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.001400717068463564 | Validation Loss:  0.0016288465121760964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.0014003190444782376 | Validation Loss:  0.001629099017009139\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.0014003387186676264 | Validation Loss:  0.0016258226241916418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.0014006156707182527 | Validation Loss:  0.0016306368634104729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.0014005802804604173 | Validation Loss:  0.0016270633786916733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.0014003129908815026 | Validation Loss:  0.0016273964429274201\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.00140027713496238 | Validation Loss:  0.0016302086878567934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.0014004578115418553 | Validation Loss:  0.0016263892175629735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.0014004800468683243 | Validation Loss:  0.00162934057880193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.0014003094984218478 | Validation Loss:  0.0016285133315250278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.0014002415118739009 | Validation Loss:  0.0016269993502646685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.0014003467513248324 | Validation Loss:  0.0016301418654620647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.0014003990218043327 | Validation Loss:  0.0016272872453555465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.0014003043761476874 | Validation Loss:  0.0016284576850011945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.0014002263778820634 | Validation Loss:  0.0016293618828058243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.001400271779857576 | Validation Loss:  0.0016272090142592788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.0014003298711031675 | Validation Loss:  0.001629676902666688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.001400292618200183 | Validation Loss:  0.0016281522111967206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.0014002242824062705 | Validation Loss:  0.0016280997078865767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.0014002263778820634 | Validation Loss:  0.0016296483809128404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.0014002714306116104 | Validation Loss:  0.0016276384703814983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.0014002722455188632 | Validation Loss:  0.0016292394138872623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.0014002264942973852 | Validation Loss:  0.001628745929338038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.0014002041425555944 | Validation Loss:  0.0016280639683827758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.001400226610712707 | Validation Loss:  0.0016296278918161988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.001400244189426303 | Validation Loss:  0.0016280843410640955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.0014002247480675578 | Validation Loss:  0.001628988073207438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.0014001973904669285 | Validation Loss:  0.0016290920320898294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.0014001973904669285 | Validation Loss:  0.0016282075084745884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.001400213921442628 | Validation Loss:  0.001629529637284577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.001400214503519237 | Validation Loss:  0.0016284530283883214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.0014001961098983884 | Validation Loss:  0.00162889517378062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.001400183653458953 | Validation Loss:  0.0016292543150484562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.0014001887757331133 | Validation Loss:  0.001628396799787879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.0014001966919749975 | Validation Loss:  0.0016294346423819661\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.0014001913368701935 | Validation Loss:  0.001628710888326168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.001400178880430758 | Validation Loss:  0.0016289014602079988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.0014001734089106321 | Validation Loss:  0.001629321021027863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.001400177483446896 | Validation Loss:  0.001628588535822928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.0014001800445839763 | Validation Loss:  0.001629386912100017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.0014001746894791722 | Validation Loss:  0.0016288880724459887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.0014001666568219662 | Validation Loss:  0.0016289681661874056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.0014001637464389205 | Validation Loss:  0.0016293458174914122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.0014001657254993916 | Validation Loss:  0.001628754660487175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.0014001661911606789 | Validation Loss:  0.0016293720109388232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.0014001618837937713 | Validation Loss:  0.0016289990162476897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.0014001565286889672 | Validation Loss:  0.0016290537314489484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.0014001540839672089 | Validation Loss:  0.0016293496591970325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.0014001547824591398 | Validation Loss:  0.00162889261264354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.0014001542003825307 | Validation Loss:  0.0016293814405798912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.0014001511735841632 | Validation Loss:  0.0016290751518681645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.0014001470990478992 | Validation Loss:  0.0016291484935209155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.0014001447707414627 | Validation Loss:  0.0016293501248583198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.0014001443050801754 | Validation Loss:  0.0016290111234411597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.0014001436065882444 | Validation Loss:  0.0016294013475999236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.0014001413946971297 | Validation Loss:  0.0016291289357468486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.0014001381350681186 | Validation Loss:  0.0016292377840727568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.0014001360395923257 | Validation Loss:  0.0016293441876769066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.0014001348754391074 | Validation Loss:  0.0016291127540171146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.001400133827701211 | Validation Loss:  0.0016294191591441631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.0014001321978867054 | Validation Loss:  0.0016291732899844646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.0014001298695802689 | Validation Loss:  0.0016293193912133574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.0014001274248585105 | Validation Loss:  0.0016293383669108152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.0014001259114593267 | Validation Loss:  0.0016292072832584381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.0014001248637214303 | Validation Loss:  0.0016294316155835986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.0014001234667375684 | Validation Loss:  0.0016292182262986898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.0014001216040924191 | Validation Loss:  0.0016293867956846952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.0014001193922013044 | Validation Loss:  0.0016293326625600457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.0014001175295561552 | Validation Loss:  0.0016292937798425555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.001400116365402937 | Validation Loss:  0.0016294310335069895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.0014001148520037532 | Validation Loss:  0.0016292684013023973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.0014001133386045694 | Validation Loss:  0.0016294346423819661\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.001400111592374742 | Validation Loss:  0.0016293334774672985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.001400109613314271 | Validation Loss:  0.0016293724766001105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.001400108216330409 | Validation Loss:  0.001629422651603818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.0014001067029312253 | Validation Loss:  0.0016293275402858853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.0014001053059473634 | Validation Loss:  0.001629460952244699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.0014001037925481796 | Validation Loss:  0.0016293474473059177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.0014001022791489959 | Validation Loss:  0.001629435340873897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.0014001004165038466 | Validation Loss:  0.0016294110100716352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.001400098903104663 | Validation Loss:  0.0016293911030516028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.0014000973897054791 | Validation Loss:  0.0016294652596116066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.0014000958763062954 | Validation Loss:  0.00162937818095088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.0014000943629071116 | Validation Loss:  0.0016294752713292837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.0014000929659232497 | Validation Loss:  0.0016294089145958424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.0014000913361087441 | Validation Loss:  0.0016294503584504128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.0014000898227095604 | Validation Loss:  0.0016294573433697224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.0014000884257256985 | Validation Loss:  0.0016294247470796108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.001400086795911193 | Validation Loss:  0.0016294895904138684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.0014000852825120091 | Validation Loss:  0.0016294247470796108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.001400084001943469 | Validation Loss:  0.001629491918720305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.0014000824885442853 | Validation Loss:  0.0016294511733576655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.0014000810915604234 | Validation Loss:  0.001629474340006709\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.0014000795781612396 | Validation Loss:  0.001629485166631639\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.001400077948346734 | Validation Loss:  0.001629459555260837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.0014000765513628721 | Validation Loss:  0.0016295075183734298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.0014000750379636884 | Validation Loss:  0.00162946165073663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.0014000737573951483 | Validation Loss:  0.001629510778002441\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.0014000723604112864 | Validation Loss:  0.0016294800443574786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.0014000707305967808 | Validation Loss:  0.0016295011155307293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.001400069217197597 | Validation Loss:  0.0016295043751597404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.001400067936629057 | Validation Loss:  0.0016294915694743395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.0014000664232298732 | Validation Loss:  0.0016295219538733363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.0014000650262460113 | Validation Loss:  0.0016294916858896613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.0014000636292621493 | Validation Loss:  0.0016295281238853931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.0014000623486936092 | Validation Loss:  0.0016295023960992694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.0014000607188791037 | Validation Loss:  0.0016295253299176693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.0014000594383105636 | Validation Loss:  0.0016295183449983597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.001400057808496058 | Validation Loss:  0.0016295198583975434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.001400056411512196 | Validation Loss:  0.0016295332461595535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.001400055130943656 | Validation Loss:  0.001629517413675785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.001400053733959794 | Validation Loss:  0.0016295422101393342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.001400052453391254 | Validation Loss:  0.0016295213717967272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.0014000508235767484 | Validation Loss:  0.0016295451205223799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.0014000494265928864 | Validation Loss:  0.0016295299865305424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.0014000480296090245 | Validation Loss:  0.0016295436071231961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.0014000467490404844 | Validation Loss:  0.0016295408131554723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.0014000453520566225 | Validation Loss:  0.0016295413952320814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.0014000440714880824 | Validation Loss:  0.0016295508248731494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.0014000424416735768 | Validation Loss:  0.001629541045986116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.001400041044689715 | Validation Loss:  0.001629557111300528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.0014000398805364966 | Validation Loss:  0.0016295432578772306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.0014000383671373129 | Validation Loss:  0.0016295603709295392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.001400036970153451 | Validation Loss:  0.0016295486129820347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.001400035573169589 | Validation Loss:  0.0016295610694214702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.0014000341761857271 | Validation Loss:  0.001629555132240057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.0014000327792018652 | Validation Loss:  0.0016295603709295392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.001400031498633325 | Validation Loss:  0.0016295617679134011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.0014000301016494632 | Validation Loss:  0.0016295603709295392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.0014000287046656013 | Validation Loss:  0.001629567239433527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.0014000273076817393 | Validation Loss:  0.0016295607201755047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.0014000260271131992 | Validation Loss:  0.0016295714303851128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.0014000246301293373 | Validation Loss:  0.0016295621171593666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.0014000232331454754 | Validation Loss:  0.0016295741079375148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.0014000218361616135 | Validation Loss:  0.0016295646782964468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.0014000205555930734 | Validation Loss:  0.0016295757377520204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.0014000190421938896 | Validation Loss:  0.0016295678215101361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.0014000178780406713 | Validation Loss:  0.0016295762034133077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.0014000164810568094 | Validation Loss:  0.0016295715468004346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.0014000150840729475 | Validation Loss:  0.0016295762034133077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.0014000136870890856 | Validation Loss:  0.0016295750392600894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.0014000122901052237 | Validation Loss:  0.0016295764362439513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.0014000108931213617 | Validation Loss:  0.001629577949643135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.0014000094961374998 | Validation Loss:  0.0016295765526592731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.0014000083319842815 | Validation Loss:  0.001629580627195537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.0014000069350004196 | Validation Loss:  0.0016295765526592731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.0014000055380165577 | Validation Loss:  0.0016295830719172955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.0014000042574480176 | Validation Loss:  0.0016295767854899168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.0014000027440488338 | Validation Loss:  0.0016295851673930883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.0014000014634802938 | Validation Loss:  0.0016295767854899168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.00139999995008111 | Validation Loss:  0.0016295869136229157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.0013999984366819263 | Validation Loss:  0.0016295765526592731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.0013999973889440298 | Validation Loss:  0.001629588776268065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.001399995875544846 | Validation Loss:  0.0016295762034133077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.001399994594976306 | Validation Loss:  0.0016295902896672487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.001399993197992444 | Validation Loss:  0.0016295751556754112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.001399991917423904 | Validation Loss:  0.0016295925015583634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.0013999904040247202 | Validation Loss:  0.0016295736422762275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.00139998912345618 | Validation Loss:  0.0016295954119414091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.0013999877264723182 | Validation Loss:  0.0016295707318931818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.0013999865623191 | Validation Loss:  0.0016295996028929949\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.001399985165335238 | Validation Loss:  0.001629565958864987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.0013999836519360542 | Validation Loss:  0.0016296054236590862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.001399982487782836 | Validation Loss:  0.0016295583918690681\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.0013999809743836522 | Validation Loss:  0.0016296147368848324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.0013999796938151121 | Validation Loss:  0.0016295466339215636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.001399978413246572 | Validation Loss:  0.001629629172384739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.00139997701626271 | Validation Loss:  0.0016295278910547495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.0013999758521094918 | Validation Loss:  0.0016296523390337825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.0013999745715409517 | Validation Loss:  0.0016294977394863963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.0013999732909724116 | Validation Loss:  0.001629690988920629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.0013999721268191934 | Validation Loss:  0.0016294476808980107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.0013999711954966187 | Validation Loss:  0.001629756879992783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.0013999704970046878 | Validation Loss:  0.0016293630469590425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.001399970380589366 | Validation Loss:  0.0016298714326694608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.001399971079081297 | Validation Loss:  0.0016292191576212645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.0013999729417264462 | Validation Loss:  0.0016300767892971635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.0013999774819239974 | Validation Loss:  0.0016289710765704513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.0013999862130731344 | Validation Loss:  0.0016304553719237447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.0014000028604641557 | Validation Loss:  0.0016285455785691738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.0014000334776937962 | Validation Loss:  0.001631182967685163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.0014000895898789167 | Validation Loss:  0.001627835095860064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.0014001931995153427 | Validation Loss:  0.0016326585318893194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.0014003836549818516 | Validation Loss:  0.0016267556929960847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.0014007363934069872 | Validation Loss:  0.001635864027775824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.0014013898326084018 | Validation Loss:  0.001625579665414989\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.0014026014832779765 | Validation Loss:  0.0016433346318081021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.0014048138400539756 | Validation Loss:  0.0016262184362858534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.001408770214766264 | Validation Loss:  0.001660763518884778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.0014153774827718735 | Validation Loss:  0.00163412990514189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.0014253087574616075 | Validation Loss:  0.00169047259259969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.0014367196708917618 | Validation Loss:  0.0016458280151709914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.0014439256628975272 | Validation Loss:  0.0016917326720431447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.0014379163039848208 | Validation Loss:  0.0016301440773531795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.0014195266412571073 | Validation Loss:  0.0016381802270188928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.0014027622528374195 | Validation Loss:  0.0016352484235540032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.001401577959768474 | Validation Loss:  0.0016264219302684069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.001412597601301968 | Validation Loss:  0.0016668179305270314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.0014202826423570514 | Validation Loss:  0.0016273404471576214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.0014150753850117326 | Validation Loss:  0.001639680820517242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.0014037310611456633 | Validation Loss:  0.001630256650969386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.0014002275420352817 | Validation Loss:  0.0016237682430073619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.0014064277056604624 | Validation Loss:  0.001652852282859385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.001411480363458395 | Validation Loss:  0.0016238588141277432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.0014080270193517208 | Validation Loss:  0.001633619423955679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.001401386340148747 | Validation Loss:  0.0016310324426740408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.0014005640987306833 | Validation Loss:  0.001623076037503779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.0014049358433112502 | Validation Loss:  0.001644689473323524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.001406799303367734 | Validation Loss:  0.0016230735927820206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.0014034596970304847 | Validation Loss:  0.00162905128672719\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.001400125096552074 | Validation Loss:  0.001632813597097993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.0014011344173923135 | Validation Loss:  0.0016230355249717832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.001403828733600676 | Validation Loss:  0.0016387165524065495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.0014036560896784067 | Validation Loss:  0.0016239631222561002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.0014010854065418243 | Validation Loss:  0.0016265157610177994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.001399980392307043 | Validation Loss:  0.0016335302498191595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.0014014006592333317 | Validation Loss:  0.0016231926856562495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.0014025918208062649 | Validation Loss:  0.0016342996386811137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.0014016658533364534 | Validation Loss:  0.0016255942173302174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.0014001657254993916 | Validation Loss:  0.001625606557354331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.001400178181938827 | Validation Loss:  0.0016333734383806586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.00140124571043998 | Validation Loss:  0.0016239192336797714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.0014015132328495383 | Validation Loss:  0.0016315901884809136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.0014006245182827115 | Validation Loss:  0.0016271566273644567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.0013999431394040585 | Validation Loss:  0.0016255215741693974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.0014002860989421606 | Validation Loss:  0.0016325874021276832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.001400888548232615 | Validation Loss:  0.0016247587045654655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.0014007820282131433 | Validation Loss:  0.0016300083370879292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.0014001727104187012 | Validation Loss:  0.001628145226277411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.0013999294023960829 | Validation Loss:  0.0016258154064416885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.0014002558309584856 | Validation Loss:  0.0016317605040967464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.0014005457051098347 | Validation Loss:  0.0016256823437288404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.0014003661926835775 | Validation Loss:  0.0016293302178382874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.001400002627633512 | Validation Loss:  0.0016287300968542695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.0013999354559928179 | Validation Loss:  0.0016263312427327037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.0014001616509631276 | Validation Loss:  0.001631088089197874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.001400295877829194 | Validation Loss:  0.001626431243494153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.001400151289999485 | Validation Loss:  0.0016290658386424184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.0013999416260048747 | Validation Loss:  0.0016289197374135256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.001399925327859819 | Validation Loss:  0.0016268357867375016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.001400065142661333 | Validation Loss:  0.001630576211027801\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.0014001344097778201 | Validation Loss:  0.0016270065680146217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.0014000418595969677 | Validation Loss:  0.001629080856218934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.0013999174116179347 | Validation Loss:  0.0016289710765704513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.0013999065849930048 | Validation Loss:  0.0016273671062663198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.0013999879593029618 | Validation Loss:  0.0016302397707477212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.0014000326627865434 | Validation Loss:  0.0016274380031973124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.0013999829534441233 | Validation Loss:  0.0016291948268190026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.0013999058865010738 | Validation Loss:  0.0016289002960547805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.0013998894719406962 | Validation Loss:  0.001627816236577928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.0013999327784404159 | Validation Loss:  0.0016299547860398889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.0013999661896377802 | Validation Loss:  0.0016277451068162918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.0013999459333717823 | Validation Loss:  0.0016293355729430914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.0013998987851664424 | Validation Loss:  0.0016288093756884336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.0013998777139931917 | Validation Loss:  0.0016282432479783893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.0013998958747833967 | Validation Loss:  0.0016297361580654979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.0013999203220009804 | Validation Loss:  0.0016280177515000105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.0013999183429405093 | Validation Loss:  0.0016294645611196756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.001399892382323742 | Validation Loss:  0.0016287167090922594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.0013998713111504912 | Validation Loss:  0.0016286094905808568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.0013998731737956405 | Validation Loss:  0.0016295108944177628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.0013998879585415125 | Validation Loss:  0.0016282539581879973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.0013998942449688911 | Validation Loss:  0.001629526261240244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.0013998840004205704 | Validation Loss:  0.0016286495374515653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.00139986805152148 | Validation Loss:  0.001628930913284421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.0013998610666021705 | Validation Loss:  0.0016293058870360255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.0013998659560456872 | Validation Loss:  0.0016285112360492349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.0013998727081343532 | Validation Loss:  0.0016295256791636348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.0013998723588883877 | Validation Loss:  0.0016286417376250029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.001399864093400538 | Validation Loss:  0.00162918318528682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.0013998552458360791 | Validation Loss:  0.001629114500246942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.001399852684698999 | Validation Loss:  0.0016287665348500013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.0013998555950820446 | Validation Loss:  0.0016294384840875864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.0013998581562191248 | Validation Loss:  0.001628694124519825\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Test Loss:  0.0012132583651691675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  0 | Train Loss:  0.11388465762138367 | Validation Loss:  0.10682743042707443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  1 | Train Loss:  0.11268460005521774 | Validation Loss:  0.10566382855176926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  2 | Train Loss:  0.1114991307258606 | Validation Loss:  0.104514479637146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  3 | Train Loss:  0.11032804846763611 | Validation Loss:  0.10337921977043152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  4 | Train Loss:  0.10917117446660995 | Validation Loss:  0.1022578626871109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  5 | Train Loss:  0.1080283671617508 | Validation Loss:  0.10115022957324982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  6 | Train Loss:  0.10689938068389893 | Validation Loss:  0.10005615651607513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  7 | Train Loss:  0.1057840958237648 | Validation Loss:  0.09897546470165253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  8 | Train Loss:  0.10468233376741409 | Validation Loss:  0.09790799021720886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  9 | Train Loss:  0.10359393060207367 | Validation Loss:  0.09685356169939041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  10 | Train Loss:  0.10251867026090622 | Validation Loss:  0.09581202268600464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  11 | Train Loss:  0.10145644843578339 | Validation Loss:  0.09478320926427841\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  12 | Train Loss:  0.10040707141160965 | Validation Loss:  0.0937669649720192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  13 | Train Loss:  0.09937039017677307 | Validation Loss:  0.09276310354471207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  14 | Train Loss:  0.09834624081850052 | Validation Loss:  0.09177152067422867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  15 | Train Loss:  0.09733445197343826 | Validation Loss:  0.09079201519489288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  16 | Train Loss:  0.09633490443229675 | Validation Loss:  0.08982447534799576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  17 | Train Loss:  0.09534742683172226 | Validation Loss:  0.08886874467134476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  18 | Train Loss:  0.09437186270952225 | Validation Loss:  0.08792465925216675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  19 | Train Loss:  0.09340808540582657 | Validation Loss:  0.08699208498001099\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  20 | Train Loss:  0.09245593845844269 | Validation Loss:  0.08607087284326553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  21 | Train Loss:  0.09151526540517807 | Validation Loss:  0.08516088873147964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  22 | Train Loss:  0.09058595448732376 | Validation Loss:  0.08426199853420258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  23 | Train Loss:  0.08966783434152603 | Validation Loss:  0.08337405323982239\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  24 | Train Loss:  0.08876078575849533 | Validation Loss:  0.08249692618846893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  25 | Train Loss:  0.08786466717720032 | Validation Loss:  0.08163046836853027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  26 | Train Loss:  0.08697935193777084 | Validation Loss:  0.08077456802129745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  27 | Train Loss:  0.08610467612743378 | Validation Loss:  0.07992907613515854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  28 | Train Loss:  0.08524055033922195 | Validation Loss:  0.07909388095140457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  29 | Train Loss:  0.08438682556152344 | Validation Loss:  0.07826884835958481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  30 | Train Loss:  0.08354338258504868 | Validation Loss:  0.0774538516998291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  31 | Train Loss:  0.08271007984876633 | Validation Loss:  0.0766487866640091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  32 | Train Loss:  0.08188681304454803 | Validation Loss:  0.07585349678993225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  33 | Train Loss:  0.08107344061136246 | Validation Loss:  0.07506789267063141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  34 | Train Loss:  0.08026985824108124 | Validation Loss:  0.07429183274507523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  35 | Train Loss:  0.07947594672441483 | Validation Loss:  0.07352522015571594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  36 | Train Loss:  0.07869157195091248 | Validation Loss:  0.07276792824268341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  37 | Train Loss:  0.07791662216186523 | Validation Loss:  0.07201985269784927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  38 | Train Loss:  0.07715100049972534 | Validation Loss:  0.07128085941076279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  39 | Train Loss:  0.07639457285404205 | Validation Loss:  0.07055085897445679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  40 | Train Loss:  0.07564723491668701 | Validation Loss:  0.06982972472906113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  41 | Train Loss:  0.07490886747837067 | Validation Loss:  0.06911736726760864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  42 | Train Loss:  0.07417939603328705 | Validation Loss:  0.06841366738080978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  43 | Train Loss:  0.07345867156982422 | Validation Loss:  0.0677185133099556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  44 | Train Loss:  0.07274659723043442 | Validation Loss:  0.06703182309865952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  45 | Train Loss:  0.07204308360815048 | Validation Loss:  0.06635347753763199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  46 | Train Loss:  0.07134801894426346 | Validation Loss:  0.06568337231874466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  47 | Train Loss:  0.07066129893064499 | Validation Loss:  0.06502141058444977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  48 | Train Loss:  0.06998282670974731 | Validation Loss:  0.06436750292778015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  49 | Train Loss:  0.06931250542402267 | Validation Loss:  0.06372153759002686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  50 | Train Loss:  0.06865021586418152 | Validation Loss:  0.06308342516422272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  51 | Train Loss:  0.06799589097499847 | Validation Loss:  0.06245307996869087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  52 | Train Loss:  0.0673494040966034 | Validation Loss:  0.06183038279414177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  53 | Train Loss:  0.0667106956243515 | Validation Loss:  0.06121526658535004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  54 | Train Loss:  0.06607965379953384 | Validation Loss:  0.06060762330889702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  55 | Train Loss:  0.06545616686344147 | Validation Loss:  0.06000736355781555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  56 | Train Loss:  0.0648401752114296 | Validation Loss:  0.059414397925138474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  57 | Train Loss:  0.06423157453536987 | Validation Loss:  0.05882865563035011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  58 | Train Loss:  0.06363027542829514 | Validation Loss:  0.05825003609061241\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  59 | Train Loss:  0.06303619593381882 | Validation Loss:  0.05767844617366791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  60 | Train Loss:  0.062449246644973755 | Validation Loss:  0.05711381137371063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  61 | Train Loss:  0.061869341880083084 | Validation Loss:  0.05655604600906372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  62 | Train Loss:  0.06129638850688934 | Validation Loss:  0.05600506439805031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  63 | Train Loss:  0.06073031947016716 | Validation Loss:  0.05546077713370323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  64 | Train Loss:  0.060171034187078476 | Validation Loss:  0.054923124611377716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  65 | Train Loss:  0.05961846560239792 | Validation Loss:  0.054392009973526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  66 | Train Loss:  0.059072528034448624 | Validation Loss:  0.05386735126376152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  67 | Train Loss:  0.05853314697742462 | Validation Loss:  0.0533490814268589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  68 | Train Loss:  0.05800022929906845 | Validation Loss:  0.05283711850643158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  69 | Train Loss:  0.05747371166944504 | Validation Loss:  0.05233139172196388\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  70 | Train Loss:  0.05695351958274841 | Validation Loss:  0.051831815391778946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  71 | Train Loss:  0.056439559906721115 | Validation Loss:  0.05133833363652229\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  72 | Train Loss:  0.05593177303671837 | Validation Loss:  0.050850849598646164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  73 | Train Loss:  0.05543007701635361 | Validation Loss:  0.05036930739879608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  74 | Train Loss:  0.054934412240982056 | Validation Loss:  0.04989362880587578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  75 | Train Loss:  0.05444468930363655 | Validation Loss:  0.04942374676465988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  76 | Train Loss:  0.053960852324962616 | Validation Loss:  0.048959583044052124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  77 | Train Loss:  0.053482815623283386 | Validation Loss:  0.04850108176469803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  78 | Train Loss:  0.05301051586866379 | Validation Loss:  0.048048168420791626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  79 | Train Loss:  0.052543897181749344 | Validation Loss:  0.047600775957107544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  80 | Train Loss:  0.05208287760615349 | Validation Loss:  0.04715883731842041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  81 | Train Loss:  0.05162738636136055 | Validation Loss:  0.04672228544950485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  82 | Train Loss:  0.05117737874388695 | Validation Loss:  0.0462910532951355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  83 | Train Loss:  0.05073276907205582 | Validation Loss:  0.04586508870124817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  84 | Train Loss:  0.05029349401593208 | Validation Loss:  0.045444317162036896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  85 | Train Loss:  0.04985949769616127 | Validation Loss:  0.0450286827981472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  86 | Train Loss:  0.0494307242333889 | Validation Loss:  0.044618118554353714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  87 | Train Loss:  0.04900708794593811 | Validation Loss:  0.04421256482601166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  88 | Train Loss:  0.048588547855615616 | Validation Loss:  0.043811969459056854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  89 | Train Loss:  0.048175036907196045 | Validation Loss:  0.043416257947683334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  90 | Train Loss:  0.047766491770744324 | Validation Loss:  0.043025389313697815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  91 | Train Loss:  0.047362860292196274 | Validation Loss:  0.04263928532600403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  92 | Train Loss:  0.04696408286690712 | Validation Loss:  0.04225790128111839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  93 | Train Loss:  0.04657009243965149 | Validation Loss:  0.04188118502497673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  94 | Train Loss:  0.046180836856365204 | Validation Loss:  0.04150906577706337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  95 | Train Loss:  0.045796263962984085 | Validation Loss:  0.04114150255918503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  96 | Train Loss:  0.04541632533073425 | Validation Loss:  0.04077843204140663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  97 | Train Loss:  0.04504093900322914 | Validation Loss:  0.040419802069664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  98 | Train Loss:  0.04467008262872696 | Validation Loss:  0.04006555676460266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  99 | Train Loss:  0.04430367425084114 | Validation Loss:  0.03971565142273903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  100 | Train Loss:  0.043941680341959 | Validation Loss:  0.03937002643942833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  101 | Train Loss:  0.04358404129743576 | Validation Loss:  0.03902863338589668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  102 | Train Loss:  0.043230701237916946 | Validation Loss:  0.03869141638278961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  103 | Train Loss:  0.042881615459918976 | Validation Loss:  0.038358334451913834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  104 | Train Loss:  0.04253672808408737 | Validation Loss:  0.03802933916449547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  105 | Train Loss:  0.04219599813222885 | Validation Loss:  0.03770436719059944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  106 | Train Loss:  0.04185936972498894 | Validation Loss:  0.03738337755203247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  107 | Train Loss:  0.04152679070830345 | Validation Loss:  0.03706632927060127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  108 | Train Loss:  0.041198208928108215 | Validation Loss:  0.03675316646695137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  109 | Train Loss:  0.04087359830737114 | Validation Loss:  0.036443840712308884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  110 | Train Loss:  0.040552884340286255 | Validation Loss:  0.03613831847906113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  111 | Train Loss:  0.04023604094982147 | Validation Loss:  0.035836540162563324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  112 | Train Loss:  0.03992301598191261 | Validation Loss:  0.03553846850991249\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  113 | Train Loss:  0.0396137610077858 | Validation Loss:  0.03524405509233475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  114 | Train Loss:  0.03930823132395744 | Validation Loss:  0.03495325893163681\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  115 | Train Loss:  0.03900638595223427 | Validation Loss:  0.0346660353243351\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  116 | Train Loss:  0.03870818018913269 | Validation Loss:  0.03438233956694603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  117 | Train Loss:  0.03841356933116913 | Validation Loss:  0.03410213440656662\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  118 | Train Loss:  0.03812250867486 | Validation Loss:  0.03382537141442299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  119 | Train Loss:  0.037834957242012024 | Validation Loss:  0.03355201706290245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  120 | Train Loss:  0.037550874054431915 | Validation Loss:  0.033282022923231125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  121 | Train Loss:  0.03727022558450699 | Validation Loss:  0.03301534801721573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  122 | Train Loss:  0.03699295595288277 | Validation Loss:  0.032751958817243576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  123 | Train Loss:  0.036719031631946564 | Validation Loss:  0.03249180689454079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  124 | Train Loss:  0.036448411643505096 | Validation Loss:  0.032234858721494675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  125 | Train Loss:  0.03618105873465538 | Validation Loss:  0.031981080770492554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  126 | Train Loss:  0.03591693937778473 | Validation Loss:  0.03173042833805084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  127 | Train Loss:  0.03565600514411926 | Validation Loss:  0.031482864171266556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  128 | Train Loss:  0.035398218780756 | Validation Loss:  0.031238354742527008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  129 | Train Loss:  0.03514355048537254 | Validation Loss:  0.030996860936284065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  130 | Train Loss:  0.03489195555448532 | Validation Loss:  0.030758343636989594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  131 | Train Loss:  0.034643396735191345 | Validation Loss:  0.030522773042321205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  132 | Train Loss:  0.03439784795045853 | Validation Loss:  0.030290106311440468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  133 | Train Loss:  0.034155264496803284 | Validation Loss:  0.030060311779379845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  134 | Train Loss:  0.03391561284661293 | Validation Loss:  0.029833361506462097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  135 | Train Loss:  0.03367885574698448 | Validation Loss:  0.029609207063913345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  136 | Train Loss:  0.03344496339559555 | Validation Loss:  0.029387827962636948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  137 | Train Loss:  0.03321390226483345 | Validation Loss:  0.02916918694972992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  138 | Train Loss:  0.0329856276512146 | Validation Loss:  0.028953244909644127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  139 | Train Loss:  0.032760120928287506 | Validation Loss:  0.02873997390270233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  140 | Train Loss:  0.03253733739256859 | Validation Loss:  0.028529345989227295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  141 | Train Loss:  0.032317258417606354 | Validation Loss:  0.028321322053670883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  142 | Train Loss:  0.03209983557462692 | Validation Loss:  0.02811586856842041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  143 | Train Loss:  0.031885042786598206 | Validation Loss:  0.027912965044379234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  144 | Train Loss:  0.031672850251197815 | Validation Loss:  0.02771257795393467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  145 | Train Loss:  0.03146323561668396 | Validation Loss:  0.027514666318893433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  146 | Train Loss:  0.03125615417957306 | Validation Loss:  0.027319209650158882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  147 | Train Loss:  0.031051578000187874 | Validation Loss:  0.027126174420118332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  148 | Train Loss:  0.030849479138851166 | Validation Loss:  0.02693554200232029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  149 | Train Loss:  0.030649829655885696 | Validation Loss:  0.026747265830636024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  150 | Train Loss:  0.030452601611614227 | Validation Loss:  0.026561331003904343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  151 | Train Loss:  0.030257761478424072 | Validation Loss:  0.02637770213186741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  152 | Train Loss:  0.03006528504192829 | Validation Loss:  0.026196353137493134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  153 | Train Loss:  0.029875144362449646 | Validation Loss:  0.026017257943749428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  154 | Train Loss:  0.0296873040497303 | Validation Loss:  0.025840390473604202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  155 | Train Loss:  0.029501745477318764 | Validation Loss:  0.02566572092473507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  156 | Train Loss:  0.02931843511760235 | Validation Loss:  0.02549322135746479\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  157 | Train Loss:  0.029137350618839264 | Validation Loss:  0.025322863832116127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  158 | Train Loss:  0.028958458453416824 | Validation Loss:  0.025154633447527885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  159 | Train Loss:  0.028781739994883537 | Validation Loss:  0.024988491088151932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  160 | Train Loss:  0.028607165440917015 | Validation Loss:  0.024824419990181923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  161 | Train Loss:  0.028434712439775467 | Validation Loss:  0.024662388488650322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  162 | Train Loss:  0.028264351189136505 | Validation Loss:  0.024502377957105637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  163 | Train Loss:  0.02809605747461319 | Validation Loss:  0.024344362318515778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  164 | Train Loss:  0.02792981080710888 | Validation Loss:  0.024188313633203506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  165 | Train Loss:  0.027765575796365738 | Validation Loss:  0.02403421141207218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  166 | Train Loss:  0.02760334126651287 | Validation Loss:  0.023882035166025162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  167 | Train Loss:  0.02744307927787304 | Validation Loss:  0.02373175323009491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  168 | Train Loss:  0.027284763753414154 | Validation Loss:  0.02358335256576538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  169 | Train Loss:  0.027128372341394424 | Validation Loss:  0.02343679964542389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  170 | Train Loss:  0.02697388082742691 | Validation Loss:  0.02329207956790924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  171 | Train Loss:  0.026821264997124672 | Validation Loss:  0.023149166256189346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  172 | Train Loss:  0.026670509949326515 | Validation Loss:  0.023008035495877266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  173 | Train Loss:  0.026521582156419754 | Validation Loss:  0.022868676111102104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  174 | Train Loss:  0.026374468579888344 | Validation Loss:  0.022731058299541473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  175 | Train Loss:  0.026229150593280792 | Validation Loss:  0.022595159709453583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  176 | Train Loss:  0.026085592806339264 | Validation Loss:  0.02246096171438694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  177 | Train Loss:  0.025943782180547714 | Validation Loss:  0.022328447550535202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  178 | Train Loss:  0.02580370008945465 | Validation Loss:  0.02219758927822113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  179 | Train Loss:  0.025665324181318283 | Validation Loss:  0.022068368270993233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  180 | Train Loss:  0.025528626516461372 | Validation Loss:  0.021940771490335464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  181 | Train Loss:  0.025393599644303322 | Validation Loss:  0.021814772859215736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  182 | Train Loss:  0.025260211899876595 | Validation Loss:  0.021690351888537407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  183 | Train Loss:  0.025128455832600594 | Validation Loss:  0.021567489951848984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  184 | Train Loss:  0.024998296052217484 | Validation Loss:  0.021446174010634422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  185 | Train Loss:  0.024869726970791817 | Validation Loss:  0.021326377987861633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  186 | Train Loss:  0.024742724373936653 | Validation Loss:  0.021208086982369423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  187 | Train Loss:  0.02461726777255535 | Validation Loss:  0.0210912823677063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  188 | Train Loss:  0.02449333854019642 | Validation Loss:  0.02097594365477562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  189 | Train Loss:  0.024370921775698662 | Validation Loss:  0.02086205780506134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  190 | Train Loss:  0.02424999698996544 | Validation Loss:  0.02074960060417652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  191 | Train Loss:  0.02413054369390011 | Validation Loss:  0.02063855715095997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  192 | Train Loss:  0.024012550711631775 | Validation Loss:  0.020528914406895638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  193 | Train Loss:  0.023895995691418648 | Validation Loss:  0.020420648157596588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  194 | Train Loss:  0.023780863732099533 | Validation Loss:  0.020313749089837074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  195 | Train Loss:  0.02366713248193264 | Validation Loss:  0.020208191126585007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  196 | Train Loss:  0.023554788902401924 | Validation Loss:  0.02010396495461464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  197 | Train Loss:  0.02344381995499134 | Validation Loss:  0.02000105381011963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  198 | Train Loss:  0.023334205150604248 | Validation Loss:  0.019899437204003334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  199 | Train Loss:  0.023225924000144005 | Validation Loss:  0.01979910396039486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  200 | Train Loss:  0.023118965327739716 | Validation Loss:  0.01970004104077816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  201 | Train Loss:  0.023013317957520485 | Validation Loss:  0.019602222368121147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  202 | Train Loss:  0.022908957675099373 | Validation Loss:  0.019505638629198074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  203 | Train Loss:  0.022805869579315186 | Validation Loss:  0.019410276785492897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  204 | Train Loss:  0.02270403876900673 | Validation Loss:  0.019316120073199272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  205 | Train Loss:  0.022603459656238556 | Validation Loss:  0.019223148003220558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  206 | Train Loss:  0.02250409871339798 | Validation Loss:  0.019131356850266457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  207 | Train Loss:  0.022405955940485 | Validation Loss:  0.019040726125240326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  208 | Train Loss:  0.022309014573693275 | Validation Loss:  0.018951237201690674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  209 | Train Loss:  0.022213255986571312 | Validation Loss:  0.018862884491682053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  210 | Train Loss:  0.022118669003248215 | Validation Loss:  0.01877564936876297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  211 | Train Loss:  0.02202523685991764 | Validation Loss:  0.01868952065706253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  212 | Train Loss:  0.021932948380708694 | Validation Loss:  0.018604479730129242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  213 | Train Loss:  0.021841786801815033 | Validation Loss:  0.018520521000027657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  214 | Train Loss:  0.021751737222075462 | Validation Loss:  0.018437622115015984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  215 | Train Loss:  0.021662792190909386 | Validation Loss:  0.018355773761868477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  216 | Train Loss:  0.02157493308186531 | Validation Loss:  0.01827496476471424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  217 | Train Loss:  0.021488146856427193 | Validation Loss:  0.01819518208503723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  218 | Train Loss:  0.021402424201369286 | Validation Loss:  0.018116412684321404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  219 | Train Loss:  0.021317752078175545 | Validation Loss:  0.018038639798760414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  220 | Train Loss:  0.021234115585684776 | Validation Loss:  0.017961859703063965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  221 | Train Loss:  0.021151499822735786 | Validation Loss:  0.017886050045490265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  222 | Train Loss:  0.02106989547610283 | Validation Loss:  0.017811208963394165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  223 | Train Loss:  0.02098929136991501 | Validation Loss:  0.017737315967679024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  224 | Train Loss:  0.020909670740365982 | Validation Loss:  0.017664363607764244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  225 | Train Loss:  0.020831024274230003 | Validation Loss:  0.01759233884513378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  226 | Train Loss:  0.020753344520926476 | Validation Loss:  0.01752123050391674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  227 | Train Loss:  0.020676612854003906 | Validation Loss:  0.017451029270887375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  228 | Train Loss:  0.0206008218228817 | Validation Loss:  0.01738172210752964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  229 | Train Loss:  0.020525958389043808 | Validation Loss:  0.017313295975327492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  230 | Train Loss:  0.02045201137661934 | Validation Loss:  0.017245745286345482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  231 | Train Loss:  0.020378967747092247 | Validation Loss:  0.017179051414132118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  232 | Train Loss:  0.020306820049881935 | Validation Loss:  0.017113210633397102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  233 | Train Loss:  0.020235558971762657 | Validation Loss:  0.01704820990562439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  234 | Train Loss:  0.02016516774892807 | Validation Loss:  0.016984036192297935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  235 | Train Loss:  0.020095638930797577 | Validation Loss:  0.016920683905482292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  236 | Train Loss:  0.02002696320414543 | Validation Loss:  0.016858140006661415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  237 | Train Loss:  0.01995912566781044 | Validation Loss:  0.016796395182609558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  238 | Train Loss:  0.019892120733857155 | Validation Loss:  0.016735440120100975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  239 | Train Loss:  0.01982593722641468 | Validation Loss:  0.01667526550590992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  240 | Train Loss:  0.01976056583225727 | Validation Loss:  0.016615858301520348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  241 | Train Loss:  0.01969599723815918 | Validation Loss:  0.01655721478164196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  242 | Train Loss:  0.019632214680314064 | Validation Loss:  0.016499316319823265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  243 | Train Loss:  0.019569218158721924 | Validation Loss:  0.016442159190773964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  244 | Train Loss:  0.019506987184286118 | Validation Loss:  0.016385739669203758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  245 | Train Loss:  0.019445523619651794 | Validation Loss:  0.016330037266016006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  246 | Train Loss:  0.01938481442630291 | Validation Loss:  0.01627505198121071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  247 | Train Loss:  0.01932484842836857 | Validation Loss:  0.01622076891362667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  248 | Train Loss:  0.019265618175268173 | Validation Loss:  0.016167182475328445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  249 | Train Loss:  0.01920711062848568 | Validation Loss:  0.016114285215735435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  250 | Train Loss:  0.01914932020008564 | Validation Loss:  0.016062064096331596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  251 | Train Loss:  0.019092245027422905 | Validation Loss:  0.01601051539182663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  252 | Train Loss:  0.019035862758755684 | Validation Loss:  0.01595962792634964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  253 | Train Loss:  0.018980175256729126 | Validation Loss:  0.01590939238667488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  254 | Train Loss:  0.018925169482827187 | Validation Loss:  0.015859805047512054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  255 | Train Loss:  0.01887083612382412 | Validation Loss:  0.015810854732990265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  256 | Train Loss:  0.018817169591784477 | Validation Loss:  0.015762530267238617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  257 | Train Loss:  0.018764162436127663 | Validation Loss:  0.01571483165025711\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  258 | Train Loss:  0.018711809068918228 | Validation Loss:  0.0156677458435297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  259 | Train Loss:  0.01866009086370468 | Validation Loss:  0.015621265396475792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  260 | Train Loss:  0.01860900968313217 | Validation Loss:  0.01557538378983736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  261 | Train Loss:  0.018558554351329803 | Validation Loss:  0.01553009171038866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  262 | Train Loss:  0.01850871741771698 | Validation Loss:  0.015485385432839394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  263 | Train Loss:  0.018459493294358253 | Validation Loss:  0.015441253781318665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  264 | Train Loss:  0.018410876393318176 | Validation Loss:  0.015397693030536175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  265 | Train Loss:  0.018362848088145256 | Validation Loss:  0.015354691073298454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  266 | Train Loss:  0.01831541210412979 | Validation Loss:  0.015312246046960354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  267 | Train Loss:  0.018268557265400887 | Validation Loss:  0.015270348638296127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  268 | Train Loss:  0.018222279846668243 | Validation Loss:  0.015228994190692902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  269 | Train Loss:  0.018176564946770668 | Validation Loss:  0.015188170596957207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  270 | Train Loss:  0.01813141629099846 | Validation Loss:  0.015147875994443893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  271 | Train Loss:  0.018086818978190422 | Validation Loss:  0.015108100138604641\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  272 | Train Loss:  0.01804276742041111 | Validation Loss:  0.015068840235471725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  273 | Train Loss:  0.017999256029725075 | Validation Loss:  0.0150300869718194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  274 | Train Loss:  0.01795627921819687 | Validation Loss:  0.014991837553679943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  275 | Train Loss:  0.017913829535245895 | Validation Loss:  0.014954080805182457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  276 | Train Loss:  0.017871899530291557 | Validation Loss:  0.014916813001036644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  277 | Train Loss:  0.017830485478043556 | Validation Loss:  0.014880029484629631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  278 | Train Loss:  0.017789574339985847 | Validation Loss:  0.014843719080090523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  279 | Train Loss:  0.01774916984140873 | Validation Loss:  0.01480787992477417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  280 | Train Loss:  0.01770925708115101 | Validation Loss:  0.0147725073620677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  281 | Train Loss:  0.017669834196567535 | Validation Loss:  0.01473759114742279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  282 | Train Loss:  0.01763089746236801 | Validation Loss:  0.01470312662422657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  283 | Train Loss:  0.017592433840036392 | Validation Loss:  0.014669114723801613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  284 | Train Loss:  0.017554448917508125 | Validation Loss:  0.01463553961366415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  285 | Train Loss:  0.01751692220568657 | Validation Loss:  0.014602403156459332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  286 | Train Loss:  0.017479857429862022 | Validation Loss:  0.014569695107638836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  287 | Train Loss:  0.017443247139453888 | Validation Loss:  0.01453741267323494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  288 | Train Loss:  0.017407085746526718 | Validation Loss:  0.014505548402667046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  289 | Train Loss:  0.017371367663145065 | Validation Loss:  0.014474100433290005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  290 | Train Loss:  0.017336085438728333 | Validation Loss:  0.01444306131452322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  291 | Train Loss:  0.01730123721063137 | Validation Loss:  0.014412423595786095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  292 | Train Loss:  0.017266815528273582 | Validation Loss:  0.014382186345756054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  293 | Train Loss:  0.01723281480371952 | Validation Loss:  0.01435234397649765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  294 | Train Loss:  0.017199233174324036 | Validation Loss:  0.014322888106107712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  295 | Train Loss:  0.017166059464216232 | Validation Loss:  0.014293815940618515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  296 | Train Loss:  0.01713329367339611 | Validation Loss:  0.014265121892094612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  297 | Train Loss:  0.017100930213928223 | Validation Loss:  0.014236805029213428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  298 | Train Loss:  0.017068961635231972 | Validation Loss:  0.014208854176104069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  299 | Train Loss:  0.01703738607466221 | Validation Loss:  0.014181269332766533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  300 | Train Loss:  0.017006194218993187 | Validation Loss:  0.014154043979942799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  301 | Train Loss:  0.016975384205579758 | Validation Loss:  0.014127174392342567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  302 | Train Loss:  0.01694495417177677 | Validation Loss:  0.014100654982030392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  303 | Train Loss:  0.016914894804358482 | Validation Loss:  0.014074482023715973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  304 | Train Loss:  0.01688520424067974 | Validation Loss:  0.014048651792109013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  305 | Train Loss:  0.01685587503015995 | Validation Loss:  0.014023158699274063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  306 | Train Loss:  0.01682690717279911 | Validation Loss:  0.013997997157275677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  307 | Train Loss:  0.016798293218016624 | Validation Loss:  0.01397316437214613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  308 | Train Loss:  0.016770025715231895 | Validation Loss:  0.013948657549917698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  309 | Train Loss:  0.016742104664444923 | Validation Loss:  0.013924473896622658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  310 | Train Loss:  0.01671453006565571 | Validation Loss:  0.013900604099035263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  311 | Train Loss:  0.016687290742993355 | Validation Loss:  0.01387704536318779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  312 | Train Loss:  0.016660379245877266 | Validation Loss:  0.013853797689080238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  313 | Train Loss:  0.01663380302488804 | Validation Loss:  0.013830854557454586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  314 | Train Loss:  0.01660754904150963 | Validation Loss:  0.01380820944905281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  315 | Train Loss:  0.016581615433096886 | Validation Loss:  0.013785865157842636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  316 | Train Loss:  0.016555998474359512 | Validation Loss:  0.013763809576630592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  317 | Train Loss:  0.01653069444000721 | Validation Loss:  0.013742047362029552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  318 | Train Loss:  0.01650569960474968 | Validation Loss:  0.013720567338168621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  319 | Train Loss:  0.016481010243296623 | Validation Loss:  0.013699370436370373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  320 | Train Loss:  0.016456622630357742 | Validation Loss:  0.013678450137376785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  321 | Train Loss:  0.01643253117799759 | Validation Loss:  0.013657807372510433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  322 | Train Loss:  0.016408735886216164 | Validation Loss:  0.013637433759868145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  323 | Train Loss:  0.01638522930443287 | Validation Loss:  0.013617328368127346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  324 | Train Loss:  0.016362009570002556 | Validation Loss:  0.013597485609352589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  325 | Train Loss:  0.016339071094989777 | Validation Loss:  0.013577906414866447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  326 | Train Loss:  0.01631641574203968 | Validation Loss:  0.013558581471443176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  327 | Train Loss:  0.01629403419792652 | Validation Loss:  0.013539514504373074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  328 | Train Loss:  0.01627192460000515 | Validation Loss:  0.013520697131752968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  329 | Train Loss:  0.016250088810920715 | Validation Loss:  0.013502126559615135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  330 | Train Loss:  0.01622851751744747 | Validation Loss:  0.013483800925314426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  331 | Train Loss:  0.01620720699429512 | Validation Loss:  0.013465716503560543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  332 | Train Loss:  0.016186155378818512 | Validation Loss:  0.013447871431708336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  333 | Train Loss:  0.016165360808372498 | Validation Loss:  0.013430259190499783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  334 | Train Loss:  0.016144821420311928 | Validation Loss:  0.013412880711257458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  335 | Train Loss:  0.016124529764056206 | Validation Loss:  0.013395728543400764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  336 | Train Loss:  0.016104483976960182 | Validation Loss:  0.013378807343542576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  337 | Train Loss:  0.016084685921669006 | Validation Loss:  0.013362104073166847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  338 | Train Loss:  0.016065124422311783 | Validation Loss:  0.013345625251531601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  339 | Train Loss:  0.01604580134153366 | Validation Loss:  0.01332936156541109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  340 | Train Loss:  0.01602671481668949 | Validation Loss:  0.01331331580877304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  341 | Train Loss:  0.016007859259843826 | Validation Loss:  0.01329747959971428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  342 | Train Loss:  0.015989232808351517 | Validation Loss:  0.013281852006912231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  343 | Train Loss:  0.015970833599567413 | Validation Loss:  0.013266431167721748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  344 | Train Loss:  0.01595265418291092 | Validation Loss:  0.013251214288175106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  345 | Train Loss:  0.015934698283672333 | Validation Loss:  0.01323619857430458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  346 | Train Loss:  0.015916958451271057 | Validation Loss:  0.013221380300819874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  347 | Train Loss:  0.015899434685707092 | Validation Loss:  0.013206759467720985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  348 | Train Loss:  0.01588212326169014 | Validation Loss:  0.013192330487072468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  349 | Train Loss:  0.0158650204539299 | Validation Loss:  0.013178094290196896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  350 | Train Loss:  0.015848124399781227 | Validation Loss:  0.013164044357836246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  351 | Train Loss:  0.01583143323659897 | Validation Loss:  0.01315018255263567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  352 | Train Loss:  0.015814945101737976 | Validation Loss:  0.013136502355337143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  353 | Train Loss:  0.01579865626990795 | Validation Loss:  0.013123005628585815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  354 | Train Loss:  0.015782566741108894 | Validation Loss:  0.01310968492180109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  355 | Train Loss:  0.01576666720211506 | Validation Loss:  0.013096543028950691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  356 | Train Loss:  0.015750961378216743 | Validation Loss:  0.01308357436209917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  357 | Train Loss:  0.0157354474067688 | Validation Loss:  0.01307077705860138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  358 | Train Loss:  0.015720117837190628 | Validation Loss:  0.013058150187134743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  359 | Train Loss:  0.01570497453212738 | Validation Loss:  0.013045690022408962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  360 | Train Loss:  0.015690013766288757 | Validation Loss:  0.013033396564424038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  361 | Train Loss:  0.01567523367702961 | Validation Loss:  0.013021264225244522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  362 | Train Loss:  0.01566063053905964 | Validation Loss:  0.013009293004870415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  363 | Train Loss:  0.015646204352378845 | Validation Loss:  0.012997481971979141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  364 | Train Loss:  0.01563195511698723 | Validation Loss:  0.012985828332602978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  365 | Train Loss:  0.015617872588336468 | Validation Loss:  0.012974328361451626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  366 | Train Loss:  0.015603962354362011 | Validation Loss:  0.012962980195879936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  367 | Train Loss:  0.01559021882712841 | Validation Loss:  0.012951784767210484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  368 | Train Loss:  0.015576640143990517 | Validation Loss:  0.01294073648750782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  369 | Train Loss:  0.015563222579658031 | Validation Loss:  0.012929835356771946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  370 | Train Loss:  0.015549969859421253 | Validation Loss:  0.012919079512357712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  371 | Train Loss:  0.01553687360137701 | Validation Loss:  0.012908467091619968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  372 | Train Loss:  0.015523936599493027 | Validation Loss:  0.012897994369268417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  373 | Train Loss:  0.01551115419715643 | Validation Loss:  0.012887660413980484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  374 | Train Loss:  0.015498523600399494 | Validation Loss:  0.012877465225756168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  375 | Train Loss:  0.015486043877899647 | Validation Loss:  0.012867405079305172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  376 | Train Loss:  0.015473716892302036 | Validation Loss:  0.01285748090595007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  377 | Train Loss:  0.01546153612434864 | Validation Loss:  0.012847686186432838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  378 | Train Loss:  0.01544949784874916 | Validation Loss:  0.012838022783398628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  379 | Train Loss:  0.01543760672211647 | Validation Loss:  0.012828487902879715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  380 | Train Loss:  0.015425855293869972 | Validation Loss:  0.012819078750908375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  381 | Train Loss:  0.01541424822062254 | Validation Loss:  0.012809796258807182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  382 | Train Loss:  0.015402778051793575 | Validation Loss:  0.012800637632608414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  383 | Train Loss:  0.015391443856060505 | Validation Loss:  0.01279159914702177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  384 | Train Loss:  0.01538024377077818 | Validation Loss:  0.012782682664692402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  385 | Train Loss:  0.015369178727269173 | Validation Loss:  0.01277388446033001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  386 | Train Loss:  0.015358243137598038 | Validation Loss:  0.012765204533934593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  387 | Train Loss:  0.015347441658377647 | Validation Loss:  0.012756637297570705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  388 | Train Loss:  0.015336764045059681 | Validation Loss:  0.012748186476528645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  389 | Train Loss:  0.015326217748224735 | Validation Loss:  0.012739846482872963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  390 | Train Loss:  0.015315794385969639 | Validation Loss:  0.012731621041893959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  391 | Train Loss:  0.015305494889616966 | Validation Loss:  0.012723501771688461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  392 | Train Loss:  0.015295318327844143 | Validation Loss:  0.012715492397546768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  393 | Train Loss:  0.01528526097536087 | Validation Loss:  0.012707589194178581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  394 | Train Loss:  0.015275323763489723 | Validation Loss:  0.0126997921615839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  395 | Train Loss:  0.015265502966940403 | Validation Loss:  0.012692098505795002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  396 | Train Loss:  0.01525579858571291 | Validation Loss:  0.01268450915813446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  397 | Train Loss:  0.015246210619807243 | Validation Loss:  0.012677016668021679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  398 | Train Loss:  0.015236735343933105 | Validation Loss:  0.012669626623392105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  399 | Train Loss:  0.015227370895445347 | Validation Loss:  0.012662336230278015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  400 | Train Loss:  0.015218118205666542 | Validation Loss:  0.01265514176338911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  401 | Train Loss:  0.015208974480628967 | Validation Loss:  0.012648042291402817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  402 | Train Loss:  0.015199937857687473 | Validation Loss:  0.012641039676964283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  403 | Train Loss:  0.015191004611551762 | Validation Loss:  0.012634127400815487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  404 | Train Loss:  0.015182181261479855 | Validation Loss:  0.01262731198221445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  405 | Train Loss:  0.01517345942556858 | Validation Loss:  0.01262058224529028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  406 | Train Loss:  0.01516483910381794 | Validation Loss:  0.012613944709300995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  407 | Train Loss:  0.015156322158873081 | Validation Loss:  0.012607395648956299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  408 | Train Loss:  0.015147903934121132 | Validation Loss:  0.012600934132933617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  409 | Train Loss:  0.015139583498239517 | Validation Loss:  0.012594559229910374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  410 | Train Loss:  0.015131361782550812 | Validation Loss:  0.012588270008563995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  411 | Train Loss:  0.015123234130442142 | Validation Loss:  0.012582061812281609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  412 | Train Loss:  0.015115202404558659 | Validation Loss:  0.012575938366353512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  413 | Train Loss:  0.015107264742255211 | Validation Loss:  0.012569895014166832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  414 | Train Loss:  0.015099418349564075 | Validation Loss:  0.012563933618366718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  415 | Train Loss:  0.015091666020452976 | Validation Loss:  0.012558050453662872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  416 | Train Loss:  0.01508400123566389 | Validation Loss:  0.012552246451377869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  417 | Train Loss:  0.015076428651809692 | Validation Loss:  0.012546519748866558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  418 | Train Loss:  0.015068940818309784 | Validation Loss:  0.012540869414806366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  419 | Train Loss:  0.01506154052913189 | Validation Loss:  0.012535294517874718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  420 | Train Loss:  0.01505422592163086 | Validation Loss:  0.012529794126749039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  421 | Train Loss:  0.015046997927129269 | Validation Loss:  0.012524367310106754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  422 | Train Loss:  0.015039852820336819 | Validation Loss:  0.012519009411334991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  423 | Train Loss:  0.01503278873860836 | Validation Loss:  0.012513726949691772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  424 | Train Loss:  0.015025807544589043 | Validation Loss:  0.012508511543273926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  425 | Train Loss:  0.015018906444311142 | Validation Loss:  0.01250336691737175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  426 | Train Loss:  0.015012085437774658 | Validation Loss:  0.01249829214066267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  427 | Train Loss:  0.015005341731011868 | Validation Loss:  0.012493284419178963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  428 | Train Loss:  0.01499867532402277 | Validation Loss:  0.012488341890275478\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  429 | Train Loss:  0.01499208528548479 | Validation Loss:  0.012483466416597366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  430 | Train Loss:  0.01498557347804308 | Validation Loss:  0.012478654272854328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  431 | Train Loss:  0.014979134313762188 | Validation Loss:  0.012473907321691513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  432 | Train Loss:  0.014972769655287266 | Validation Loss:  0.012469222769141197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  433 | Train Loss:  0.014966477639973164 | Validation Loss:  0.01246460061520338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  434 | Train Loss:  0.014960256405174732 | Validation Loss:  0.012460040859878063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  435 | Train Loss:  0.01495410781353712 | Validation Loss:  0.012455540709197521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  436 | Train Loss:  0.014948028139770031 | Validation Loss:  0.01245109923183918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  437 | Train Loss:  0.014942016452550888 | Validation Loss:  0.01244671642780304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  438 | Train Loss:  0.014936074614524841 | Validation Loss:  0.012442395091056824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  439 | Train Loss:  0.014930200763046741 | Validation Loss:  0.01243812870234251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  440 | Train Loss:  0.014924392104148865 | Validation Loss:  0.012433918192982674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  441 | Train Loss:  0.014918648637831211 | Validation Loss:  0.012429763562977314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  442 | Train Loss:  0.01491297222673893 | Validation Loss:  0.012425667606294155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  443 | Train Loss:  0.014907359145581722 | Validation Loss:  0.012421621941030025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  444 | Train Loss:  0.014901809394359589 | Validation Loss:  0.012417631223797798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  445 | Train Loss:  0.014896320179104805 | Validation Loss:  0.01241369266062975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  446 | Train Loss:  0.01489089522510767 | Validation Loss:  0.012409805320203304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  447 | Train Loss:  0.01488553173840046 | Validation Loss:  0.012405971065163612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  448 | Train Loss:  0.01488022692501545 | Validation Loss:  0.01240218710154295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  449 | Train Loss:  0.014874981716275215 | Validation Loss:  0.012398453429341316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  450 | Train Loss:  0.014869794249534607 | Validation Loss:  0.012394767254590988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  451 | Train Loss:  0.0148646654561162 | Validation Loss:  0.01239113137125969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  452 | Train Loss:  0.014859594404697418 | Validation Loss:  0.01238754391670227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  453 | Train Loss:  0.01485457830131054 | Validation Loss:  0.012384001165628433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  454 | Train Loss:  0.014849619008600712 | Validation Loss:  0.012380506843328476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  455 | Train Loss:  0.014844715595245361 | Validation Loss:  0.0123770572245121\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  456 | Train Loss:  0.014839865267276764 | Validation Loss:  0.01237365510314703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  457 | Train Loss:  0.01483506802469492 | Validation Loss:  0.012370296753942966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  458 | Train Loss:  0.014830324798822403 | Validation Loss:  0.01236698217689991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  459 | Train Loss:  0.014825633727014065 | Validation Loss:  0.01236371137201786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  460 | Train Loss:  0.01482099574059248 | Validation Loss:  0.012360482476651669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  461 | Train Loss:  0.01481640711426735 | Validation Loss:  0.012357296422123909\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  462 | Train Loss:  0.014811867848038673 | Validation Loss:  0.012354152277112007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  463 | Train Loss:  0.014807380735874176 | Validation Loss:  0.012351050041615963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  464 | Train Loss:  0.01480294018983841 | Validation Loss:  0.012347987852990627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  465 | Train Loss:  0.014798550866544247 | Validation Loss:  0.012344966642558575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  466 | Train Loss:  0.01479420717805624 | Validation Loss:  0.012341982685029507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  467 | Train Loss:  0.014789911918342113 | Validation Loss:  0.012339037843048573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  468 | Train Loss:  0.014785662293434143 | Validation Loss:  0.012336133047938347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  469 | Train Loss:  0.014781460165977478 | Validation Loss:  0.012333263643085957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  470 | Train Loss:  0.014777302742004395 | Validation Loss:  0.012330434285104275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  471 | Train Loss:  0.014773189090192318 | Validation Loss:  0.012327641248703003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  472 | Train Loss:  0.014769122004508972 | Validation Loss:  0.012324882671236992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  473 | Train Loss:  0.014765098690986633 | Validation Loss:  0.01232216041535139\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  474 | Train Loss:  0.014761118218302727 | Validation Loss:  0.0123194744810462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  475 | Train Loss:  0.014757180586457253 | Validation Loss:  0.01231682300567627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  476 | Train Loss:  0.014753283932805061 | Validation Loss:  0.0123142059892416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  477 | Train Loss:  0.014749429188668728 | Validation Loss:  0.012311622500419617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  478 | Train Loss:  0.014745615422725677 | Validation Loss:  0.012309071607887745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  479 | Train Loss:  0.014741840772330761 | Validation Loss:  0.01230655424296856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  480 | Train Loss:  0.014738109894096851 | Validation Loss:  0.012304069474339485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  481 | Train Loss:  0.014734414406120777 | Validation Loss:  0.012301615439355373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  482 | Train Loss:  0.01473076269030571 | Validation Loss:  0.012299194000661373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  483 | Train Loss:  0.014727146364748478 | Validation Loss:  0.01229680422693491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  484 | Train Loss:  0.01472356915473938 | Validation Loss:  0.01229444332420826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  485 | Train Loss:  0.014720031060278416 | Validation Loss:  0.012292114086449146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  486 | Train Loss:  0.014716527424752712 | Validation Loss:  0.012289813719689846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  487 | Train Loss:  0.014713060110807419 | Validation Loss:  0.012287544086575508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  488 | Train Loss:  0.014709630981087685 | Validation Loss:  0.012285302393138409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  489 | Train Loss:  0.014706237241625786 | Validation Loss:  0.012283088639378548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  490 | Train Loss:  0.014702878892421722 | Validation Loss:  0.0122809037566185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  491 | Train Loss:  0.014699554070830345 | Validation Loss:  0.01227874681353569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  492 | Train Loss:  0.014696263708174229 | Validation Loss:  0.012276615016162395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  493 | Train Loss:  0.014693007804453373 | Validation Loss:  0.012274513952434063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  494 | Train Loss:  0.014689785428345203 | Validation Loss:  0.012272438034415245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  495 | Train Loss:  0.014686597511172295 | Validation Loss:  0.012270386330783367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  496 | Train Loss:  0.014683440327644348 | Validation Loss:  0.012268360704183578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  497 | Train Loss:  0.014680315740406513 | Validation Loss:  0.012266362085938454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  498 | Train Loss:  0.014677224680781364 | Validation Loss:  0.012264389544725418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  499 | Train Loss:  0.014674163423478603 | Validation Loss:  0.012262437492609024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  500 | Train Loss:  0.01467113196849823 | Validation Loss:  0.012260514311492443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  501 | Train Loss:  0.014668134972453117 | Validation Loss:  0.012258613482117653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  502 | Train Loss:  0.014665165916085243 | Validation Loss:  0.012256735935807228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  503 | Train Loss:  0.014662226662039757 | Validation Loss:  0.012254881672561169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  504 | Train Loss:  0.014659317210316658 | Validation Loss:  0.012253052555024624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  505 | Train Loss:  0.014656435698270798 | Validation Loss:  0.01225124392658472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  506 | Train Loss:  0.014653585851192474 | Validation Loss:  0.012249457649886608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  507 | Train Loss:  0.01465076394379139 | Validation Loss:  0.012247692793607712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  508 | Train Loss:  0.014647966250777245 | Validation Loss:  0.012245950289070606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  509 | Train Loss:  0.014645200222730637 | Validation Loss:  0.012244230136275291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  510 | Train Loss:  0.014642460271716118 | Validation Loss:  0.012242528609931469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  511 | Train Loss:  0.014639746397733688 | Validation Loss:  0.012240851297974586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  512 | Train Loss:  0.014637061394751072 | Validation Loss:  0.012239192612469196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  513 | Train Loss:  0.014634399674832821 | Validation Loss:  0.012237554416060448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  514 | Train Loss:  0.014631765894591808 | Validation Loss:  0.01223593670874834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  515 | Train Loss:  0.014629158191382885 | Validation Loss:  0.012234337627887726\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  516 | Train Loss:  0.014626576565206051 | Validation Loss:  0.012232757173478603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  517 | Train Loss:  0.014624016359448433 | Validation Loss:  0.012231196276843548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  518 | Train Loss:  0.014621484093368053 | Validation Loss:  0.01222965307533741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  519 | Train Loss:  0.01461897324770689 | Validation Loss:  0.012228133156895638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  520 | Train Loss:  0.01461648941040039 | Validation Loss:  0.012226627208292484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  521 | Train Loss:  0.014614027924835682 | Validation Loss:  0.012225139886140823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  522 | Train Loss:  0.014611591584980488 | Validation Loss:  0.012223669327795506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  523 | Train Loss:  0.014609177596867085 | Validation Loss:  0.01222221739590168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  524 | Train Loss:  0.014606784097850323 | Validation Loss:  0.012220784090459347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  525 | Train Loss:  0.014604414813220501 | Validation Loss:  0.012219364754855633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  526 | Train Loss:  0.01460206974297762 | Validation Loss:  0.012217963114380836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  527 | Train Loss:  0.014599745161831379 | Validation Loss:  0.012216579169034958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  528 | Train Loss:  0.01459744106978178 | Validation Loss:  0.012215211987495422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  529 | Train Loss:  0.014595160260796547 | Validation Loss:  0.012213857844471931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  530 | Train Loss:  0.014592899940907955 | Validation Loss:  0.012212519533932209\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  531 | Train Loss:  0.014590660110116005 | Validation Loss:  0.01221119798719883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  532 | Train Loss:  0.014588440768420696 | Validation Loss:  0.012209892272949219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  533 | Train Loss:  0.014586244709789753 | Validation Loss:  0.012208600528538227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  534 | Train Loss:  0.014584065414965153 | Validation Loss:  0.012207325547933578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  535 | Train Loss:  0.014581907540559769 | Validation Loss:  0.0122060626745224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  536 | Train Loss:  0.014579768292605877 | Validation Loss:  0.01220481563359499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  537 | Train Loss:  0.014577649533748627 | Validation Loss:  0.01220358069986105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  538 | Train Loss:  0.014575548470020294 | Validation Loss:  0.012202361598610878\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  539 | Train Loss:  0.014573466964066029 | Validation Loss:  0.012201156467199326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  540 | Train Loss:  0.014571404084563255 | Validation Loss:  0.012199963442981243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  541 | Train Loss:  0.014569360762834549 | Validation Loss:  0.01219878625124693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  542 | Train Loss:  0.014567334204912186 | Validation Loss:  0.012197619304060936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  543 | Train Loss:  0.01456532720476389 | Validation Loss:  0.012196466326713562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  544 | Train Loss:  0.014563336968421936 | Validation Loss:  0.012195326387882233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  545 | Train Loss:  0.014561363495886326 | Validation Loss:  0.012194199487566948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  546 | Train Loss:  0.014559407718479633 | Validation Loss:  0.012193083763122559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  547 | Train Loss:  0.014557468704879284 | Validation Loss:  0.01219198014587164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  548 | Train Loss:  0.014555548317730427 | Validation Loss:  0.01219088863581419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  549 | Train Loss:  0.014553641900420189 | Validation Loss:  0.012189810164272785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  550 | Train Loss:  0.014551754109561443 | Validation Loss:  0.012188742868602276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  551 | Train Loss:  0.014549881219863892 | Validation Loss:  0.012187685817480087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  552 | Train Loss:  0.014548025093972683 | Validation Loss:  0.012186640873551369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  553 | Train Loss:  0.014546184800565243 | Validation Loss:  0.012185606174170971\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  554 | Train Loss:  0.014544359408318996 | Validation Loss:  0.012184584513306618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  555 | Train Loss:  0.014542549848556519 | Validation Loss:  0.012183571234345436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  556 | Train Loss:  0.014540755189955235 | Validation Loss:  0.012182570062577724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  557 | Train Loss:  0.01453897450119257 | Validation Loss:  0.012181579135358334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  558 | Train Loss:  0.014537211507558823 | Validation Loss:  0.012180597521364689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  559 | Train Loss:  0.014535460621118546 | Validation Loss:  0.01217962708324194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  560 | Train Loss:  0.014533724635839462 | Validation Loss:  0.012178665958344936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  561 | Train Loss:  0.014532003551721573 | Validation Loss:  0.012177715077996254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  562 | Train Loss:  0.014530295506119728 | Validation Loss:  0.012176774442195892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  563 | Train Loss:  0.014528604224324226 | Validation Loss:  0.012175840325653553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  564 | Train Loss:  0.01452692411839962 | Validation Loss:  0.012174918316304684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  565 | Train Loss:  0.014525257050991058 | Validation Loss:  0.012174004688858986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  566 | Train Loss:  0.014523603022098541 | Validation Loss:  0.012173101305961609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  567 | Train Loss:  0.014521964825689793 | Validation Loss:  0.012172204442322254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  568 | Train Loss:  0.01452033780515194 | Validation Loss:  0.01217131968587637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  569 | Train Loss:  0.014518724754452705 | Validation Loss:  0.012170439586043358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  570 | Train Loss:  0.014517124742269516 | Validation Loss:  0.012169569730758667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  571 | Train Loss:  0.014515534974634647 | Validation Loss:  0.012168710120022297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  572 | Train Loss:  0.014513959176838398 | Validation Loss:  0.012167857959866524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  573 | Train Loss:  0.014512395486235619 | Validation Loss:  0.012167012318968773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  574 | Train Loss:  0.014510845765471458 | Validation Loss:  0.012166175991296768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  575 | Train Loss:  0.01450930442661047 | Validation Loss:  0.01216534711420536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  576 | Train Loss:  0.0145077770575881 | Validation Loss:  0.0121645238250494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  577 | Train Loss:  0.014506260864436626 | Validation Loss:  0.012163711711764336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  578 | Train Loss:  0.014504754915833473 | Validation Loss:  0.012162905186414719\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  579 | Train Loss:  0.014503262005746365 | Validation Loss:  0.012162107042968273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  580 | Train Loss:  0.014501779340207577 | Validation Loss:  0.01216131541877985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  581 | Train Loss:  0.014500308781862259 | Validation Loss:  0.01216053031384945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  582 | Train Loss:  0.014498850330710411 | Validation Loss:  0.012159752659499645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  583 | Train Loss:  0.014497398398816586 | Validation Loss:  0.012158982455730438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  584 | Train Loss:  0.01449595857411623 | Validation Loss:  0.012158219702541828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  585 | Train Loss:  0.014494530856609344 | Validation Loss:  0.012157462537288666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  586 | Train Loss:  0.01449311338365078 | Validation Loss:  0.012156711891293526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  587 | Train Loss:  0.014491706155240536 | Validation Loss:  0.012155966833233833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  588 | Train Loss:  0.014490308240056038 | Validation Loss:  0.012155230157077312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  589 | Train Loss:  0.014488919638097286 | Validation Loss:  0.012154498137533665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  590 | Train Loss:  0.014487541280686855 | Validation Loss:  0.012153773568570614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  591 | Train Loss:  0.014486173167824745 | Validation Loss:  0.012153053656220436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  592 | Train Loss:  0.014484814368188381 | Validation Loss:  0.01215234212577343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  593 | Train Loss:  0.014483466744422913 | Validation Loss:  0.012151634320616722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  594 | Train Loss:  0.014482127502560616 | Validation Loss:  0.012150933034718037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  595 | Train Loss:  0.01448079477995634 | Validation Loss:  0.012150236405432224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  596 | Train Loss:  0.01447947509586811 | Validation Loss:  0.012149547226727009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  597 | Train Loss:  0.014478161931037903 | Validation Loss:  0.012148862704634666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  598 | Train Loss:  0.014476858079433441 | Validation Loss:  0.012148183770477772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  599 | Train Loss:  0.014475562609732151 | Validation Loss:  0.012147507630288601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  600 | Train Loss:  0.014474276453256607 | Validation Loss:  0.012146839872002602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  601 | Train Loss:  0.014472998678684235 | Validation Loss:  0.0121461758390069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  602 | Train Loss:  0.014471730217337608 | Validation Loss:  0.012145518325269222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  603 | Train Loss:  0.014470470137894154 | Validation Loss:  0.012144863605499268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  604 | Train Loss:  0.014469217509031296 | Validation Loss:  0.012144215404987335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  605 | Train Loss:  0.014467972330749035 | Validation Loss:  0.012143570929765701\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  606 | Train Loss:  0.014466735534369946 | Validation Loss:  0.012142932042479515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  607 | Train Loss:  0.014465507119894028 | Validation Loss:  0.012142296880483627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  608 | Train Loss:  0.014464286155998707 | Validation Loss:  0.012141664512455463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  609 | Train Loss:  0.014463072642683983 | Validation Loss:  0.012141040526330471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  610 | Train Loss:  0.01446186751127243 | Validation Loss:  0.012140419334173203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  611 | Train Loss:  0.014460669830441475 | Validation Loss:  0.012139802798628807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  612 | Train Loss:  0.014459479600191116 | Validation Loss:  0.01213918998837471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  613 | Train Loss:  0.014458296820521355 | Validation Loss:  0.012138580903410912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  614 | Train Loss:  0.01445711962878704 | Validation Loss:  0.012137976475059986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  615 | Train Loss:  0.014455951750278473 | Validation Loss:  0.012137375771999359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  616 | Train Loss:  0.014454789459705353 | Validation Loss:  0.012136779725551605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  617 | Train Loss:  0.014453635551035404 | Validation Loss:  0.012136185541749\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  618 | Train Loss:  0.014452489092946053 | Validation Loss:  0.012135598808526993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  619 | Train Loss:  0.014451346360147 | Validation Loss:  0.012135012075304985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  620 | Train Loss:  0.014450212940573692 | Validation Loss:  0.012134430930018425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  621 | Train Loss:  0.014449086040258408 | Validation Loss:  0.012133853510022163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  622 | Train Loss:  0.01444796472787857 | Validation Loss:  0.0121332798153162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  623 | Train Loss:  0.014446849934756756 | Validation Loss:  0.012132708914577961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  624 | Train Loss:  0.014445741660892963 | Validation Loss:  0.01213214173913002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  625 | Train Loss:  0.014444638043642044 | Validation Loss:  0.012131578288972378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  626 | Train Loss:  0.014443543739616871 | Validation Loss:  0.01213101763278246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  627 | Train Loss:  0.014442455023527145 | Validation Loss:  0.012130459770560265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  628 | Train Loss:  0.014441369101405144 | Validation Loss:  0.012129905633628368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  629 | Train Loss:  0.014440292492508888 | Validation Loss:  0.012129353359341621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  630 | Train Loss:  0.014439219608902931 | Validation Loss:  0.012128806672990322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  631 | Train Loss:  0.014438153244554996 | Validation Loss:  0.012128260917961597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  632 | Train Loss:  0.014437094330787659 | Validation Loss:  0.012127719819545746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  633 | Train Loss:  0.014436040073633194 | Validation Loss:  0.012127179652452469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  634 | Train Loss:  0.014434991404414177 | Validation Loss:  0.01212664321064949\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  635 | Train Loss:  0.014433946460485458 | Validation Loss:  0.012126109562814236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  636 | Train Loss:  0.014432909898459911 | Validation Loss:  0.012125578708946705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  637 | Train Loss:  0.014431877061724663 | Validation Loss:  0.012125050649046898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  638 | Train Loss:  0.014430849812924862 | Validation Loss:  0.01212452631443739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  639 | Train Loss:  0.014429827220737934 | Validation Loss:  0.012124002911150455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  640 | Train Loss:  0.014428810216486454 | Validation Loss:  0.01212348137050867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  641 | Train Loss:  0.014427798800170422 | Validation Loss:  0.012122963555157185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  642 | Train Loss:  0.014426791109144688 | Validation Loss:  0.012122448533773422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  643 | Train Loss:  0.01442579086869955 | Validation Loss:  0.012121937237679958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  644 | Train Loss:  0.014424794353544712 | Validation Loss:  0.012121425941586494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  645 | Train Loss:  0.014423800632357597 | Validation Loss:  0.012120917439460754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  646 | Train Loss:  0.014422815293073654 | Validation Loss:  0.012120411731302738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  647 | Train Loss:  0.01442183181643486 | Validation Loss:  0.012119906023144722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  648 | Train Loss:  0.014420854859054089 | Validation Loss:  0.012119404971599579\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  649 | Train Loss:  0.01441988069564104 | Validation Loss:  0.012118905782699585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  650 | Train Loss:  0.014418913051486015 | Validation Loss:  0.01211840845644474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  651 | Train Loss:  0.014417950063943863 | Validation Loss:  0.012117911130189896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  652 | Train Loss:  0.01441698893904686 | Validation Loss:  0.012117418460547924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  653 | Train Loss:  0.014416036196053028 | Validation Loss:  0.012116926722228527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  654 | Train Loss:  0.014415085315704346 | Validation Loss:  0.012116437777876854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  655 | Train Loss:  0.014414138160645962 | Validation Loss:  0.012115951627492905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  656 | Train Loss:  0.014413196593523026 | Validation Loss:  0.012115463614463806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  657 | Train Loss:  0.014412258751690388 | Validation Loss:  0.012114979326725006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  658 | Train Loss:  0.014411324635148048 | Validation Loss:  0.012114498764276505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  659 | Train Loss:  0.014410394243896008 | Validation Loss:  0.012114018201828003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  660 | Train Loss:  0.01440946850925684 | Validation Loss:  0.012113538570702076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  661 | Train Loss:  0.014408545568585396 | Validation Loss:  0.012113061733543873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  662 | Train Loss:  0.014407629147171974 | Validation Loss:  0.012112585827708244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  663 | Train Loss:  0.014406715519726276 | Validation Loss:  0.012112111784517765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  664 | Train Loss:  0.014405804686248302 | Validation Loss:  0.01211163867264986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  665 | Train Loss:  0.014404897578060627 | Validation Loss:  0.012111167423427105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  666 | Train Loss:  0.0144039960578084 | Validation Loss:  0.012110698036849499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  667 | Train Loss:  0.014403094537556171 | Validation Loss:  0.012110230512917042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  668 | Train Loss:  0.01440220046788454 | Validation Loss:  0.012109764851629734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  669 | Train Loss:  0.014401309192180634 | Validation Loss:  0.012109300121665001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  670 | Train Loss:  0.014400421641767025 | Validation Loss:  0.012108832597732544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  671 | Train Loss:  0.014399535022675991 | Validation Loss:  0.012108372524380684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  672 | Train Loss:  0.014398653991520405 | Validation Loss:  0.0121079096570611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  673 | Train Loss:  0.014397775754332542 | Validation Loss:  0.012107450515031815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  674 | Train Loss:  0.014396902173757553 | Validation Loss:  0.01210699137300253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  675 | Train Loss:  0.014396029524505138 | Validation Loss:  0.012106535024940968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  676 | Train Loss:  0.014395163394510746 | Validation Loss:  0.012106077745556831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  677 | Train Loss:  0.014394296333193779 | Validation Loss:  0.012105623260140419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  678 | Train Loss:  0.014393435791134834 | Validation Loss:  0.012105167843401432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  679 | Train Loss:  0.014392577111721039 | Validation Loss:  0.012104716151952744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  680 | Train Loss:  0.014391722157597542 | Validation Loss:  0.01210426352918148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  681 | Train Loss:  0.014390869066119194 | Validation Loss:  0.012103812769055367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  682 | Train Loss:  0.01439002063125372 | Validation Loss:  0.012103363871574402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  683 | Train Loss:  0.014389174990355968 | Validation Loss:  0.012102914042770863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  684 | Train Loss:  0.014388331212103367 | Validation Loss:  0.012102466076612473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  685 | Train Loss:  0.014387489296495914 | Validation Loss:  0.012102019973099232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  686 | Train Loss:  0.01438665296882391 | Validation Loss:  0.012101574800908566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  687 | Train Loss:  0.01438581757247448 | Validation Loss:  0.01210112776607275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  688 | Train Loss:  0.014384984970092773 | Validation Loss:  0.012100684456527233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  689 | Train Loss:  0.01438415702432394 | Validation Loss:  0.012100240215659142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  690 | Train Loss:  0.014383330941200256 | Validation Loss:  0.012099798768758774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  691 | Train Loss:  0.014382505789399147 | Validation Loss:  0.012099356390535831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  692 | Train Loss:  0.01438168529421091 | Validation Loss:  0.012098915874958038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  693 | Train Loss:  0.014380866661667824 | Validation Loss:  0.01209847442805767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  694 | Train Loss:  0.014380051754415035 | Validation Loss:  0.012098034843802452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  695 | Train Loss:  0.014379237778484821 | Validation Loss:  0.012097595259547234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  696 | Train Loss:  0.014378427527844906 | Validation Loss:  0.012097158469259739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  697 | Train Loss:  0.014377620071172714 | Validation Loss:  0.01209672074764967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  698 | Train Loss:  0.014376813545823097 | Validation Loss:  0.012096283957362175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  699 | Train Loss:  0.014376009814441204 | Validation Loss:  0.01209584716707468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  700 | Train Loss:  0.01437520980834961 | Validation Loss:  0.01209541317075491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  701 | Train Loss:  0.014374411664903164 | Validation Loss:  0.01209497731178999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  702 | Train Loss:  0.014373614452779293 | Validation Loss:  0.012094543315470219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  703 | Train Loss:  0.01437282096594572 | Validation Loss:  0.012094110250473022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  704 | Train Loss:  0.014372030273079872 | Validation Loss:  0.012093676254153252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  705 | Train Loss:  0.014371239580214024 | Validation Loss:  0.012093243189156055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  706 | Train Loss:  0.014370452612638474 | Validation Loss:  0.01209281012415886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  707 | Train Loss:  0.014369669370353222 | Validation Loss:  0.012092379853129387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  708 | Train Loss:  0.014368885196745396 | Validation Loss:  0.01209194678813219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  709 | Train Loss:  0.014368104748427868 | Validation Loss:  0.012091517448425293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  710 | Train Loss:  0.014367327094078064 | Validation Loss:  0.01209108717739582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  711 | Train Loss:  0.014366550371050835 | Validation Loss:  0.012090657837688923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  712 | Train Loss:  0.01436577644199133 | Validation Loss:  0.0120902294293046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  713 | Train Loss:  0.014365004375576973 | Validation Loss:  0.012089799158275127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  714 | Train Loss:  0.01436423510313034 | Validation Loss:  0.012089368887245655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  715 | Train Loss:  0.014363464899361134 | Validation Loss:  0.012088940478861332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  716 | Train Loss:  0.014362698420882225 | Validation Loss:  0.012088512070477009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  717 | Train Loss:  0.01436193473637104 | Validation Loss:  0.01208808459341526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  718 | Train Loss:  0.014361172914505005 | Validation Loss:  0.012087658047676086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  719 | Train Loss:  0.014360412023961544 | Validation Loss:  0.012087231501936913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  720 | Train Loss:  0.014359652996063232 | Validation Loss:  0.012086804956197739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  721 | Train Loss:  0.014358896762132645 | Validation Loss:  0.01208637747913599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  722 | Train Loss:  0.014358140528202057 | Validation Loss:  0.012085951864719391\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  723 | Train Loss:  0.014357388950884342 | Validation Loss:  0.012085525318980217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  724 | Train Loss:  0.014356637373566628 | Validation Loss:  0.012085099704563618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  725 | Train Loss:  0.014355887658894062 | Validation Loss:  0.012084674090147018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  726 | Train Loss:  0.014355139806866646 | Validation Loss:  0.012084249407052994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  727 | Train Loss:  0.014354393817484379 | Validation Loss:  0.01208382286131382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  728 | Train Loss:  0.014353649690747261 | Validation Loss:  0.012083398178219795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  729 | Train Loss:  0.014352905564010143 | Validation Loss:  0.01208297349512577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  730 | Train Loss:  0.01435216423124075 | Validation Loss:  0.012082548812031746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  731 | Train Loss:  0.01435142569243908 | Validation Loss:  0.012082125060260296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  732 | Train Loss:  0.014350688084959984 | Validation Loss:  0.012081700377166271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  733 | Train Loss:  0.014349950477480888 | Validation Loss:  0.012081276625394821\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  734 | Train Loss:  0.014349214732646942 | Validation Loss:  0.012080851942300797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  735 | Train Loss:  0.014348480850458145 | Validation Loss:  0.012080427259206772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  736 | Train Loss:  0.014347749762237072 | Validation Loss:  0.012080005370080471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  737 | Train Loss:  0.014347019605338573 | Validation Loss:  0.012079580686986446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  738 | Train Loss:  0.01434629037976265 | Validation Loss:  0.012079157866537571\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  739 | Train Loss:  0.01434556394815445 | Validation Loss:  0.012078733183443546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  740 | Train Loss:  0.01434483751654625 | Validation Loss:  0.012078310362994671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  741 | Train Loss:  0.014344112016260624 | Validation Loss:  0.01207788661122322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  742 | Train Loss:  0.014343388378620148 | Validation Loss:  0.012077463790774345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  743 | Train Loss:  0.014342667534947395 | Validation Loss:  0.012077040039002895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  744 | Train Loss:  0.014341945759952068 | Validation Loss:  0.01207661721855402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  745 | Train Loss:  0.014341228641569614 | Validation Loss:  0.01207619346678257\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  746 | Train Loss:  0.014340510591864586 | Validation Loss:  0.012075770646333694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  747 | Train Loss:  0.014339794404804707 | Validation Loss:  0.012075346894562244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  748 | Train Loss:  0.014339079149067402 | Validation Loss:  0.01207492221146822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  749 | Train Loss:  0.014338363893330097 | Validation Loss:  0.012074500322341919\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  750 | Train Loss:  0.014337651431560516 | Validation Loss:  0.012074076570570469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  751 | Train Loss:  0.01433694176375866 | Validation Loss:  0.012073652818799019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  752 | Train Loss:  0.014336231164634228 | Validation Loss:  0.012073230929672718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  753 | Train Loss:  0.014335522428154945 | Validation Loss:  0.012072806246578693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  754 | Train Loss:  0.014334814622998238 | Validation Loss:  0.012072381563484669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  755 | Train Loss:  0.014334108680486679 | Validation Loss:  0.012071959674358368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  756 | Train Loss:  0.01433340273797512 | Validation Loss:  0.012071534991264343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  757 | Train Loss:  0.014332698658108711 | Validation Loss:  0.012071112170815468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  758 | Train Loss:  0.014331995509564877 | Validation Loss:  0.012070688419044018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  759 | Train Loss:  0.014331294223666191 | Validation Loss:  0.012070265598595142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  760 | Train Loss:  0.014330592937767506 | Validation Loss:  0.012069842778146267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  761 | Train Loss:  0.01432989351451397 | Validation Loss:  0.012069416232407093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  762 | Train Loss:  0.014329195953905582 | Validation Loss:  0.012068993411958218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  763 | Train Loss:  0.014328496530652046 | Validation Loss:  0.012068566866219044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  764 | Train Loss:  0.014327801764011383 | Validation Loss:  0.012068144045770168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  765 | Train Loss:  0.01432710699737072 | Validation Loss:  0.012067720293998718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  766 | Train Loss:  0.014326411299407482 | Validation Loss:  0.012067294679582119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  767 | Train Loss:  0.014325719326734543 | Validation Loss:  0.012066869996488094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  768 | Train Loss:  0.014325026422739029 | Validation Loss:  0.01206644531339407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  769 | Train Loss:  0.014324336312711239 | Validation Loss:  0.01206601969897747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  770 | Train Loss:  0.014323645271360874 | Validation Loss:  0.012065595015883446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  771 | Train Loss:  0.014322957023978233 | Validation Loss:  0.012065170332789421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  772 | Train Loss:  0.014322267845273018 | Validation Loss:  0.012064742855727673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  773 | Train Loss:  0.014321580529212952 | Validation Loss:  0.012064319103956223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  774 | Train Loss:  0.01432089414447546 | Validation Loss:  0.012063892558217049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  775 | Train Loss:  0.014320208691060543 | Validation Loss:  0.012063466012477875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  776 | Train Loss:  0.014319523237645626 | Validation Loss:  0.012063040398061275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  777 | Train Loss:  0.014318839646875858 | Validation Loss:  0.012062613852322102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  778 | Train Loss:  0.01431815791875124 | Validation Loss:  0.012062186375260353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  779 | Train Loss:  0.014317476190626621 | Validation Loss:  0.012061760760843754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  780 | Train Loss:  0.014316795393824577 | Validation Loss:  0.012061333283782005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  781 | Train Loss:  0.014316114597022533 | Validation Loss:  0.012060905806720257\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  782 | Train Loss:  0.014315434731543064 | Validation Loss:  0.012060478329658508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  783 | Train Loss:  0.014314757660031319 | Validation Loss:  0.012060049921274185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  784 | Train Loss:  0.014314079657196999 | Validation Loss:  0.012059622444212437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  785 | Train Loss:  0.014313401654362679 | Validation Loss:  0.012059194967150688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  786 | Train Loss:  0.014312725514173508 | Validation Loss:  0.012058766558766365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  787 | Train Loss:  0.014312050305306911 | Validation Loss:  0.012058339081704617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  788 | Train Loss:  0.01431137602776289 | Validation Loss:  0.012057908810675144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  789 | Train Loss:  0.014310701750218868 | Validation Loss:  0.012057481333613396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  790 | Train Loss:  0.014310029335319996 | Validation Loss:  0.012057051993906498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  791 | Train Loss:  0.014309355989098549 | Validation Loss:  0.0120566226541996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  792 | Train Loss:  0.014308684505522251 | Validation Loss:  0.012056192383170128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  793 | Train Loss:  0.014308015815913677 | Validation Loss:  0.012055762112140656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  794 | Train Loss:  0.01430734433233738 | Validation Loss:  0.012055332772433758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  795 | Train Loss:  0.014306675642728806 | Validation Loss:  0.012054902501404285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  796 | Train Loss:  0.014306006953120232 | Validation Loss:  0.012054471299052238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  797 | Train Loss:  0.014305337332189083 | Validation Loss:  0.01205404195934534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  798 | Train Loss:  0.014304671436548233 | Validation Loss:  0.012053610756993294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  799 | Train Loss:  0.014304006472229958 | Validation Loss:  0.012053179554641247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  800 | Train Loss:  0.014303340576589108 | Validation Loss:  0.012052747420966625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  801 | Train Loss:  0.014302673749625683 | Validation Loss:  0.012052314355969429\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  802 | Train Loss:  0.014302009716629982 | Validation Loss:  0.012051882222294807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  803 | Train Loss:  0.014301345683634281 | Validation Loss:  0.012051450088620186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  804 | Train Loss:  0.01430068351328373 | Validation Loss:  0.012051017954945564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  805 | Train Loss:  0.014300020411610603 | Validation Loss:  0.012050584889948368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  806 | Train Loss:  0.014299360103905201 | Validation Loss:  0.012050151824951172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  807 | Train Loss:  0.014298697002232075 | Validation Loss:  0.012049718759953976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  808 | Train Loss:  0.014298034831881523 | Validation Loss:  0.012049284763634205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  809 | Train Loss:  0.01429737638682127 | Validation Loss:  0.012048850767314434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  810 | Train Loss:  0.014296717010438442 | Validation Loss:  0.012048414908349514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  811 | Train Loss:  0.01429605670273304 | Validation Loss:  0.012047980912029743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  812 | Train Loss:  0.014295399188995361 | Validation Loss:  0.012047545053064823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  813 | Train Loss:  0.014294740743935108 | Validation Loss:  0.012047111056745052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  814 | Train Loss:  0.014294084161520004 | Validation Loss:  0.012046675197780132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  815 | Train Loss:  0.014293426647782326 | Validation Loss:  0.012046238407492638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  816 | Train Loss:  0.014292770996689796 | Validation Loss:  0.012045804411172867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  817 | Train Loss:  0.014292115345597267 | Validation Loss:  0.012045365758240223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  818 | Train Loss:  0.014291459694504738 | Validation Loss:  0.012044929899275303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  819 | Train Loss:  0.014290804974734783 | Validation Loss:  0.012044493108987808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  820 | Train Loss:  0.014290152117609978 | Validation Loss:  0.012044056318700314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  821 | Train Loss:  0.014289498329162598 | Validation Loss:  0.012043618597090244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  822 | Train Loss:  0.014288844540715218 | Validation Loss:  0.012043179012835026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  823 | Train Loss:  0.014288191683590412 | Validation Loss:  0.012042741291224957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  824 | Train Loss:  0.014287539757788181 | Validation Loss:  0.012042302638292313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  825 | Train Loss:  0.014286888763308525 | Validation Loss:  0.012041863985359669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  826 | Train Loss:  0.014286237768828869 | Validation Loss:  0.01204142440110445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  827 | Train Loss:  0.014285586774349213 | Validation Loss:  0.012040984816849232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  828 | Train Loss:  0.014284936711192131 | Validation Loss:  0.012040546163916588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  829 | Train Loss:  0.01428428664803505 | Validation Loss:  0.012040105648338795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  830 | Train Loss:  0.014283638447523117 | Validation Loss:  0.012039665132761002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  831 | Train Loss:  0.014282988384366035 | Validation Loss:  0.012039224617183208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  832 | Train Loss:  0.014282342046499252 | Validation Loss:  0.01203878503292799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  833 | Train Loss:  0.014281692914664745 | Validation Loss:  0.012038342654705048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  834 | Train Loss:  0.014281044714152813 | Validation Loss:  0.012037902139127254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  835 | Train Loss:  0.014280399307608604 | Validation Loss:  0.012037456966936588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  836 | Train Loss:  0.014279751107096672 | Validation Loss:  0.012037016451358795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  837 | Train Loss:  0.014279106631875038 | Validation Loss:  0.012036574073135853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  838 | Train Loss:  0.01427845936268568 | Validation Loss:  0.012036129832267761\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  839 | Train Loss:  0.014277814887464046 | Validation Loss:  0.012035687454044819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  840 | Train Loss:  0.014277168549597263 | Validation Loss:  0.012035243213176727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  841 | Train Loss:  0.01427652407437563 | Validation Loss:  0.01203479990363121\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  842 | Train Loss:  0.01427588053047657 | Validation Loss:  0.012034355662763119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  843 | Train Loss:  0.01427523698657751 | Validation Loss:  0.012033911421895027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  844 | Train Loss:  0.014274593442678452 | Validation Loss:  0.012033466249704361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  845 | Train Loss:  0.014273949898779392 | Validation Loss:  0.012033021077513695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  846 | Train Loss:  0.014273308217525482 | Validation Loss:  0.012032574974000454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  847 | Train Loss:  0.014272665604948997 | Validation Loss:  0.012032129801809788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  848 | Train Loss:  0.014272022992372513 | Validation Loss:  0.012031684629619122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  849 | Train Loss:  0.014271380379796028 | Validation Loss:  0.012031239457428455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  850 | Train Loss:  0.014270740561187267 | Validation Loss:  0.012030791491270065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  851 | Train Loss:  0.014270100742578506 | Validation Loss:  0.012030343525111675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  852 | Train Loss:  0.014269459992647171 | Validation Loss:  0.01202989649027586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  853 | Train Loss:  0.014268819242715836 | Validation Loss:  0.01202944852411747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  854 | Train Loss:  0.0142681784927845 | Validation Loss:  0.01202900055795908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  855 | Train Loss:  0.014267540536820889 | Validation Loss:  0.01202855259180069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  856 | Train Loss:  0.014266900718212128 | Validation Loss:  0.012028103694319725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  857 | Train Loss:  0.014266261830925941 | Validation Loss:  0.012027655728161335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  858 | Train Loss:  0.01426562387496233 | Validation Loss:  0.012027204968035221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  859 | Train Loss:  0.014264985918998718 | Validation Loss:  0.012026757001876831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  860 | Train Loss:  0.014264348894357681 | Validation Loss:  0.012026306241750717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  861 | Train Loss:  0.01426370907574892 | Validation Loss:  0.012025856412947178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  862 | Train Loss:  0.014263072051107883 | Validation Loss:  0.012025405652821064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  863 | Train Loss:  0.014262435957789421 | Validation Loss:  0.012024953961372375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  864 | Train Loss:  0.014261799864470959 | Validation Loss:  0.012024503201246262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  865 | Train Loss:  0.014261162839829922 | Validation Loss:  0.012024052441120148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  866 | Train Loss:  0.01426052674651146 | Validation Loss:  0.012023599818348885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  867 | Train Loss:  0.014259891584515572 | Validation Loss:  0.012023148126900196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  868 | Train Loss:  0.01425925549119711 | Validation Loss:  0.012022694572806358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  869 | Train Loss:  0.014258620329201221 | Validation Loss:  0.01202224288135767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  870 | Train Loss:  0.014257986098527908 | Validation Loss:  0.012021790258586407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  871 | Train Loss:  0.01425735093653202 | Validation Loss:  0.012021335773169994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  872 | Train Loss:  0.014256717637181282 | Validation Loss:  0.012020883150398731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  873 | Train Loss:  0.014256083406507969 | Validation Loss:  0.012020428664982319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  874 | Train Loss:  0.014255448244512081 | Validation Loss:  0.012019975110888481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  875 | Train Loss:  0.014254815876483917 | Validation Loss:  0.012019520625472069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  876 | Train Loss:  0.014254183508455753 | Validation Loss:  0.012019065208733082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  877 | Train Loss:  0.014253548346459866 | Validation Loss:  0.01201860886067152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  878 | Train Loss:  0.014252915978431702 | Validation Loss:  0.012018154375255108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  879 | Train Loss:  0.014252284541726112 | Validation Loss:  0.012017698027193546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  880 | Train Loss:  0.014251651242375374 | Validation Loss:  0.01201724074780941\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  881 | Train Loss:  0.014251019805669785 | Validation Loss:  0.012016784399747849\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  882 | Train Loss:  0.014250388368964195 | Validation Loss:  0.012016328051686287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  883 | Train Loss:  0.014249756932258606 | Validation Loss:  0.012015871703624725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  884 | Train Loss:  0.014249125495553017 | Validation Loss:  0.01201541442424059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  885 | Train Loss:  0.014248494058847427 | Validation Loss:  0.012014955282211304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  886 | Train Loss:  0.014247863553464413 | Validation Loss:  0.012014498002827168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  887 | Train Loss:  0.014247232116758823 | Validation Loss:  0.012014039792120457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  888 | Train Loss:  0.014246601611375809 | Validation Loss:  0.012013580650091171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  889 | Train Loss:  0.014245972037315369 | Validation Loss:  0.01201312243938446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  890 | Train Loss:  0.014245341531932354 | Validation Loss:  0.012012663297355175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  891 | Train Loss:  0.014244711957871914 | Validation Loss:  0.01201220229268074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  892 | Train Loss:  0.014244082383811474 | Validation Loss:  0.01201174221932888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  893 | Train Loss:  0.014243453741073608 | Validation Loss:  0.012011283077299595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  894 | Train Loss:  0.014242824167013168 | Validation Loss:  0.01201082207262516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  895 | Train Loss:  0.014242194592952728 | Validation Loss:  0.012010362930595875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  896 | Train Loss:  0.014241567812860012 | Validation Loss:  0.012009900994598866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  897 | Train Loss:  0.014240937307476997 | Validation Loss:  0.012009439058601856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  898 | Train Loss:  0.014240308664739132 | Validation Loss:  0.012008977122604847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  899 | Train Loss:  0.014239680022001266 | Validation Loss:  0.012008516117930412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  900 | Train Loss:  0.014239054173231125 | Validation Loss:  0.012008052319288254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  901 | Train Loss:  0.014238426461815834 | Validation Loss:  0.012007591314613819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  902 | Train Loss:  0.014237797819077969 | Validation Loss:  0.01200712751597166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  903 | Train Loss:  0.014237171038985252 | Validation Loss:  0.012006663717329502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  904 | Train Loss:  0.014236543327569962 | Validation Loss:  0.012006201781332493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  905 | Train Loss:  0.014235916547477245 | Validation Loss:  0.01200573705136776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  906 | Train Loss:  0.014235289767384529 | Validation Loss:  0.012005273252725601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  907 | Train Loss:  0.014234662055969238 | Validation Loss:  0.012004808522760868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  908 | Train Loss:  0.014234035275876522 | Validation Loss:  0.012004345655441284\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  909 | Train Loss:  0.014233410358428955 | Validation Loss:  0.012003879062831402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  910 | Train Loss:  0.014232782647013664 | Validation Loss:  0.012003413401544094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  911 | Train Loss:  0.014232157729566097 | Validation Loss:  0.012002947740256786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  912 | Train Loss:  0.014231531880795956 | Validation Loss:  0.012002483010292053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  913 | Train Loss:  0.014230906032025814 | Validation Loss:  0.012002017349004745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  914 | Train Loss:  0.014230279251933098 | Validation Loss:  0.012001549825072289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  915 | Train Loss:  0.014229655265808105 | Validation Loss:  0.01200108416378498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  916 | Train Loss:  0.014229029417037964 | Validation Loss:  0.012000618502497673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  917 | Train Loss:  0.014228404499590397 | Validation Loss:  0.012000150047242641\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  918 | Train Loss:  0.01422777958214283 | Validation Loss:  0.011999682523310184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  919 | Train Loss:  0.014227155596017838 | Validation Loss:  0.011999214999377728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  920 | Train Loss:  0.01422653067857027 | Validation Loss:  0.011998745612800121\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  921 | Train Loss:  0.014225905761122704 | Validation Loss:  0.011998278088867664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  922 | Train Loss:  0.014225281774997711 | Validation Loss:  0.011997808702290058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  923 | Train Loss:  0.014224656857550144 | Validation Loss:  0.011997341178357601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  924 | Train Loss:  0.014224032871425152 | Validation Loss:  0.01199687086045742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  925 | Train Loss:  0.01422340888530016 | Validation Loss:  0.011996401473879814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  926 | Train Loss:  0.014222785830497742 | Validation Loss:  0.011995933018624783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  927 | Train Loss:  0.014222162775695324 | Validation Loss:  0.011995462700724602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  928 | Train Loss:  0.014221539720892906 | Validation Loss:  0.011994992382824421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  929 | Train Loss:  0.01422091480344534 | Validation Loss:  0.011994521133601665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  930 | Train Loss:  0.014220292679965496 | Validation Loss:  0.01199405174702406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  931 | Train Loss:  0.014219668693840504 | Validation Loss:  0.01199357956647873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  932 | Train Loss:  0.014219045639038086 | Validation Loss:  0.011993108317255974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  933 | Train Loss:  0.014218423515558243 | Validation Loss:  0.011992638930678368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  934 | Train Loss:  0.01421779952943325 | Validation Loss:  0.011992165818810463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  935 | Train Loss:  0.014217179268598557 | Validation Loss:  0.011991693638265133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  936 | Train Loss:  0.014216556213796139 | Validation Loss:  0.011991220526397228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  937 | Train Loss:  0.014215933158993721 | Validation Loss:  0.011990747414529324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  938 | Train Loss:  0.014215311035513878 | Validation Loss:  0.011990275233983994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  939 | Train Loss:  0.01421468798071146 | Validation Loss:  0.011989802122116089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  940 | Train Loss:  0.014214067719876766 | Validation Loss:  0.011989329941570759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  941 | Train Loss:  0.014213446527719498 | Validation Loss:  0.01198885589838028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  942 | Train Loss:  0.01421282347291708 | Validation Loss:  0.0119883818551898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  943 | Train Loss:  0.014212201349437237 | Validation Loss:  0.011987907811999321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  944 | Train Loss:  0.014211581088602543 | Validation Loss:  0.011987433768808842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  945 | Train Loss:  0.0142109589651227 | Validation Loss:  0.011986957862973213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  946 | Train Loss:  0.014210337772965431 | Validation Loss:  0.011986483819782734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  947 | Train Loss:  0.014209717512130737 | Validation Loss:  0.011986007913947105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  948 | Train Loss:  0.014209095388650894 | Validation Loss:  0.011985532939434052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  949 | Train Loss:  0.014208476059138775 | Validation Loss:  0.011985057033598423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  950 | Train Loss:  0.014207853935658932 | Validation Loss:  0.011984582059085369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  951 | Train Loss:  0.014207232743501663 | Validation Loss:  0.011984105221927166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  952 | Train Loss:  0.014206613413989544 | Validation Loss:  0.011983628384768963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  953 | Train Loss:  0.014205992221832275 | Validation Loss:  0.01198315154761076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  954 | Train Loss:  0.014205371029675007 | Validation Loss:  0.011982675641775131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  955 | Train Loss:  0.014204750768840313 | Validation Loss:  0.011982196010649204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  956 | Train Loss:  0.014204130508005619 | Validation Loss:  0.011981719173491001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  957 | Train Loss:  0.014203510247170925 | Validation Loss:  0.011981241405010223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  958 | Train Loss:  0.014202889986336231 | Validation Loss:  0.01198076456785202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  959 | Train Loss:  0.014202269725501537 | Validation Loss:  0.011980285868048668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  960 | Train Loss:  0.014201650395989418 | Validation Loss:  0.011979806236922741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  961 | Train Loss:  0.014201031066477299 | Validation Loss:  0.011979328468441963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  962 | Train Loss:  0.014200410805642605 | Validation Loss:  0.011978850699961185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  963 | Train Loss:  0.014199791476130486 | Validation Loss:  0.011978372000157833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  964 | Train Loss:  0.01419917307794094 | Validation Loss:  0.011977891437709332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  965 | Train Loss:  0.014198552817106247 | Validation Loss:  0.011977411806583405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  966 | Train Loss:  0.014197933487594128 | Validation Loss:  0.011976930312812328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  967 | Train Loss:  0.014197314158082008 | Validation Loss:  0.01197645254433155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  968 | Train Loss:  0.014196693897247314 | Validation Loss:  0.011975972913205624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  969 | Train Loss:  0.014196076430380344 | Validation Loss:  0.011975490488111973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  970 | Train Loss:  0.01419545616954565 | Validation Loss:  0.011975010856986046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  971 | Train Loss:  0.014194837771356106 | Validation Loss:  0.011974528431892395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  972 | Train Loss:  0.014194217510521412 | Validation Loss:  0.011974048800766468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  973 | Train Loss:  0.014193599112331867 | Validation Loss:  0.011973568238317966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  974 | Train Loss:  0.014192980714142323 | Validation Loss:  0.011973085813224316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  975 | Train Loss:  0.014192362315952778 | Validation Loss:  0.011972603388130665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  976 | Train Loss:  0.014191743917763233 | Validation Loss:  0.011972121894359589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  977 | Train Loss:  0.014191124588251114 | Validation Loss:  0.011971638537943363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  978 | Train Loss:  0.01419050619006157 | Validation Loss:  0.011971157975494862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  979 | Train Loss:  0.014189887791872025 | Validation Loss:  0.011970673687756062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  980 | Train Loss:  0.01418926939368248 | Validation Loss:  0.01197019126266241\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  981 | Train Loss:  0.01418865192681551 | Validation Loss:  0.011969707906246185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  982 | Train Loss:  0.014188033528625965 | Validation Loss:  0.01196922268718481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  983 | Train Loss:  0.014187414199113846 | Validation Loss:  0.01196874026209116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  984 | Train Loss:  0.01418679766356945 | Validation Loss:  0.011968256905674934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  985 | Train Loss:  0.014186178334057331 | Validation Loss:  0.01196777168661356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  986 | Train Loss:  0.014185560867190361 | Validation Loss:  0.011967288330197334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  987 | Train Loss:  0.014184943400323391 | Validation Loss:  0.011966804042458534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  988 | Train Loss:  0.014184325933456421 | Validation Loss:  0.01196631882339716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  989 | Train Loss:  0.01418370846658945 | Validation Loss:  0.01196583453565836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  990 | Train Loss:  0.01418309099972248 | Validation Loss:  0.011965349316596985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  991 | Train Loss:  0.01418247353285551 | Validation Loss:  0.01196486409753561\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  992 | Train Loss:  0.01418185606598854 | Validation Loss:  0.011964377015829086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  993 | Train Loss:  0.01418123859912157 | Validation Loss:  0.011963892728090286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  994 | Train Loss:  0.0141806211322546 | Validation Loss:  0.011963405646383762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  995 | Train Loss:  0.01418000366538763 | Validation Loss:  0.011962920427322388\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  996 | Train Loss:  0.01417938619852066 | Validation Loss:  0.011962434276938438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  997 | Train Loss:  0.01417876873165369 | Validation Loss:  0.01196194812655449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  998 | Train Loss:  0.01417815126478672 | Validation Loss:  0.011961461044847965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  999 | Train Loss:  0.014177534729242325 | Validation Loss:  0.011960973963141441\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Test Loss:  0.012044437229633331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  0 | Train Loss:  0.11612656712532043 | Validation Loss:  0.09686417877674103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  1 | Train Loss:  0.09574763476848602 | Validation Loss:  0.032469313591718674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  2 | Train Loss:  0.03645578771829605 | Validation Loss:  0.011722801253199577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  3 | Train Loss:  0.014066985808312893 | Validation Loss:  0.010737797245383263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  4 | Train Loss:  0.012735322117805481 | Validation Loss:  0.01009513158351183\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  5 | Train Loss:  0.011922561563551426 | Validation Loss:  0.009427029639482498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  6 | Train Loss:  0.011121063493192196 | Validation Loss:  0.008737479336559772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  7 | Train Loss:  0.010301357135176659 | Validation Loss:  0.00802993681281805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  8 | Train Loss:  0.009461159817874432 | Validation Loss:  0.0073086656630039215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  9 | Train Loss:  0.00860481709241867 | Validation Loss:  0.006582043133676052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  10 | Train Loss:  0.007742190267890692 | Validation Loss:  0.005862774793058634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  11 | Train Loss:  0.006888082716614008 | Validation Loss:  0.005167140159755945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  12 | Train Loss:  0.006061275489628315 | Validation Loss:  0.004513521678745747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  13 | Train Loss:  0.0052828602492809296 | Validation Loss:  0.003920216578990221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  14 | Train Loss:  0.0045737894251942635 | Validation Loss:  0.0034027581568807364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  15 | Train Loss:  0.003951928112655878 | Validation Loss:  0.0029713036492466927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  16 | Train Loss:  0.0034291348420083523 | Validation Loss:  0.0026288607623428106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  17 | Train Loss:  0.003009258536621928 | Validation Loss:  0.002370963804423809\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  18 | Train Loss:  0.0026877280324697495 | Validation Loss:  0.0021870031487196684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  19 | Train Loss:  0.002452968619763851 | Validation Loss:  0.0020627377089112997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  20 | Train Loss:  0.0022891529370099306 | Validation Loss:  0.0019831124227494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  21 | Train Loss:  0.0021793388295918703 | Validation Loss:  0.0019345271866768599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  22 | Train Loss:  0.002108020009472966 | Validation Loss:  0.001906085410155356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  23 | Train Loss:  0.002062583575025201 | Validation Loss:  0.0018898820271715522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  24 | Train Loss:  0.0020336885936558247 | Validation Loss:  0.0018806345760822296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  25 | Train Loss:  0.0020149245392531157 | Validation Loss:  0.0018750721355900168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  26 | Train Loss:  0.0020021554082632065 | Validation Loss:  0.001871280837804079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  27 | Train Loss:  0.001992839388549328 | Validation Loss:  0.0018682422814890742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  28 | Train Loss:  0.0019854758866131306 | Validation Loss:  0.0018654054729267955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  29 | Train Loss:  0.0019792048260569572 | Validation Loss:  0.001862586010247469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  30 | Train Loss:  0.0019735461100935936 | Validation Loss:  0.0018596010049805045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  31 | Train Loss:  0.001968235708773136 | Validation Loss:  0.0018566105281934142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  32 | Train Loss:  0.0019631304312497377 | Validation Loss:  0.001853298395872116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  33 | Train Loss:  0.0019581536762416363 | Validation Loss:  0.0018503167666494846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  34 | Train Loss:  0.0019532646983861923 | Validation Loss:  0.0018464927561581135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  35 | Train Loss:  0.0019484427757561207 | Validation Loss:  0.0018441701540723443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  36 | Train Loss:  0.0019436796428635716 | Validation Loss:  0.0018388654571026564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  37 | Train Loss:  0.0019389870576560497 | Validation Loss:  0.0018395580118522048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  38 | Train Loss:  0.0019344326574355364 | Validation Loss:  0.0018288802821189165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  39 | Train Loss:  0.0019303082954138517 | Validation Loss:  0.001843475503847003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  40 | Train Loss:  0.0019278405234217644 | Validation Loss:  0.001816886360757053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  41 | Train Loss:  0.0019322525477036834 | Validation Loss:  0.001917189103551209\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  42 | Train Loss:  0.001965764444321394 | Validation Loss:  0.0019393130205571651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  43 | Train Loss:  0.002123975194990635 | Validation Loss:  0.0028510470874607563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  44 | Train Loss:  0.002764870645478368 | Validation Loss:  0.004436159506440163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  45 | Train Loss:  0.0049049765802919865 | Validation Loss:  0.008675340563058853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  46 | Train Loss:  0.008305611088871956 | Validation Loss:  0.01039736159145832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  47 | Train Loss:  0.011300243437290192 | Validation Loss:  0.0070024896413087845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  48 | Train Loss:  0.006726302206516266 | Validation Loss:  0.0041813175193965435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  49 | Train Loss:  0.004790603648871183 | Validation Loss:  0.0029225775506347418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  50 | Train Loss:  0.002910878276452422 | Validation Loss:  0.002109528286382556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  51 | Train Loss:  0.0024177986197173595 | Validation Loss:  0.002085478510707617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  52 | Train Loss:  0.0021509944926947355 | Validation Loss:  0.001844986923970282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  53 | Train Loss:  0.0020402001682668924 | Validation Loss:  0.0018975212005898356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  54 | Train Loss:  0.001976446248590946 | Validation Loss:  0.001796080032363534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  55 | Train Loss:  0.0019422924378886819 | Validation Loss:  0.0018418291583657265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  56 | Train Loss:  0.0019209018209949136 | Validation Loss:  0.001786186476238072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  57 | Train Loss:  0.0019077742472290993 | Validation Loss:  0.0018215577583760023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  58 | Train Loss:  0.0018986959476023912 | Validation Loss:  0.0017843995010480285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  59 | Train Loss:  0.0018923149909824133 | Validation Loss:  0.0018120495369657874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  60 | Train Loss:  0.0018872978398576379 | Validation Loss:  0.0017835197504609823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  61 | Train Loss:  0.0018832390196621418 | Validation Loss:  0.001806052285246551\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  62 | Train Loss:  0.0018796613439917564 | Validation Loss:  0.0017819867935031652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  63 | Train Loss:  0.0018764602718874812 | Validation Loss:  0.0018013633089140058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  64 | Train Loss:  0.0018734425539150834 | Validation Loss:  0.0017796751344576478\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  65 | Train Loss:  0.0018705993425101042 | Validation Loss:  0.0017973361536860466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  66 | Train Loss:  0.001867837505415082 | Validation Loss:  0.0017767478711903095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  67 | Train Loss:  0.0018651792779564857 | Validation Loss:  0.0017938195960596204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  68 | Train Loss:  0.0018625699449330568 | Validation Loss:  0.0017733668209984899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  69 | Train Loss:  0.0018600447801873088 | Validation Loss:  0.0017908457666635513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  70 | Train Loss:  0.0018575647845864296 | Validation Loss:  0.0017696395516395569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  71 | Train Loss:  0.0018551766406744719 | Validation Loss:  0.0017885577399283648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  72 | Train Loss:  0.0018528493819758296 | Validation Loss:  0.0017656361451372504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  73 | Train Loss:  0.0018506497144699097 | Validation Loss:  0.0017872380558401346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  74 | Train Loss:  0.0018485510954633355 | Validation Loss:  0.0017614258686080575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  75 | Train Loss:  0.001846660627052188 | Validation Loss:  0.0017874111654236913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  76 | Train Loss:  0.0018449585186317563 | Validation Loss:  0.0017571629723533988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  77 | Train Loss:  0.0018436377868056297 | Validation Loss:  0.0017900759121403098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  78 | Train Loss:  0.0018426956376060843 | Validation Loss:  0.001753286924213171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  79 | Train Loss:  0.0018425164744257927 | Validation Loss:  0.0017972263740375638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  80 | Train Loss:  0.0018431408097967505 | Validation Loss:  0.0017510635079815984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  81 | Train Loss:  0.0018454192904755473 | Validation Loss:  0.0018130760872736573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  82 | Train Loss:  0.0018494894029572606 | Validation Loss:  0.0017540630651637912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  83 | Train Loss:  0.0018574065761640668 | Validation Loss:  0.0018470602808520198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  84 | Train Loss:  0.0018694837344810367 | Validation Loss:  0.001772295101545751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  85 | Train Loss:  0.0018910932121798396 | Validation Loss:  0.0019213146297261119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  86 | Train Loss:  0.001922515337355435 | Validation Loss:  0.001833440619520843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  87 | Train Loss:  0.001978679560124874 | Validation Loss:  0.0020881739910691977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  88 | Train Loss:  0.002056844299659133 | Validation Loss:  0.0020095284562557936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  89 | Train Loss:  0.0021995108108967543 | Validation Loss:  0.0024588098749518394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  90 | Train Loss:  0.002379288896918297 | Validation Loss:  0.0024496561381965876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  91 | Train Loss:  0.002711460692808032 | Validation Loss:  0.003171548480167985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  92 | Train Loss:  0.0030299564823508263 | Validation Loss:  0.003254570299759507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  93 | Train Loss:  0.0036127192433923483 | Validation Loss:  0.004020553082227707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  94 | Train Loss:  0.0038263832684606314 | Validation Loss:  0.0039003973361104727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  95 | Train Loss:  0.004334440920501947 | Validation Loss:  0.004144457168877125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  96 | Train Loss:  0.003949197940528393 | Validation Loss:  0.0035512028262019157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  97 | Train Loss:  0.003973572980612516 | Validation Loss:  0.0034457233268767595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  98 | Train Loss:  0.003299838164821267 | Validation Loss:  0.0027611232362687588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  99 | Train Loss:  0.0031044050119817257 | Validation Loss:  0.002754239598289132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  100 | Train Loss:  0.0026615771930664778 | Validation Loss:  0.002248436212539673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  101 | Train Loss:  0.0025131902657449245 | Validation Loss:  0.0023521140683442354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  102 | Train Loss:  0.002294811187312007 | Validation Loss:  0.0020048420410603285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  103 | Train Loss:  0.0022152161691337824 | Validation Loss:  0.0021461278665810823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  104 | Train Loss:  0.0021092051174491644 | Validation Loss:  0.0018930936930701137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  105 | Train Loss:  0.0020694048143923283 | Validation Loss:  0.0020399338100105524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  106 | Train Loss:  0.002014388795942068 | Validation Loss:  0.0018394950311630964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  107 | Train Loss:  0.0019950096029788256 | Validation Loss:  0.0019838963635265827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  108 | Train Loss:  0.0019643513951450586 | Validation Loss:  0.001812971313484013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  109 | Train Loss:  0.0019561320077627897 | Validation Loss:  0.001955588348209858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  110 | Train Loss:  0.0019384458428248763 | Validation Loss:  0.001800961559638381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  111 | Train Loss:  0.0019374549156054854 | Validation Loss:  0.0019453609129413962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  112 | Train Loss:  0.001927824690937996 | Validation Loss:  0.0017987421015277505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  113 | Train Loss:  0.0019328234484419227 | Validation Loss:  0.0019497318426147103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  114 | Train Loss:  0.0019293037476018071 | Validation Loss:  0.001805377658456564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  115 | Train Loss:  0.0019406818319112062 | Validation Loss:  0.001968746306374669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  116 | Train Loss:  0.0019429430831223726 | Validation Loss:  0.0018223939696326852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  117 | Train Loss:  0.0019624598789960146 | Validation Loss:  0.0020050769671797752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  118 | Train Loss:  0.0019712618086487055 | Validation Loss:  0.0018535708077251911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  119 | Train Loss:  0.0020021889358758926 | Validation Loss:  0.0020637528505176306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  120 | Train Loss:  0.002019070088863373 | Validation Loss:  0.001905021257698536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  121 | Train Loss:  0.002066422486677766 | Validation Loss:  0.0021515563130378723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  122 | Train Loss:  0.002093038521707058 | Validation Loss:  0.0019845175556838512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  123 | Train Loss:  0.0021632928401231766 | Validation Loss:  0.002274546306580305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  124 | Train Loss:  0.002199528506025672 | Validation Loss:  0.002098051831126213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  125 | Train Loss:  0.002298556035384536 | Validation Loss:  0.0024314329493790865\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  126 | Train Loss:  0.0023384438827633858 | Validation Loss:  0.0022411386016756296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  127 | Train Loss:  0.002466045320034027 | Validation Loss:  0.0026025520637631416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  128 | Train Loss:  0.0024926348123699427 | Validation Loss:  0.0023869075812399387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  129 | Train Loss:  0.0026349565014243126 | Validation Loss:  0.0027436886448413134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  130 | Train Loss:  0.00262149004265666 | Validation Loss:  0.0024862620048224926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  131 | Train Loss:  0.0027504004538059235 | Validation Loss:  0.0028037673328071833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  132 | Train Loss:  0.0026770757976919413 | Validation Loss:  0.002497720532119274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  133 | Train Loss:  0.0027659942861646414 | Validation Loss:  0.002763872966170311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  134 | Train Loss:  0.0026410063728690147 | Validation Loss:  0.0024243860971182585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  135 | Train Loss:  0.0026843028608709574 | Validation Loss:  0.0026529738679528236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  136 | Train Loss:  0.0025397075805813074 | Validation Loss:  0.002308397786691785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  137 | Train Loss:  0.002551587764173746 | Validation Loss:  0.0025193176697939634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  138 | Train Loss:  0.0024177180603146553 | Validation Loss:  0.0021924760658293962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  139 | Train Loss:  0.0024161403998732567 | Validation Loss:  0.0023976454976946115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  140 | Train Loss:  0.0023069633170962334 | Validation Loss:  0.0020982136484235525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  141 | Train Loss:  0.002303635235875845 | Validation Loss:  0.002301913918927312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  142 | Train Loss:  0.00222003017552197 | Validation Loss:  0.002029791008681059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  143 | Train Loss:  0.002220208290964365 | Validation Loss:  0.002233307110145688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  144 | Train Loss:  0.0021577246952801943 | Validation Loss:  0.0019839927554130554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  145 | Train Loss:  0.0021631638519465923 | Validation Loss:  0.00218838918954134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  146 | Train Loss:  0.0021166843362152576 | Validation Loss:  0.001956323627382517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  147 | Train Loss:  0.0021278606727719307 | Validation Loss:  0.002163264900445938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  148 | Train Loss:  0.0020932205952703953 | Validation Loss:  0.0019432274857535958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  149 | Train Loss:  0.0021103983744978905 | Validation Loss:  0.002154945395886898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  150 | Train Loss:  0.0020845597609877586 | Validation Loss:  0.001942450413480401\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  151 | Train Loss:  0.0021081746090203524 | Validation Loss:  0.0021614707075059414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  152 | Train Loss:  0.002088946755975485 | Validation Loss:  0.0019527742406353354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  153 | Train Loss:  0.00211966666392982 | Validation Loss:  0.0021815572399646044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  154 | Train Loss:  0.002105309860780835 | Validation Loss:  0.001973536564037204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  155 | Train Loss:  0.0021439173724502325 | Validation Loss:  0.002214029897004366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  156 | Train Loss:  0.0021327349822968245 | Validation Loss:  0.0020040159579366446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  157 | Train Loss:  0.0021798526868224144 | Validation Loss:  0.0022570814471691847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  158 | Train Loss:  0.002169777173548937 | Validation Loss:  0.0020426607225090265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  159 | Train Loss:  0.0022254257928580046 | Validation Loss:  0.002307451330125332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  160 | Train Loss:  0.0022136743646115065 | Validation Loss:  0.0020862899255007505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  161 | Train Loss:  0.0022767377085983753 | Validation Loss:  0.0023598161060363054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  162 | Train Loss:  0.0022597445640712976 | Validation Loss:  0.0021296427585184574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  163 | Train Loss:  0.002327574649825692 | Validation Loss:  0.00240694684907794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  164 | Train Loss:  0.002301474567502737 | Validation Loss:  0.002165890298783779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  165 | Train Loss:  0.0023700122255831957 | Validation Loss:  0.0024411145132035017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  166 | Train Loss:  0.002331784926354885 | Validation Loss:  0.0021884518209844828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  167 | Train Loss:  0.00239646527916193 | Validation Loss:  0.0024565423373132944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  168 | Train Loss:  0.002345297485589981 | Validation Loss:  0.0021934702526777983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  169 | Train Loss:  0.002402459504082799 | Validation Loss:  0.0024516189005225897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  170 | Train Loss:  0.002340424107387662 | Validation Loss:  0.0021814010106027126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  171 | Train Loss:  0.002388422843068838 | Validation Loss:  0.002429370302706957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  172 | Train Loss:  0.002319843042641878 | Validation Loss:  0.0021565533243119717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  173 | Train Loss:  0.0023592100478708744 | Validation Loss:  0.002395899733528495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  174 | Train Loss:  0.0022890798281878233 | Validation Loss:  0.0021250133868306875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  175 | Train Loss:  0.002321797888725996 | Validation Loss:  0.0023579862900078297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  176 | Train Loss:  0.002254287712275982 | Validation Loss:  0.0020924690179526806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  177 | Train Loss:  0.0022828239016234875 | Validation Loss:  0.002321254927664995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  178 | Train Loss:  0.002220560098066926 | Validation Loss:  0.0020630185026675463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  179 | Train Loss:  0.002247181488201022 | Validation Loss:  0.0022894812282174826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  180 | Train Loss:  0.002191290259361267 | Validation Loss:  0.002039026003330946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  181 | Train Loss:  0.002217791276052594 | Validation Loss:  0.0022647343575954437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  182 | Train Loss:  0.002168316161260009 | Validation Loss:  0.002021526452153921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  183 | Train Loss:  0.0021960162557661533 | Validation Loss:  0.0022478417959064245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  184 | Train Loss:  0.0021523553878068924 | Validation Loss:  0.002010715426877141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  185 | Train Loss:  0.0021821982227265835 | Validation Loss:  0.0022388407960534096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  186 | Train Loss:  0.002143425866961479 | Validation Loss:  0.00200631539337337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  187 | Train Loss:  0.002176075940951705 | Validation Loss:  0.0022372708190232515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  188 | Train Loss:  0.0021411157213151455 | Validation Loss:  0.002007764531299472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  189 | Train Loss:  0.002177008893340826 | Validation Loss:  0.0022423104383051395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  190 | Train Loss:  0.002144705969840288 | Validation Loss:  0.002014270517975092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  191 | Train Loss:  0.0021840501576662064 | Validation Loss:  0.0022528013214468956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  192 | Train Loss:  0.0021531886886805296 | Validation Loss:  0.0020247825887054205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  193 | Train Loss:  0.0021959298755973577 | Validation Loss:  0.0022672340273857117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  194 | Train Loss:  0.0021652448922395706 | Validation Loss:  0.0020379519555717707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  195 | Train Loss:  0.0022110200952738523 | Validation Loss:  0.002283749170601368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  196 | Train Loss:  0.002179235452786088 | Validation Loss:  0.002052134135738015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  197 | Train Loss:  0.0022273508366197348 | Validation Loss:  0.0023002284578979015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  198 | Train Loss:  0.002193274674937129 | Validation Loss:  0.0020654932595789433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  199 | Train Loss:  0.0022427404765039682 | Validation Loss:  0.002314506098628044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  200 | Train Loss:  0.0022054158616811037 | Validation Loss:  0.0020762344356626272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  201 | Train Loss:  0.002255063969641924 | Validation Loss:  0.0023246901109814644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  202 | Train Loss:  0.0022139425855129957 | Validation Loss:  0.002082912717014551\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  203 | Train Loss:  0.0022626123391091824 | Validation Loss:  0.0023295057471841574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  204 | Train Loss:  0.0022176839411258698 | Validation Loss:  0.002084726933389902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  205 | Train Loss:  0.0022644272539764643 | Validation Loss:  0.002328533213585615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  206 | Train Loss:  0.0022162392269819975 | Validation Loss:  0.0020816707983613014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  207 | Train Loss:  0.0022604793775826693 | Validation Loss:  0.002322245156392455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  208 | Train Loss:  0.002210016828030348 | Validation Loss:  0.002074478194117546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  209 | Train Loss:  0.0022516041062772274 | Validation Loss:  0.0023118287790566683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  210 | Train Loss:  0.0022000751923769712 | Validation Loss:  0.002064388245344162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  211 | Train Loss:  0.0022392396349459887 | Validation Loss:  0.0022988684941083193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  212 | Train Loss:  0.0021878366824239492 | Validation Loss:  0.002052842639386654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  213 | Train Loss:  0.002225075149908662 | Validation Loss:  0.0022850176319479942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  214 | Train Loss:  0.0021747874561697245 | Validation Loss:  0.0020412083249539137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  215 | Train Loss:  0.0022107302211225033 | Validation Loss:  0.0022717409301549196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  216 | Train Loss:  0.0021622420754283667 | Validation Loss:  0.002030601492151618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  217 | Train Loss:  0.0021975431591272354 | Validation Loss:  0.0022601706441491842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  218 | Train Loss:  0.002151216845959425 | Validation Loss:  0.0020218142308294773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  219 | Train Loss:  0.0021864783484488726 | Validation Loss:  0.002251060912385583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  220 | Train Loss:  0.0021423851139843464 | Validation Loss:  0.0020153119694441557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  221 | Train Loss:  0.0021781118120998144 | Validation Loss:  0.002244798466563225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  222 | Train Loss:  0.0021360956598073244 | Validation Loss:  0.002011270262300968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  223 | Train Loss:  0.0021726752165704966 | Validation Loss:  0.0022414512932300568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  224 | Train Loss:  0.00213240971788764 | Validation Loss:  0.0020096199586987495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  225 | Train Loss:  0.0021701022051274776 | Validation Loss:  0.002240810776129365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  226 | Train Loss:  0.002131147077307105 | Validation Loss:  0.002010090509429574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  227 | Train Loss:  0.0021700782235711813 | Validation Loss:  0.0022424461785703897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  228 | Train Loss:  0.0021319289226084948 | Validation Loss:  0.0020122460555285215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  229 | Train Loss:  0.002172088483348489 | Validation Loss:  0.0022457465529441833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  230 | Train Loss:  0.0021342148538678885 | Validation Loss:  0.0020155247766524553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  231 | Train Loss:  0.002175465226173401 | Validation Loss:  0.002249983372166753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  232 | Train Loss:  0.0021373589988797903 | Validation Loss:  0.002019289182499051\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  233 | Train Loss:  0.0021794461645185947 | Validation Loss:  0.0022543752565979958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  234 | Train Loss:  0.002140665892511606 | Validation Loss:  0.0020228836219757795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  235 | Train Loss:  0.002183249918743968 | Validation Loss:  0.0022581720259040594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  236 | Train Loss:  0.0021434626542031765 | Validation Loss:  0.0020257094874978065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  237 | Train Loss:  0.002186160534620285 | Validation Loss:  0.0022607396822422743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  238 | Train Loss:  0.00214517954736948 | Validation Loss:  0.002027295297011733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  239 | Train Loss:  0.0021876124665141106 | Validation Loss:  0.0022616374772042036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  240 | Train Loss:  0.0021454165689647198 | Validation Loss:  0.0020273569971323013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  241 | Train Loss:  0.002187262289226055 | Validation Loss:  0.0022606647107750177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  242 | Train Loss:  0.002143988385796547 | Validation Loss:  0.002025823574513197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  243 | Train Loss:  0.0021850173361599445 | Validation Loss:  0.0022578707430511713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  244 | Train Loss:  0.0021409345790743828 | Validation Loss:  0.0020228319335728884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  245 | Train Loss:  0.002181032206863165 | Validation Loss:  0.002253526821732521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  246 | Train Loss:  0.0021364938002079725 | Validation Loss:  0.002018686616793275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  247 | Train Loss:  0.0021756617352366447 | Validation Loss:  0.0022480618208646774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  248 | Train Loss:  0.0021310478914529085 | Validation Loss:  0.0020137971732765436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  249 | Train Loss:  0.0021693871822208166 | Validation Loss:  0.002241983078420162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  250 | Train Loss:  0.002125050639733672 | Validation Loss:  0.0020086062140762806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  251 | Train Loss:  0.0021627333480864763 | Validation Loss:  0.0022358058486133814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  252 | Train Loss:  0.002118961652740836 | Validation Loss:  0.002003532135859132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  253 | Train Loss:  0.0021562003530561924 | Validation Loss:  0.0022299860138446093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  254 | Train Loss:  0.002113190246745944 | Validation Loss:  0.0019989237189292908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  255 | Train Loss:  0.002150207292288542 | Validation Loss:  0.00222488259896636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  256 | Train Loss:  0.0021080567967146635 | Validation Loss:  0.0019950319547206163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  257 | Train Loss:  0.0021450601052492857 | Validation Loss:  0.002220730995759368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  258 | Train Loss:  0.002103772945702076 | Validation Loss:  0.0019920021295547485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  259 | Train Loss:  0.0021409380715340376 | Validation Loss:  0.002217640867456794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  260 | Train Loss:  0.0021004383452236652 | Validation Loss:  0.001989873591810465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  261 | Train Loss:  0.0021378931123763323 | Validation Loss:  0.0022155970800668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  262 | Train Loss:  0.0020980413537472486 | Validation Loss:  0.00198858673684299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  263 | Train Loss:  0.0021358595695346594 | Validation Loss:  0.0022144801914691925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  264 | Train Loss:  0.0020964774303138256 | Validation Loss:  0.0019880065228790045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  265 | Train Loss:  0.0021346781868487597 | Validation Loss:  0.0022140871733427048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  266 | Train Loss:  0.002095566364005208 | Validation Loss:  0.001987934112548828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  267 | Train Loss:  0.0021341152023524046 | Validation Loss:  0.0022141574881970882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  268 | Train Loss:  0.0020950783509761095 | Validation Loss:  0.001988139236345887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  269 | Train Loss:  0.002133894944563508 | Validation Loss:  0.0022144063841551542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  270 | Train Loss:  0.0020947593729943037 | Validation Loss:  0.0019883804488927126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  271 | Train Loss:  0.0021337291691452265 | Validation Loss:  0.0022145533002913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  272 | Train Loss:  0.0020943591371178627 | Validation Loss:  0.0019884335342794657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  273 | Train Loss:  0.0021333489567041397 | Validation Loss:  0.002214355394244194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  274 | Train Loss:  0.002093659481033683 | Validation Loss:  0.0019881147891283035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  275 | Train Loss:  0.002132534049451351 | Validation Loss:  0.002213630359619856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  276 | Train Loss:  0.0020924974232912064 | Validation Loss:  0.0019873029086738825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  277 | Train Loss:  0.0021311359014362097 | Validation Loss:  0.0022122764494270086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  278 | Train Loss:  0.0020907798316329718 | Validation Loss:  0.001985943177714944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  279 | Train Loss:  0.002129086060449481 | Validation Loss:  0.0022102720104157925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  280 | Train Loss:  0.0020884862169623375 | Validation Loss:  0.0019840516615659\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  281 | Train Loss:  0.002126401523128152 | Validation Loss:  0.002207676414400339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  282 | Train Loss:  0.002085668034851551 | Validation Loss:  0.0019817061256617308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  283 | Train Loss:  0.002123171929270029 | Validation Loss:  0.0022046116646379232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  284 | Train Loss:  0.002082433085888624 | Validation Loss:  0.001979028806090355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  285 | Train Loss:  0.0021195420995354652 | Validation Loss:  0.002201240509748459\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  286 | Train Loss:  0.002078925957903266 | Validation Loss:  0.0019761675503104925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  287 | Train Loss:  0.002115686656907201 | Validation Loss:  0.0021977443248033524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  288 | Train Loss:  0.002075307769700885 | Validation Loss:  0.001973275560885668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  289 | Train Loss:  0.0021117888391017914 | Validation Loss:  0.0021942981984466314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  290 | Train Loss:  0.0020717354491353035 | Validation Loss:  0.001970490673556924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  291 | Train Loss:  0.002108014654368162 | Validation Loss:  0.00219105277210474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  292 | Train Loss:  0.002068342873826623 | Validation Loss:  0.0019679260440170765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  293 | Train Loss:  0.0021045003086328506 | Validation Loss:  0.0021881202701479197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  294 | Train Loss:  0.0020652306266129017 | Validation Loss:  0.001965657342225313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  295 | Train Loss:  0.0021013389341533184 | Validation Loss:  0.0021855682134628296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  296 | Train Loss:  0.0020624594762921333 | Validation Loss:  0.0019637218210846186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  297 | Train Loss:  0.0020985763985663652 | Validation Loss:  0.002183416858315468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  298 | Train Loss:  0.002060048747807741 | Validation Loss:  0.001962116686627269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  299 | Train Loss:  0.002096211537718773 | Validation Loss:  0.0021816412918269634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  300 | Train Loss:  0.0020579760894179344 | Validation Loss:  0.0019608051516115665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  301 | Train Loss:  0.0020942017436027527 | Validation Loss:  0.002180178416892886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  302 | Train Loss:  0.002056186320260167 | Validation Loss:  0.0019597227219492197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  303 | Train Loss:  0.00209247088059783 | Validation Loss:  0.0021789390593767166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  304 | Train Loss:  0.002054600976407528 | Validation Loss:  0.001958786742761731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  305 | Train Loss:  0.0020909213926643133 | Validation Loss:  0.002177820075303316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  306 | Train Loss:  0.002053128322586417 | Validation Loss:  0.0019579085055738688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  307 | Train Loss:  0.0020894454792141914 | Validation Loss:  0.0021767138969153166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  308 | Train Loss:  0.0020516719669103622 | Validation Loss:  0.0019570011645555496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  309 | Train Loss:  0.002087940229102969 | Validation Loss:  0.0021755238994956017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  310 | Train Loss:  0.0020501466933637857 | Validation Loss:  0.0019559902139008045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  311 | Train Loss:  0.0020863155368715525 | Validation Loss:  0.00217417161911726\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  312 | Train Loss:  0.0020484819542616606 | Validation Loss:  0.0019548204727470875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  313 | Train Loss:  0.0020845034159719944 | Validation Loss:  0.0021726072300225496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  314 | Train Loss:  0.002046631881967187 | Validation Loss:  0.00195346144028008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  315 | Train Loss:  0.0020824670791625977 | Validation Loss:  0.0021708086133003235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  316 | Train Loss:  0.002044577617198229 | Validation Loss:  0.0019519078778102994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  317 | Train Loss:  0.0020801990758627653 | Validation Loss:  0.002168786246329546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  318 | Train Loss:  0.002042326843366027 | Validation Loss:  0.0019501789938658476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  319 | Train Loss:  0.002077721059322357 | Validation Loss:  0.002166574355214834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  320 | Train Loss:  0.002039909828454256 | Validation Loss:  0.0019483119249343872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  321 | Train Loss:  0.0020750765688717365 | Validation Loss:  0.0021642260253429413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  322 | Train Loss:  0.002037374535575509 | Validation Loss:  0.0019463561475276947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  323 | Train Loss:  0.0020723252091556787 | Validation Loss:  0.002161804586648941\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  324 | Train Loss:  0.002034775447100401 | Validation Loss:  0.0019443671917542815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  325 | Train Loss:  0.0020695326384156942 | Validation Loss:  0.0021593773271888494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  326 | Train Loss:  0.0020321733318269253 | Validation Loss:  0.0019424002384766936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  327 | Train Loss:  0.0020667649805545807 | Validation Loss:  0.0021570066455751657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  328 | Train Loss:  0.0020296231377869844 | Validation Loss:  0.0019405025523155928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  329 | Train Loss:  0.0020640799775719643 | Validation Loss:  0.002154741669073701\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  330 | Train Loss:  0.00202716956846416 | Validation Loss:  0.001938709057867527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  331 | Train Loss:  0.00206152000464499 | Validation Loss:  0.0021526161581277847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  332 | Train Loss:  0.0020248424261808395 | Validation Loss:  0.0019370407098904252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  333 | Train Loss:  0.0020591109059751034 | Validation Loss:  0.0021506466437131166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  334 | Train Loss:  0.0020226561464369297 | Validation Loss:  0.0019355040276423097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  335 | Train Loss:  0.0020568608306348324 | Validation Loss:  0.002148830331861973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  336 | Train Loss:  0.002020609565079212 | Validation Loss:  0.0019340901635587215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  337 | Train Loss:  0.002054760232567787 | Validation Loss:  0.002147150691598654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  338 | Train Loss:  0.002018687315285206 | Validation Loss:  0.0019327796762809157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  339 | Train Loss:  0.0020527865272015333 | Validation Loss:  0.00214557652361691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  340 | Train Loss:  0.0020168626215308905 | Validation Loss:  0.0019315429963171482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  341 | Train Loss:  0.002050903392955661 | Validation Loss:  0.0021440708078444004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  342 | Train Loss:  0.0020151017233729362 | Validation Loss:  0.0019303465960547328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  343 | Train Loss:  0.0020490719471126795 | Validation Loss:  0.0021425916347652674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  344 | Train Loss:  0.002013368299230933 | Validation Loss:  0.0019291567150503397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  345 | Train Loss:  0.0020472512114793062 | Validation Loss:  0.0021410989575088024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  346 | Train Loss:  0.0020116267260164022 | Validation Loss:  0.0019279421539977193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  347 | Train Loss:  0.0020454032346606255 | Validation Loss:  0.002139560878276825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  348 | Train Loss:  0.002009847667068243 | Validation Loss:  0.0019266800954937935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  349 | Train Loss:  0.0020435003098100424 | Validation Loss:  0.0021379536483436823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  350 | Train Loss:  0.002008011331781745 | Validation Loss:  0.0019253549398854375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  351 | Train Loss:  0.002041523577645421 | Validation Loss:  0.0021362649276852608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  352 | Train Loss:  0.0020061046816408634 | Validation Loss:  0.0019239606335759163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  353 | Train Loss:  0.0020394641906023026 | Validation Loss:  0.0021344933193176985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  354 | Train Loss:  0.0020041274838149548 | Validation Loss:  0.0019225002033635974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  355 | Train Loss:  0.0020373272709548473 | Validation Loss:  0.002132647903636098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  356 | Train Loss:  0.002002087654545903 | Validation Loss:  0.0019209861056879163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  357 | Train Loss:  0.0020351256243884563 | Validation Loss:  0.002130746142938733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  358 | Train Loss:  0.0020000000949949026 | Validation Loss:  0.0019194346386939287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  359 | Train Loss:  0.0020328795071691275 | Validation Loss:  0.0021288127172738314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  360 | Train Loss:  0.001997885759919882 | Validation Loss:  0.0019178672228008509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  361 | Train Loss:  0.0020306145306676626 | Validation Loss:  0.0021268718410283327\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  362 | Train Loss:  0.0019957676995545626 | Validation Loss:  0.0019163048127666116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  363 | Train Loss:  0.0020283556077629328 | Validation Loss:  0.002124949125573039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  364 | Train Loss:  0.0019936677999794483 | Validation Loss:  0.0019147662678733468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  365 | Train Loss:  0.0020261257886886597 | Validation Loss:  0.0021230648271739483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  366 | Train Loss:  0.0019916046876460314 | Validation Loss:  0.0019132682355120778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  367 | Train Loss:  0.002023945329710841 | Validation Loss:  0.002121236640959978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  368 | Train Loss:  0.001989593030884862 | Validation Loss:  0.0019118207274004817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  369 | Train Loss:  0.0020218261051923037 | Validation Loss:  0.002119472250342369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  370 | Train Loss:  0.0019876400474458933 | Validation Loss:  0.001910428167320788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  371 | Train Loss:  0.0020197744015604258 | Validation Loss:  0.002117772586643696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  372 | Train Loss:  0.001985747367143631 | Validation Loss:  0.0019090883433818817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  373 | Train Loss:  0.00201778719201684 | Validation Loss:  0.0021161341574043036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  374 | Train Loss:  0.001983911730349064 | Validation Loss:  0.0019077964825555682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  375 | Train Loss:  0.0020158595871180296 | Validation Loss:  0.0021145460195839405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  376 | Train Loss:  0.0019821226596832275 | Validation Loss:  0.0019065409433096647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  377 | Train Loss:  0.0020139773841947317 | Validation Loss:  0.0021129949018359184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  378 | Train Loss:  0.0019803696777671576 | Validation Loss:  0.001905310433357954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  379 | Train Loss:  0.0020121268462389708 | Validation Loss:  0.0021114640403538942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  380 | Train Loss:  0.001978637184947729 | Validation Loss:  0.001904090866446495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  381 | Train Loss:  0.00201029097661376 | Validation Loss:  0.002109937835484743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  382 | Train Loss:  0.0019769116770476103 | Validation Loss:  0.0019028699025511742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  383 | Train Loss:  0.0020084555726498365 | Validation Loss:  0.0021084032487124205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  384 | Train Loss:  0.0019751808140426874 | Validation Loss:  0.0019016374135389924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  385 | Train Loss:  0.002006607363000512 | Validation Loss:  0.0021068479400128126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  386 | Train Loss:  0.001973434817045927 | Validation Loss:  0.001900385250337422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  387 | Train Loss:  0.002004736801609397 | Validation Loss:  0.0021052660886198282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  388 | Train Loss:  0.0019716673996299505 | Validation Loss:  0.001899110502563417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  389 | Train Loss:  0.00200284062884748 | Validation Loss:  0.002103655831888318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  390 | Train Loss:  0.001969877164810896 | Validation Loss:  0.0018978139851242304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  391 | Train Loss:  0.0020009183790534735 | Validation Loss:  0.0021020197309553623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  392 | Train Loss:  0.0019680664408951998 | Validation Loss:  0.0018964981427416205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  393 | Train Loss:  0.0019989744760096073 | Validation Loss:  0.002100363839417696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  394 | Train Loss:  0.0019662403501570225 | Validation Loss:  0.0018951701931655407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  395 | Train Loss:  0.0019970154389739037 | Validation Loss:  0.0020986967720091343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  396 | Train Loss:  0.0019644061103463173 | Validation Loss:  0.0018938372377306223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  397 | Train Loss:  0.0019950515124946833 | Validation Loss:  0.002097028074786067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  398 | Train Loss:  0.001962572569027543 | Validation Loss:  0.001892507541924715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  399 | Train Loss:  0.001993092242628336 | Validation Loss:  0.0020953689236193895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  400 | Train Loss:  0.001960749737918377 | Validation Loss:  0.0018911899533122778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  401 | Train Loss:  0.00199114833958447 | Validation Loss:  0.0020937274675816298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  402 | Train Loss:  0.001958944136276841 | Validation Loss:  0.0018898906419053674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  403 | Train Loss:  0.0019892265554517508 | Validation Loss:  0.00209211022593081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  404 | Train Loss:  0.0019571625161916018 | Validation Loss:  0.0018886133329942822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  405 | Train Loss:  0.0019873324781656265 | Validation Loss:  0.0020905225537717342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  406 | Train Loss:  0.001955408602952957 | Validation Loss:  0.001887361635453999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  407 | Train Loss:  0.0019854700658470392 | Validation Loss:  0.002088965382426977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  408 | Train Loss:  0.0019536837935447693 | Validation Loss:  0.0018861355492845178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  409 | Train Loss:  0.0019836395513266325 | Validation Loss:  0.0020874375477433205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  410 | Train Loss:  0.0019519873894751072 | Validation Loss:  0.001884933328256011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  411 | Train Loss:  0.0019818388391286135 | Validation Loss:  0.0020859360229223967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  412 | Train Loss:  0.0019503162475302815 | Validation Loss:  0.0018837503157556057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  413 | Train Loss:  0.0019800623413175344 | Validation Loss:  0.0020844547543674707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  414 | Train Loss:  0.0019486648961901665 | Validation Loss:  0.001882582320831716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  415 | Train Loss:  0.0019783054012805223 | Validation Loss:  0.0020829872228205204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  416 | Train Loss:  0.0019470282131806016 | Validation Loss:  0.0018814230570569634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  417 | Train Loss:  0.001976560102775693 | Validation Loss:  0.002081527141854167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  418 | Train Loss:  0.0019454003777354956 | Validation Loss:  0.0018802685663104057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  419 | Train Loss:  0.001974821323528886 | Validation Loss:  0.002080068690702319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  420 | Train Loss:  0.0019437758019194007 | Validation Loss:  0.0018791138427332044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  421 | Train Loss:  0.001973083708435297 | Validation Loss:  0.0020786074455827475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  422 | Train Loss:  0.0019421508768573403 | Validation Loss:  0.0018779551610350609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  423 | Train Loss:  0.001971342135220766 | Validation Loss:  0.00207713944837451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  424 | Train Loss:  0.0019405219936743379 | Validation Loss:  0.001876791357062757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  425 | Train Loss:  0.0019695954397320747 | Validation Loss:  0.002075664233416319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  426 | Train Loss:  0.0019388890359550714 | Validation Loss:  0.001875622314400971\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  427 | Train Loss:  0.0019678426906466484 | Validation Loss:  0.0020741818007081747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  428 | Train Loss:  0.0019372515380382538 | Validation Loss:  0.0018744479166343808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  429 | Train Loss:  0.0019660843536257744 | Validation Loss:  0.002072694944217801\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  430 | Train Loss:  0.0019356117118149996 | Validation Loss:  0.0018732721218839288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  431 | Train Loss:  0.0019643253181129694 | Validation Loss:  0.002071207156404853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  432 | Train Loss:  0.0019339727004989982 | Validation Loss:  0.0018720976077020168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  433 | Train Loss:  0.00196256791241467 | Validation Loss:  0.002069722395390272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  434 | Train Loss:  0.0019323381129652262 | Validation Loss:  0.001870927051641047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  435 | Train Loss:  0.0019608165603131056 | Validation Loss:  0.00206824392080307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  436 | Train Loss:  0.0019307113252580166 | Validation Loss:  0.0018697644118219614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  437 | Train Loss:  0.0019590745214372873 | Validation Loss:  0.002066776854917407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  438 | Train Loss:  0.0019290958298370242 | Validation Loss:  0.0018686124822124839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  439 | Train Loss:  0.001957346685230732 | Validation Loss:  0.0020653237588703632\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  440 | Train Loss:  0.0019274953519925475 | Validation Loss:  0.0018674738239496946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  441 | Train Loss:  0.0019556351471692324 | Validation Loss:  0.0020638862624764442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  442 | Train Loss:  0.0019259100081399083 | Validation Loss:  0.00186634820420295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  443 | Train Loss:  0.001953940140083432 | Validation Loss:  0.002062465762719512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  444 | Train Loss:  0.0019243411952629685 | Validation Loss:  0.0018652366707101464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  445 | Train Loss:  0.0019522629445418715 | Validation Loss:  0.002061061095446348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  446 | Train Loss:  0.0019227878656238317 | Validation Loss:  0.0018641381757333875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  447 | Train Loss:  0.0019506020471453667 | Validation Loss:  0.0020596703980118036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  448 | Train Loss:  0.001921249320730567 | Validation Loss:  0.0018630505073815584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  449 | Train Loss:  0.0019489555852487683 | Validation Loss:  0.0020582922734320164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  450 | Train Loss:  0.001919722999446094 | Validation Loss:  0.0018619728507474065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  451 | Train Loss:  0.0019473214633762836 | Validation Loss:  0.0020569234620779753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  452 | Train Loss:  0.0019182071555405855 | Validation Loss:  0.0018609018297865987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  453 | Train Loss:  0.0019456964218989015 | Validation Loss:  0.002055562101304531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  454 | Train Loss:  0.0019166995771229267 | Validation Loss:  0.0018598365131765604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  455 | Train Loss:  0.0019440790638327599 | Validation Loss:  0.0020542049314826727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  456 | Train Loss:  0.0019151977030560374 | Validation Loss:  0.0018587737577036023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  457 | Train Loss:  0.0019424657803028822 | Validation Loss:  0.002052850555628538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  458 | Train Loss:  0.001913699321448803 | Validation Loss:  0.0018577128648757935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  459 | Train Loss:  0.0019408550579100847 | Validation Loss:  0.0020514964126050472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  460 | Train Loss:  0.0019122036173939705 | Validation Loss:  0.0018566531362012029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  461 | Train Loss:  0.0019392463145777583 | Validation Loss:  0.002050143200904131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  462 | Train Loss:  0.001910710008814931 | Validation Loss:  0.0018555941060185432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  463 | Train Loss:  0.0019376392010599375 | Validation Loss:  0.0020487906876951456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  464 | Train Loss:  0.001909218612127006 | Validation Loss:  0.0018545363564044237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  465 | Train Loss:  0.0019360346486791968 | Validation Loss:  0.002047439571470022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  466 | Train Loss:  0.0019077302422374487 | Validation Loss:  0.0018534804694354534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  467 | Train Loss:  0.001934432191774249 | Validation Loss:  0.0020460914820432663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  468 | Train Loss:  0.001906246063299477 | Validation Loss:  0.0018524285405874252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  469 | Train Loss:  0.0019328355556353927 | Validation Loss:  0.00204474781639874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  470 | Train Loss:  0.0019047673558816314 | Validation Loss:  0.0018513812683522701\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  471 | Train Loss:  0.001931244507431984 | Validation Loss:  0.002043411135673523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  472 | Train Loss:  0.0019032962154597044 | Validation Loss:  0.0018503399332985282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  473 | Train Loss:  0.0019296619575470686 | Validation Loss:  0.0020420823711901903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  474 | Train Loss:  0.0019018335733562708 | Validation Loss:  0.0018493067473173141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  475 | Train Loss:  0.001928088953718543 | Validation Loss:  0.002040763385593891\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  476 | Train Loss:  0.0019003812922164798 | Validation Loss:  0.0018482821760699153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  477 | Train Loss:  0.001926527009345591 | Validation Loss:  0.002039453713223338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  478 | Train Loss:  0.0018989390227943659 | Validation Loss:  0.001847265986725688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  479 | Train Loss:  0.0019249760080128908 | Validation Loss:  0.002038155449554324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  480 | Train Loss:  0.001897507579997182 | Validation Loss:  0.0018462585285305977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  481 | Train Loss:  0.0019234360661357641 | Validation Loss:  0.0020368671976029873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  482 | Train Loss:  0.0018960870802402496 | Validation Loss:  0.001845259452238679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  483 | Train Loss:  0.0019219075329601765 | Validation Loss:  0.0020355882588773966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  484 | Train Loss:  0.001894676242955029 | Validation Loss:  0.001844267826527357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  485 | Train Loss:  0.0019203880801796913 | Validation Loss:  0.002034316770732403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  486 | Train Loss:  0.001893273089081049 | Validation Loss:  0.001843282370828092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  487 | Train Loss:  0.0019188766600564122 | Validation Loss:  0.0020330525003373623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  488 | Train Loss:  0.0018918787827715278 | Validation Loss:  0.001842302968725562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  489 | Train Loss:  0.0019173729233443737 | Validation Loss:  0.0020317942835390568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  490 | Train Loss:  0.0018904905300587416 | Validation Loss:  0.0018413279904052615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  491 | Train Loss:  0.0019158757058903575 | Validation Loss:  0.002030540956184268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  492 | Train Loss:  0.0018891080981120467 | Validation Loss:  0.0018403573194518685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  493 | Train Loss:  0.0019143841927871108 | Validation Loss:  0.0020292920526117086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  494 | Train Loss:  0.0018877314869314432 | Validation Loss:  0.0018393900245428085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  495 | Train Loss:  0.0019128968706354499 | Validation Loss:  0.0020280464086681604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  496 | Train Loss:  0.0018863589502871037 | Validation Loss:  0.0018384261056780815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  497 | Train Loss:  0.0019114139722660184 | Validation Loss:  0.002026804257184267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  498 | Train Loss:  0.0018849907210096717 | Validation Loss:  0.001837464515119791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  499 | Train Loss:  0.001909935032017529 | Validation Loss:  0.002025565830990672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  500 | Train Loss:  0.0018836274975910783 | Validation Loss:  0.0018365072319284081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  501 | Train Loss:  0.0019084614468738437 | Validation Loss:  0.0020243318285793066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  502 | Train Loss:  0.0018822692800313234 | Validation Loss:  0.0018355536740273237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  503 | Train Loss:  0.001906993449665606 | Validation Loss:  0.002023101784288883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  504 | Train Loss:  0.0018809163011610508 | Validation Loss:  0.0018346041906625032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  505 | Train Loss:  0.0019055298762395978 | Validation Loss:  0.002021877095103264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  506 | Train Loss:  0.0018795696087181568 | Validation Loss:  0.001833659945987165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  507 | Train Loss:  0.0019040735205635428 | Validation Loss:  0.002020658692345023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  508 | Train Loss:  0.0018782294355332851 | Validation Loss:  0.0018327204743400216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  509 | Train Loss:  0.0019026240333914757 | Validation Loss:  0.0020194463431835175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  510 | Train Loss:  0.001876896363683045 | Validation Loss:  0.0018317868234589696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  511 | Train Loss:  0.0019011811818927526 | Validation Loss:  0.002018240513280034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  512 | Train Loss:  0.0018755699275061488 | Validation Loss:  0.001830858993344009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  513 | Train Loss:  0.0018997464794665575 | Validation Loss:  0.002017041901126504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  514 | Train Loss:  0.0018742515239864588 | Validation Loss:  0.001829937449656427\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  515 | Train Loss:  0.0018983196932822466 | Validation Loss:  0.0020158502738922834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  516 | Train Loss:  0.001872941036708653 | Validation Loss:  0.0018290220759809017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  517 | Train Loss:  0.0018969010561704636 | Validation Loss:  0.002014666795730591\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  518 | Train Loss:  0.0018716383492574096 | Validation Loss:  0.0018281129887327552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  519 | Train Loss:  0.0018954901024699211 | Validation Loss:  0.002013489603996277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  520 | Train Loss:  0.0018703427631407976 | Validation Loss:  0.0018272092565894127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  521 | Train Loss:  0.0018940867157652974 | Validation Loss:  0.0020123186986893415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  522 | Train Loss:  0.0018690547440201044 | Validation Loss:  0.001826310995966196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  523 | Train Loss:  0.0018926903139799833 | Validation Loss:  0.0020111538469791412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  524 | Train Loss:  0.0018677725456655025 | Validation Loss:  0.0018254172755405307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  525 | Train Loss:  0.0018912999657914042 | Validation Loss:  0.0020099941175431013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  526 | Train Loss:  0.0018664965173229575 | Validation Loss:  0.0018245280953124166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  527 | Train Loss:  0.0018899155547842383 | Validation Loss:  0.0020088395103812218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  528 | Train Loss:  0.0018652264261618257 | Validation Loss:  0.0018236434552818537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  529 | Train Loss:  0.0018885370809584856 | Validation Loss:  0.002007689792662859\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  530 | Train Loss:  0.0018639619229361415 | Validation Loss:  0.001822762773372233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  531 | Train Loss:  0.0018871643114835024 | Validation Loss:  0.0020065444987267256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  532 | Train Loss:  0.0018627024255692959 | Validation Loss:  0.0018218860495835543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  533 | Train Loss:  0.0018857966642826796 | Validation Loss:  0.002005404094234109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  534 | Train Loss:  0.001861449098214507 | Validation Loss:  0.001821013749577105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  535 | Train Loss:  0.00188443495426327 | Validation Loss:  0.0020042683463543653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  536 | Train Loss:  0.0018602005438879132 | Validation Loss:  0.0018201451748609543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  537 | Train Loss:  0.0018830779008567333 | Validation Loss:  0.002003137022256851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  538 | Train Loss:  0.0018589572282508016 | Validation Loss:  0.0018192806746810675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  539 | Train Loss:  0.0018817269010469317 | Validation Loss:  0.0020020101219415665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  540 | Train Loss:  0.0018577196169644594 | Validation Loss:  0.0018184202490374446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  541 | Train Loss:  0.001880380790680647 | Validation Loss:  0.002000888343900442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  542 | Train Loss:  0.0018564873607829213 | Validation Loss:  0.0018175644800066948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  543 | Train Loss:  0.0018790410831570625 | Validation Loss:  0.001999771920964122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  544 | Train Loss:  0.0018552611581981182 | Validation Loss:  0.0018167130183428526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  545 | Train Loss:  0.0018777070799842477 | Validation Loss:  0.0019986601546406746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  546 | Train Loss:  0.0018540406599640846 | Validation Loss:  0.0018158660968765616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  547 | Train Loss:  0.0018763793632388115 | Validation Loss:  0.001997554674744606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  548 | Train Loss:  0.0018528267974033952 | Validation Loss:  0.0018150246469303966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  549 | Train Loss:  0.001875058631412685 | Validation Loss:  0.0019964552484452724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  550 | Train Loss:  0.001851618872024119 | Validation Loss:  0.0018141880864277482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  551 | Train Loss:  0.001873744186013937 | Validation Loss:  0.0019953614100813866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  552 | Train Loss:  0.001850418047979474 | Validation Loss:  0.001813356066122651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  553 | Train Loss:  0.0018724363762885332 | Validation Loss:  0.0019942729268223047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  554 | Train Loss:  0.0018492229282855988 | Validation Loss:  0.0018125295173376799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  555 | Train Loss:  0.00187113496940583 | Validation Loss:  0.0019931907299906015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  556 | Train Loss:  0.0018480344442650676 | Validation Loss:  0.001811707392334938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  557 | Train Loss:  0.0018698401981964707 | Validation Loss:  0.001992113422602415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  558 | Train Loss:  0.001846851664595306 | Validation Loss:  0.001810890156775713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  559 | Train Loss:  0.0018685515969991684 | Validation Loss:  0.00199104193598032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  560 | Train Loss:  0.0018456752877682447 | Validation Loss:  0.001810077577829361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  561 | Train Loss:  0.0018672691658139229 | Validation Loss:  0.001989975804463029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  562 | Train Loss:  0.001844504615291953 | Validation Loss:  0.0018092693062499166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  563 | Train Loss:  0.0018659926718100905 | Validation Loss:  0.0019889138638973236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  564 | Train Loss:  0.0018433390650898218 | Validation Loss:  0.0018084648763760924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  565 | Train Loss:  0.001864721765741706 | Validation Loss:  0.0019878570456057787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  566 | Train Loss:  0.00184217921923846 | Validation Loss:  0.001807664753869176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  567 | Train Loss:  0.0018634560983628035 | Validation Loss:  0.0019868051167577505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  568 | Train Loss:  0.0018410247284919024 | Validation Loss:  0.001806868240237236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  569 | Train Loss:  0.0018621959025040269 | Validation Loss:  0.0019857571460306644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  570 | Train Loss:  0.0018398751271888614 | Validation Loss:  0.001806075917556882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  571 | Train Loss:  0.0018609410617500544 | Validation Loss:  0.0019847138319164515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  572 | Train Loss:  0.0018387308809906244 | Validation Loss:  0.0018052877858281136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  573 | Train Loss:  0.0018596920417621732 | Validation Loss:  0.0019836751744151115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  574 | Train Loss:  0.0018375917570665479 | Validation Loss:  0.0018045029137283564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  575 | Train Loss:  0.0018584481440484524 | Validation Loss:  0.001982640940696001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  576 | Train Loss:  0.0018364576390013099 | Validation Loss:  0.0018037225818261504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  577 | Train Loss:  0.0018572097178548574 | Validation Loss:  0.00198161113075912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  578 | Train Loss:  0.0018353292252868414 | Validation Loss:  0.001802946557290852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  579 | Train Loss:  0.001855976996012032 | Validation Loss:  0.001980587374418974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  580 | Train Loss:  0.001834206748753786 | Validation Loss:  0.001802175072953105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  581 | Train Loss:  0.001854750677011907 | Validation Loss:  0.001979568274691701\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  582 | Train Loss:  0.0018330898601561785 | Validation Loss:  0.0018014073139056563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  583 | Train Loss:  0.001853529829531908 | Validation Loss:  0.0019785533659160137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  584 | Train Loss:  0.001831977628171444 | Validation Loss:  0.001800644095055759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  585 | Train Loss:  0.0018523144535720348 | Validation Loss:  0.001977544045075774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  586 | Train Loss:  0.0018308719154447317 | Validation Loss:  0.0017998854164034128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  587 | Train Loss:  0.0018511051312088966 | Validation Loss:  0.0019765400793403387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  588 | Train Loss:  0.0018297717906534672 | Validation Loss:  0.001799131277948618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  589 | Train Loss:  0.0018499019788578153 | Validation Loss:  0.0019755412358790636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  590 | Train Loss:  0.0018286773702129722 | Validation Loss:  0.0017983808647841215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  591 | Train Loss:  0.0018487044144421816 | Validation Loss:  0.001974546816200018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  592 | Train Loss:  0.0018275881884619594 | Validation Loss:  0.00179763522464782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  593 | Train Loss:  0.0018475131364539266 | Validation Loss:  0.0019735577516257763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  594 | Train Loss:  0.001826504711061716 | Validation Loss:  0.0017968931933864951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  595 | Train Loss:  0.001846326980739832 | Validation Loss:  0.0019725726451724768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  596 | Train Loss:  0.0018254262395203114 | Validation Loss:  0.0017961552366614342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  597 | Train Loss:  0.0018451467622071505 | Validation Loss:  0.0019715926609933376\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  598 | Train Loss:  0.0018243526574224234 | Validation Loss:  0.0017954211216419935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  599 | Train Loss:  0.001843971898779273 | Validation Loss:  0.0019706166349351406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  600 | Train Loss:  0.0018232843140140176 | Validation Loss:  0.0017946903826668859\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  601 | Train Loss:  0.0018428023904561996 | Validation Loss:  0.0019696447998285294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  602 | Train Loss:  0.0018222203943878412 | Validation Loss:  0.0017939637182280421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  603 | Train Loss:  0.0018416382372379303 | Validation Loss:  0.0019686773885041475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  604 | Train Loss:  0.0018211615970358253 | Validation Loss:  0.001793240662664175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  605 | Train Loss:  0.0018404793227091432 | Validation Loss:  0.001967714400961995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  606 | Train Loss:  0.0018201080383732915 | Validation Loss:  0.0017925219144672155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  607 | Train Loss:  0.0018393259961158037 | Validation Loss:  0.001966756535694003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  608 | Train Loss:  0.0018190599512308836 | Validation Loss:  0.0017918073572218418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  609 | Train Loss:  0.0018381784902885556 | Validation Loss:  0.0019658023957163095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  610 | Train Loss:  0.0018180167535319924 | Validation Loss:  0.0017910965252667665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  611 | Train Loss:  0.0018370355246588588 | Validation Loss:  0.0019648538436740637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  612 | Train Loss:  0.0018169796094298363 | Validation Loss:  0.0017903901170939207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  613 | Train Loss:  0.0018358981469646096 | Validation Loss:  0.001963909948244691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  614 | Train Loss:  0.0018159480532631278 | Validation Loss:  0.001789688365533948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  615 | Train Loss:  0.0018347655422985554 | Validation Loss:  0.001962970942258835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  616 | Train Loss:  0.0018149232491850853 | Validation Loss:  0.0017889911541715264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  617 | Train Loss:  0.001833637128584087 | Validation Loss:  0.001962036360055208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  618 | Train Loss:  0.001813904382288456 | Validation Loss:  0.0017882987158372998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  619 | Train Loss:  0.0018325119744986296 | Validation Loss:  0.001961106900125742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  620 | Train Loss:  0.0018128930823877454 | Validation Loss:  0.0017876115161925554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  621 | Train Loss:  0.0018313901964575052 | Validation Loss:  0.0019601816311478615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  622 | Train Loss:  0.0018118886509910226 | Validation Loss:  0.0017869294388219714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  623 | Train Loss:  0.0018302691169083118 | Validation Loss:  0.0019592612516134977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  624 | Train Loss:  0.0018108927179127932 | Validation Loss:  0.001786253065802157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  625 | Train Loss:  0.0018291486194357276 | Validation Loss:  0.0019583450630307198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  626 | Train Loss:  0.0018099057488143444 | Validation Loss:  0.001785582397133112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  627 | Train Loss:  0.0018280253279954195 | Validation Loss:  0.0019574323669075966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  628 | Train Loss:  0.0018089291406795382 | Validation Loss:  0.0017849188297986984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  629 | Train Loss:  0.0018268973799422383 | Validation Loss:  0.0019565231632441282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  630 | Train Loss:  0.001807964639738202 | Validation Loss:  0.0017842631787061691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  631 | Train Loss:  0.001825758838094771 | Validation Loss:  0.0019556162878870964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  632 | Train Loss:  0.0018070153892040253 | Validation Loss:  0.0017836187034845352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  633 | Train Loss:  0.0018246042309328914 | Validation Loss:  0.001954712439328432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  634 | Train Loss:  0.00180608790833503 | Validation Loss:  0.0017829909920692444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  635 | Train Loss:  0.0018234220333397388 | Validation Loss:  0.001953809754922986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  636 | Train Loss:  0.0018051930237561464 | Validation Loss:  0.001782392617315054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  637 | Train Loss:  0.001822192221879959 | Validation Loss:  0.0019529078854247928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  638 | Train Loss:  0.0018043542513623834 | Validation Loss:  0.0017818518681451678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  639 | Train Loss:  0.0018208769615739584 | Validation Loss:  0.0019520098576322198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  640 | Train Loss:  0.001803624676540494 | Validation Loss:  0.0017814368475228548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  641 | Train Loss:  0.0018194012809544802 | Validation Loss:  0.001951123820617795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  642 | Train Loss:  0.0018031290965154767 | Validation Loss:  0.0017813227605074644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  643 | Train Loss:  0.0018176164012402296 | Validation Loss:  0.0019502989016473293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  644 | Train Loss:  0.0018031935906037688 | Validation Loss:  0.0017820019274950027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  645 | Train Loss:  0.001815266441553831 | Validation Loss:  0.0019497806206345558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  646 | Train Loss:  0.0018047797493636608 | Validation Loss:  0.0017850571312010288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  647 | Train Loss:  0.0018122084438800812 | Validation Loss:  0.001950896461494267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  648 | Train Loss:  0.0018111758399754763 | Validation Loss:  0.001795955584384501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  649 | Train Loss:  0.0018103823531419039 | Validation Loss:  0.0019593301694840193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  650 | Train Loss:  0.0018333449261263013 | Validation Loss:  0.001827285042963922\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  651 | Train Loss:  0.0018184975488111377 | Validation Loss:  0.0019757451955229044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  652 | Train Loss:  0.001879952964372933 | Validation Loss:  0.0018491015071049333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  653 | Train Loss:  0.0018125631613656878 | Validation Loss:  0.0019060737686231732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  654 | Train Loss:  0.0018422944704070687 | Validation Loss:  0.0017633113311603665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  655 | Train Loss:  0.0017082219710573554 | Validation Loss:  0.0017367652617394924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  656 | Train Loss:  0.001677866792306304 | Validation Loss:  0.0016616256907582283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  657 | Train Loss:  0.0016001395415514708 | Validation Loss:  0.0016432983102276921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  658 | Train Loss:  0.0015767286531627178 | Validation Loss:  0.0016115582548081875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  659 | Train Loss:  0.001551200868561864 | Validation Loss:  0.0016184193082153797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  660 | Train Loss:  0.0015424200100824237 | Validation Loss:  0.0015902919694781303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  661 | Train Loss:  0.0015363584971055388 | Validation Loss:  0.0016260190168395638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  662 | Train Loss:  0.001537058036774397 | Validation Loss:  0.0015826105372980237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  663 | Train Loss:  0.0015438650734722614 | Validation Loss:  0.0016713384538888931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  664 | Train Loss:  0.0015608108369633555 | Validation Loss:  0.0016061540227383375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  665 | Train Loss:  0.001602712320163846 | Validation Loss:  0.0018164091743528843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  666 | Train Loss:  0.001670311437919736 | Validation Loss:  0.001762671978212893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  667 | Train Loss:  0.001836210722103715 | Validation Loss:  0.00215693237259984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  668 | Train Loss:  0.001967562362551689 | Validation Loss:  0.0021629782859236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  669 | Train Loss:  0.0023416702169924974 | Validation Loss:  0.0024391324259340763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  670 | Train Loss:  0.0022234192583709955 | Validation Loss:  0.0023959181271493435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  671 | Train Loss:  0.00258623412810266 | Validation Loss:  0.0025320809800177813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  672 | Train Loss:  0.0023170649074018 | Validation Loss:  0.0023515038192272186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  673 | Train Loss:  0.0024881092831492424 | Validation Loss:  0.002536814659833908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  674 | Train Loss:  0.0023356336168944836 | Validation Loss:  0.0022013806737959385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  675 | Train Loss:  0.0023058163933455944 | Validation Loss:  0.0023433235473930836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  676 | Train Loss:  0.0021588627714663744 | Validation Loss:  0.0020046308636665344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  677 | Train Loss:  0.0020862978417426348 | Validation Loss:  0.0021224201191216707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  678 | Train Loss:  0.001954354578629136 | Validation Loss:  0.001850543194450438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  679 | Train Loss:  0.001910546561703086 | Validation Loss:  0.0019679609686136246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  680 | Train Loss:  0.0018144022906199098 | Validation Loss:  0.001751827890984714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  681 | Train Loss:  0.0017918372759595513 | Validation Loss:  0.0018705883994698524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  682 | Train Loss:  0.001728622242808342 | Validation Loss:  0.0016934149898588657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  683 | Train Loss:  0.001716913073323667 | Validation Loss:  0.001810029149055481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  684 | Train Loss:  0.0016766710905358195 | Validation Loss:  0.0016601287061348557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  685 | Train Loss:  0.001671010279096663 | Validation Loss:  0.0017730320105329156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  686 | Train Loss:  0.001645549084059894 | Validation Loss:  0.0016420638421550393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  687 | Train Loss:  0.0016439309110864997 | Validation Loss:  0.001752011594362557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  688 | Train Loss:  0.0016279827104881406 | Validation Loss:  0.0016336314147338271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  689 | Train Loss:  0.001629626494832337 | Validation Loss:  0.001742911641485989\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  690 | Train Loss:  0.0016201826510950923 | Validation Loss:  0.0016321070725098252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  691 | Train Loss:  0.0016250091139227152 | Validation Loss:  0.0017439950024709105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  692 | Train Loss:  0.0016205875435844064 | Validation Loss:  0.0016366876661777496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  693 | Train Loss:  0.0016290900530293584 | Validation Loss:  0.0017552354838699102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  694 | Train Loss:  0.0016292593209072948 | Validation Loss:  0.001648021163418889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  695 | Train Loss:  0.0016425073845312 | Validation Loss:  0.0017780008492991328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  696 | Train Loss:  0.0016475975280627608 | Validation Loss:  0.0016679591499269009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  697 | Train Loss:  0.0016672255005687475 | Validation Loss:  0.0018146226648241282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  698 | Train Loss:  0.0016779943834990263 | Validation Loss:  0.001699063926935196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  699 | Train Loss:  0.0017059135716408491 | Validation Loss:  0.001867192331701517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  700 | Train Loss:  0.0017227845964953303 | Validation Loss:  0.0017430951120331883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  701 | Train Loss:  0.0017601962899789214 | Validation Loss:  0.001934710773639381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  702 | Train Loss:  0.001781604951247573 | Validation Loss:  0.0017978143878281116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  703 | Train Loss:  0.0018271059961989522 | Validation Loss:  0.002008820651099086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  704 | Train Loss:  0.001847260631620884 | Validation Loss:  0.0018534878036007285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  705 | Train Loss:  0.0018953067483380437 | Validation Loss:  0.0020720844622701406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  706 | Train Loss:  0.0019038268364965916 | Validation Loss:  0.001894140150398016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  707 | Train Loss:  0.0019465074874460697 | Validation Loss:  0.002105130348354578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  708 | Train Loss:  0.0019332095980644226 | Validation Loss:  0.0019067027606070042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  709 | Train Loss:  0.001965440809726715 | Validation Loss:  0.0021000117994844913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  710 | Train Loss:  0.0019278895342722535 | Validation Loss:  0.0018902641022577882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  711 | Train Loss:  0.001950155827216804 | Validation Loss:  0.002065011765807867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  712 | Train Loss:  0.001895611989311874 | Validation Loss:  0.0018549803644418716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  713 | Train Loss:  0.0019112898735329509 | Validation Loss:  0.0020159243140369654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  714 | Train Loss:  0.001851085340604186 | Validation Loss:  0.0018136241706088185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  715 | Train Loss:  0.001863087178207934 | Validation Loss:  0.0019659483805298805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  716 | Train Loss:  0.001806408865377307 | Validation Loss:  0.0017753767315298319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  717 | Train Loss:  0.0018163895001634955 | Validation Loss:  0.001922465511597693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  718 | Train Loss:  0.0017681436147540808 | Validation Loss:  0.0017447320278733969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  719 | Train Loss:  0.0017770191188901663 | Validation Loss:  0.001888428581878543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  720 | Train Loss:  0.0017387466505169868 | Validation Loss:  0.0017229971708729863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  721 | Train Loss:  0.0017471292521804571 | Validation Loss:  0.001864503021351993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  722 | Train Loss:  0.0017186502227559686 | Validation Loss:  0.0017100127879530191\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  723 | Train Loss:  0.0017269841628149152 | Validation Loss:  0.0018505249172449112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  724 | Train Loss:  0.0017076467629522085 | Validation Loss:  0.0017052481416612864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  725 | Train Loss:  0.001716171856969595 | Validation Loss:  0.0018461942672729492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  726 | Train Loss:  0.0017055391799658537 | Validation Loss:  0.0017082091653719544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  727 | Train Loss:  0.0017141328426077962 | Validation Loss:  0.0018511457601562142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  728 | Train Loss:  0.001712172874249518 | Validation Loss:  0.0017181594157591462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  729 | Train Loss:  0.0017200037837028503 | Validation Loss:  0.0018642956856638193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  730 | Train Loss:  0.0017266965005546808 | Validation Loss:  0.001733051729388535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  731 | Train Loss:  0.001731732627376914 | Validation Loss:  0.00188239268027246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  732 | Train Loss:  0.0017458796501159668 | Validation Loss:  0.0017481372924521565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  733 | Train Loss:  0.0017449712613597512 | Validation Loss:  0.0018990545067936182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  734 | Train Loss:  0.0017627377528697252 | Validation Loss:  0.0017567655304446816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  735 | Train Loss:  0.0017541766865178943 | Validation Loss:  0.001908180653117597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  736 | Train Loss:  0.0017697701696306467 | Validation Loss:  0.0017561425920575857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  737 | Train Loss:  0.0017585119931027293 | Validation Loss:  0.0019123629899695516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  738 | Train Loss:  0.0017680131131783128 | Validation Loss:  0.0017534717917442322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  739 | Train Loss:  0.001768008223734796 | Validation Loss:  0.0019256154773756862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  740 | Train Loss:  0.001770935021340847 | Validation Loss:  0.0017647669883444905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  741 | Train Loss:  0.00180220662150532 | Validation Loss:  0.00196228944696486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  742 | Train Loss:  0.0017948359018191695 | Validation Loss:  0.0018045726465061307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  743 | Train Loss:  0.001876459107734263 | Validation Loss:  0.002008459297940135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  744 | Train Loss:  0.001832235837355256 | Validation Loss:  0.0018453334923833609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  745 | Train Loss:  0.0019461208721622825 | Validation Loss:  0.00199493276886642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  746 | Train Loss:  0.0018204249208793044 | Validation Loss:  0.001806023996323347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  747 | Train Loss:  0.0018951048841699958 | Validation Loss:  0.0019174549961462617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  748 | Train Loss:  0.0017519040266051888 | Validation Loss:  0.00173655163962394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  749 | Train Loss:  0.0017862037057057023 | Validation Loss:  0.0018724034307524562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  750 | Train Loss:  0.0017165427561849356 | Validation Loss:  0.0017345071537420154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  751 | Train Loss:  0.0017527329036965966 | Validation Loss:  0.0019114299211651087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  752 | Train Loss:  0.0017637521959841251 | Validation Loss:  0.001812849543057382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  753 | Train Loss:  0.0018130261451005936 | Validation Loss:  0.0020340157207101583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  754 | Train Loss:  0.0018949690274894238 | Validation Loss:  0.0019044425571337342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  755 | Train Loss:  0.001897030626423657 | Validation Loss:  0.002111655194312334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  756 | Train Loss:  0.001976500963792205 | Validation Loss:  0.001881302217952907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  757 | Train Loss:  0.0018781640101224184 | Validation Loss:  0.002039775950834155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  758 | Train Loss:  0.00189568055793643 | Validation Loss:  0.0018094832776114345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  759 | Train Loss:  0.0018221850041300058 | Validation Loss:  0.0019681365229189396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  760 | Train Loss:  0.0018127080984413624 | Validation Loss:  0.0017758843023329973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  761 | Train Loss:  0.0018083994509652257 | Validation Loss:  0.0019443287746980786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  762 | Train Loss:  0.0017820150824263692 | Validation Loss:  0.0017671360401436687\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  763 | Train Loss:  0.0018141166074201465 | Validation Loss:  0.001929452526383102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  764 | Train Loss:  0.0017655979609116912 | Validation Loss:  0.0017544464208185673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  765 | Train Loss:  0.001803582883439958 | Validation Loss:  0.001904051285237074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  766 | Train Loss:  0.0017429509898647666 | Validation Loss:  0.0017327616224065423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  767 | Train Loss:  0.0017728378297761083 | Validation Loss:  0.0018757046200335026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  768 | Train Loss:  0.0017194347456097603 | Validation Loss:  0.0017141919815912843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  769 | Train Loss:  0.0017411843873560429 | Validation Loss:  0.0018561504548415542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  770 | Train Loss:  0.0017047732835635543 | Validation Loss:  0.0017065246356651187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  771 | Train Loss:  0.0017219261499121785 | Validation Loss:  0.0018498033750802279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  772 | Train Loss:  0.001702258363366127 | Validation Loss:  0.001709584379568696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  773 | Train Loss:  0.0017167601035907865 | Validation Loss:  0.0018554156413301826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  774 | Train Loss:  0.0017102634301409125 | Validation Loss:  0.0017197063425555825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  775 | Train Loss:  0.0017220939043909311 | Validation Loss:  0.0018688602140173316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  776 | Train Loss:  0.0017246053321287036 | Validation Loss:  0.0017321654595434666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  777 | Train Loss:  0.0017328846734017134 | Validation Loss:  0.001884789438918233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  778 | Train Loss:  0.001739908941090107 | Validation Loss:  0.0017427678685635328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  779 | Train Loss:  0.0017447113059461117 | Validation Loss:  0.0018988483352586627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  780 | Train Loss:  0.0017518053064122796 | Validation Loss:  0.0017497636144980788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  781 | Train Loss:  0.001755662728101015 | Validation Loss:  0.0019097654148936272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  782 | Train Loss:  0.001759218517690897 | Validation Loss:  0.001754343044012785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  783 | Train Loss:  0.0017665857449173927 | Validation Loss:  0.0019187808502465487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  784 | Train Loss:  0.0017639430006965995 | Validation Loss:  0.0017586483154445887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  785 | Train Loss:  0.0017787961987778544 | Validation Loss:  0.0019267204916104674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  786 | Train Loss:  0.0017676636343821883 | Validation Loss:  0.0017631883965805173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  787 | Train Loss:  0.001791272428818047 | Validation Loss:  0.0019321151776239276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  788 | Train Loss:  0.0017698511946946383 | Validation Loss:  0.0017660993617027998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  789 | Train Loss:  0.001800042693503201 | Validation Loss:  0.0019323929445818067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  790 | Train Loss:  0.0017686319770291448 | Validation Loss:  0.001765062683261931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  791 | Train Loss:  0.0018008785555139184 | Validation Loss:  0.0019268974428996444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  792 | Train Loss:  0.0017635456752032042 | Validation Loss:  0.001760298851877451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  793 | Train Loss:  0.0017933712806552649 | Validation Loss:  0.001918504131026566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  794 | Train Loss:  0.0017571832286193967 | Validation Loss:  0.0017552805365994573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  795 | Train Loss:  0.0017819376662373543 | Validation Loss:  0.0019120455253869295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  796 | Train Loss:  0.0017539337277412415 | Validation Loss:  0.0017542123096063733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  797 | Train Loss:  0.001772440504282713 | Validation Loss:  0.0019112048903480172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  798 | Train Loss:  0.0017572024371474981 | Validation Loss:  0.0017588097834959626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  799 | Train Loss:  0.0017680164892226458 | Validation Loss:  0.0019157283240929246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  800 | Train Loss:  0.0017666425555944443 | Validation Loss:  0.0017660364974290133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  801 | Train Loss:  0.0017666860949248075 | Validation Loss:  0.0019194722408428788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  802 | Train Loss:  0.0017756983870640397 | Validation Loss:  0.001767865032888949\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  803 | Train Loss:  0.001761613180860877 | Validation Loss:  0.0019125206163153052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  804 | Train Loss:  0.0017729863757267594 | Validation Loss:  0.0017568556359037757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  805 | Train Loss:  0.0017470475286245346 | Validation Loss:  0.0018919475842267275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  806 | Train Loss:  0.001753315795212984 | Validation Loss:  0.0017356452299281955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  807 | Train Loss:  0.0017276597209274769 | Validation Loss:  0.001870452193543315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  808 | Train Loss:  0.0017278236337006092 | Validation Loss:  0.001717150560580194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  809 | Train Loss:  0.001718623097985983 | Validation Loss:  0.0018673039739951491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  810 | Train Loss:  0.0017158039845526218 | Validation Loss:  0.0017165206372737885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  811 | Train Loss:  0.001737584243528545 | Validation Loss:  0.0018935903208330274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  812 | Train Loss:  0.0017305578803643584 | Validation Loss:  0.001743624801747501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  813 | Train Loss:  0.001794197945855558 | Validation Loss:  0.0019336234545335174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  814 | Train Loss:  0.001762261032126844 | Validation Loss:  0.0017754638101905584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  815 | Train Loss:  0.0018506769556552172 | Validation Loss:  0.0019287856994196773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  816 | Train Loss:  0.0017580530839040875 | Validation Loss:  0.0017501512775197625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  817 | Train Loss:  0.0018172294367104769 | Validation Loss:  0.0018688039854168892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  818 | Train Loss:  0.0017052978510037065 | Validation Loss:  0.0016985188703984022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  819 | Train Loss:  0.0017332058632746339 | Validation Loss:  0.001829340704716742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  820 | Train Loss:  0.0016738070407882333 | Validation Loss:  0.0016986459959298372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  821 | Train Loss:  0.0017059789970517159 | Validation Loss:  0.0018627212848514318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  822 | Train Loss:  0.0017134329536929727 | Validation Loss:  0.0017737677553668618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  823 | Train Loss:  0.001765205874107778 | Validation Loss:  0.001983579248189926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  824 | Train Loss:  0.0018410132033750415 | Validation Loss:  0.0018786382861435413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  825 | Train Loss:  0.0018634484149515629 | Validation Loss:  0.002090836176648736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  826 | Train Loss:  0.0019518964691087604 | Validation Loss:  0.001882026786915958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  827 | Train Loss:  0.001872015418484807 | Validation Loss:  0.0020515271462500095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  828 | Train Loss:  0.0019027371890842915 | Validation Loss:  0.0018220648635178804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  829 | Train Loss:  0.0018303500255569816 | Validation Loss:  0.0019890787079930305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  830 | Train Loss:  0.0018270212458446622 | Validation Loss:  0.0017932262271642685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  831 | Train Loss:  0.0018242703517898917 | Validation Loss:  0.001964407041668892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  832 | Train Loss:  0.001795163145288825 | Validation Loss:  0.0017835003091022372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  833 | Train Loss:  0.0018295932095497847 | Validation Loss:  0.0019426158396527171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  834 | Train Loss:  0.0017726728692650795 | Validation Loss:  0.0017655122792348266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  835 | Train Loss:  0.0018117245053872466 | Validation Loss:  0.0019112533191218972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  836 | Train Loss:  0.0017452616011723876 | Validation Loss:  0.0017382069490849972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  837 | Train Loss:  0.0017724980134516954 | Validation Loss:  0.001877600559964776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  838 | Train Loss:  0.0017175249522551894 | Validation Loss:  0.0017154646338894963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  839 | Train Loss:  0.0017348929541185498 | Validation Loss:  0.0018526746425777674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  840 | Train Loss:  0.0016989889554679394 | Validation Loss:  0.0017042538383975625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  841 | Train Loss:  0.001710279961116612 | Validation Loss:  0.0018407755997031927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  842 | Train Loss:  0.001692219520919025 | Validation Loss:  0.0017029291484504938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  843 | Train Loss:  0.0016997451893985271 | Validation Loss:  0.0018389943288639188\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  844 | Train Loss:  0.0016940259374678135 | Validation Loss:  0.0017067920416593552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  845 | Train Loss:  0.0016979493666440248 | Validation Loss:  0.0018427708419039845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  846 | Train Loss:  0.0016996198100969195 | Validation Loss:  0.0017113880021497607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  847 | Train Loss:  0.0017002725508064032 | Validation Loss:  0.0018483094172552228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  848 | Train Loss:  0.0017050746828317642 | Validation Loss:  0.0017145037418231368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  849 | Train Loss:  0.001704217866063118 | Validation Loss:  0.0018543318146839738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  850 | Train Loss:  0.0017090473556891084 | Validation Loss:  0.0017165892058983445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  851 | Train Loss:  0.00171014538500458 | Validation Loss:  0.001861950964666903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  852 | Train Loss:  0.0017129041953012347 | Validation Loss:  0.0017198259010910988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  853 | Train Loss:  0.0017198999412357807 | Validation Loss:  0.0018726533744484186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  854 | Train Loss:  0.0017187590710818768 | Validation Loss:  0.0017260013846680522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  855 | Train Loss:  0.00173447304405272 | Validation Loss:  0.001885915407910943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  856 | Train Loss:  0.0017270446987822652 | Validation Loss:  0.001734764315187931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  857 | Train Loss:  0.0017519006505608559 | Validation Loss:  0.0018985630013048649\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  858 | Train Loss:  0.001735665719024837 | Validation Loss:  0.0017433626344427466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  859 | Train Loss:  0.001767116948030889 | Validation Loss:  0.0019067323300987482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  860 | Train Loss:  0.0017415952170267701 | Validation Loss:  0.0017487937584519386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  861 | Train Loss:  0.001774948905222118 | Validation Loss:  0.001909311511553824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  862 | Train Loss:  0.0017440160736441612 | Validation Loss:  0.0017510580364614725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  863 | Train Loss:  0.0017746621742844582 | Validation Loss:  0.0019094398012384772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  864 | Train Loss:  0.0017458826769143343 | Validation Loss:  0.0017537055537104607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  865 | Train Loss:  0.0017707647057250142 | Validation Loss:  0.0019120462238788605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  866 | Train Loss:  0.001751863630488515 | Validation Loss:  0.0017603179439902306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  867 | Train Loss:  0.001768541638739407 | Validation Loss:  0.0019192659528926015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  868 | Train Loss:  0.0017640185542404652 | Validation Loss:  0.0017698927549645305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  869 | Train Loss:  0.001768556539900601 | Validation Loss:  0.001926144352182746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  870 | Train Loss:  0.0017771143466234207 | Validation Loss:  0.001774297095835209\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  871 | Train Loss:  0.0017643349710851908 | Validation Loss:  0.0019206361612305045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  872 | Train Loss:  0.001777547993697226 | Validation Loss:  0.0017633311217650771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  873 | Train Loss:  0.0017477988731116056 | Validation Loss:  0.0018959837034344673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  874 | Train Loss:  0.0017556007951498032 | Validation Loss:  0.0017377716721966863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  875 | Train Loss:  0.0017222481546923518 | Validation Loss:  0.0018655903404578567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  876 | Train Loss:  0.0017223625909537077 | Validation Loss:  0.00171239348128438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  877 | Train Loss:  0.0017049055313691497 | Validation Loss:  0.0018535268027335405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  878 | Train Loss:  0.0017017367063090205 | Validation Loss:  0.0017048369627445936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  879 | Train Loss:  0.0017162581207230687 | Validation Loss:  0.0018744324333965778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  880 | Train Loss:  0.0017107268795371056 | Validation Loss:  0.001727554015815258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  881 | Train Loss:  0.0017690845998004079 | Validation Loss:  0.0019143534591421485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  882 | Train Loss:  0.0017418895149603486 | Validation Loss:  0.0017598054837435484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  883 | Train Loss:  0.0018278183415532112 | Validation Loss:  0.0019099755445495248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  884 | Train Loss:  0.0017383955419063568 | Validation Loss:  0.0017334399744868279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  885 | Train Loss:  0.0017930206377059221 | Validation Loss:  0.0018458371050655842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  886 | Train Loss:  0.0016820376040413976 | Validation Loss:  0.0016787206986919045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  887 | Train Loss:  0.0017041920218616724 | Validation Loss:  0.0018031871877610683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  888 | Train Loss:  0.0016474907752126455 | Validation Loss:  0.0016795267583802342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  889 | Train Loss:  0.0016771565424278378 | Validation Loss:  0.0018373896600678563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  890 | Train Loss:  0.0016878466121852398 | Validation Loss:  0.0017592408694326878\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  891 | Train Loss:  0.0017413792666047812 | Validation Loss:  0.0019618389196693897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  892 | Train Loss:  0.0018192512216046453 | Validation Loss:  0.0018629105761647224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  893 | Train Loss:  0.001839397824369371 | Validation Loss:  0.002063916064798832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  894 | Train Loss:  0.0019232211634516716 | Validation Loss:  0.0018629013793542981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  895 | Train Loss:  0.0018480306025594473 | Validation Loss:  0.002030637813732028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  896 | Train Loss:  0.0018776077777147293 | Validation Loss:  0.0018198024481534958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  897 | Train Loss:  0.0018257928313687444 | Validation Loss:  0.0019990296568721533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  898 | Train Loss:  0.0018307986902073026 | Validation Loss:  0.0018072652164846659\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  899 | Train Loss:  0.0018399268155917525 | Validation Loss:  0.0019817526917904615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  900 | Train Loss:  0.0018065939657390118 | Validation Loss:  0.0017957077361643314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  901 | Train Loss:  0.0018388406606391072 | Validation Loss:  0.0019472079584375024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  902 | Train Loss:  0.0017739104805514216 | Validation Loss:  0.0017644893378019333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  903 | Train Loss:  0.0018031334038823843 | Validation Loss:  0.0019014788558706641\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  904 | Train Loss:  0.0017345669912174344 | Validation Loss:  0.0017290905816480517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  905 | Train Loss:  0.001751075848005712 | Validation Loss:  0.0018618099857121706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  906 | Train Loss:  0.0017024015542119741 | Validation Loss:  0.0017037788638845086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  907 | Train Loss:  0.0017108122119680047 | Validation Loss:  0.0018332506297156215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  908 | Train Loss:  0.0016808179207146168 | Validation Loss:  0.0016906358068808913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  909 | Train Loss:  0.0016850942047312856 | Validation Loss:  0.0018174268770962954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  910 | Train Loss:  0.0016702827997505665 | Validation Loss:  0.0016858744202181697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  911 | Train Loss:  0.0016719981795176864 | Validation Loss:  0.001810263143852353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  912 | Train Loss:  0.0016666173469275236 | Validation Loss:  0.0016854259883984923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  913 | Train Loss:  0.0016665856819599867 | Validation Loss:  0.0018088778015226126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  914 | Train Loss:  0.0016668293392285705 | Validation Loss:  0.0016867087688297033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  915 | Train Loss:  0.0016659953398630023 | Validation Loss:  0.0018115548882633448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  916 | Train Loss:  0.0016691306373104453 | Validation Loss:  0.0016888080863282084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  917 | Train Loss:  0.0016691461205482483 | Validation Loss:  0.0018180395709350705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  918 | Train Loss:  0.0016732901567593217 | Validation Loss:  0.0016922734212130308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  919 | Train Loss:  0.0016764962347224355 | Validation Loss:  0.0018289323197677732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  920 | Train Loss:  0.001680160639807582 | Validation Loss:  0.001698406063951552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  921 | Train Loss:  0.0016890971455723047 | Validation Loss:  0.0018444490851834416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  922 | Train Loss:  0.001690537086687982 | Validation Loss:  0.0017081111436709762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  923 | Train Loss:  0.0017071344191208482 | Validation Loss:  0.0018631353741511703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  924 | Train Loss:  0.0017038725782185793 | Validation Loss:  0.0017207153141498566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  925 | Train Loss:  0.0017284408677369356 | Validation Loss:  0.0018815652001649141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  926 | Train Loss:  0.001717807026579976 | Validation Loss:  0.0017336419550701976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  927 | Train Loss:  0.0017482434632256627 | Validation Loss:  0.001895960420370102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  928 | Train Loss:  0.001729438197799027 | Validation Loss:  0.0017440238734707236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  929 | Train Loss:  0.0017615826800465584 | Validation Loss:  0.0019050008850172162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  930 | Train Loss:  0.0017378510674461722 | Validation Loss:  0.0017513670027256012\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  931 | Train Loss:  0.001767110894434154 | Validation Loss:  0.0019108891719952226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  932 | Train Loss:  0.001745245885103941 | Validation Loss:  0.0017578977858647704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  933 | Train Loss:  0.0017677485011518002 | Validation Loss:  0.0019167996942996979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  934 | Train Loss:  0.001754695433191955 | Validation Loss:  0.0017652673413977027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  935 | Train Loss:  0.0017666772473603487 | Validation Loss:  0.001922388793900609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  936 | Train Loss:  0.0017657161224633455 | Validation Loss:  0.0017705499194562435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  937 | Train Loss:  0.0017627241322770715 | Validation Loss:  0.0019209551392123103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  938 | Train Loss:  0.0017708204686641693 | Validation Loss:  0.0017661018064245582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  939 | Train Loss:  0.0017502930713817477 | Validation Loss:  0.001903725671581924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  940 | Train Loss:  0.001759026199579239 | Validation Loss:  0.0017470077145844698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  941 | Train Loss:  0.0017268949886783957 | Validation Loss:  0.0018729075090959668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  942 | Train Loss:  0.0017295930301770568 | Validation Loss:  0.0017196154221892357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  943 | Train Loss:  0.0017014339100569487 | Validation Loss:  0.0018464182503521442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  944 | Train Loss:  0.0016986557748168707 | Validation Loss:  0.0016984676476567984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  945 | Train Loss:  0.0016913098515942693 | Validation Loss:  0.0018445700407028198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  946 | Train Loss:  0.0016868386883288622 | Validation Loss:  0.0016994266770780087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  947 | Train Loss:  0.0017155992100015283 | Validation Loss:  0.0018755228957161307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  948 | Train Loss:  0.001706162584014237 | Validation Loss:  0.001730634248815477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  949 | Train Loss:  0.0017798482440412045 | Validation Loss:  0.001906750025227666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  950 | Train Loss:  0.001732733566313982 | Validation Loss:  0.0017454164335504174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  951 | Train Loss:  0.0018104346236214042 | Validation Loss:  0.001865882659330964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  952 | Train Loss:  0.0016977040795609355 | Validation Loss:  0.001687657437287271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  953 | Train Loss:  0.0017255188431590796 | Validation Loss:  0.001791304792277515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  954 | Train Loss:  0.0016326014883816242 | Validation Loss:  0.0016453193966299295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  955 | Train Loss:  0.0016465525841340423 | Validation Loss:  0.001774478703737259\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  956 | Train Loss:  0.001622758456505835 | Validation Loss:  0.0016876987647265196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  957 | Train Loss:  0.0016668493626639247 | Validation Loss:  0.0018678867490962148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  958 | Train Loss:  0.0017219743458554149 | Validation Loss:  0.0018174489960074425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  959 | Train Loss:  0.0017865182599052787 | Validation Loss:  0.0020341535564512014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  960 | Train Loss:  0.0018942947499454021 | Validation Loss:  0.0018807072192430496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  961 | Train Loss:  0.0018549230881035328 | Validation Loss:  0.0020652201492339373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  962 | Train Loss:  0.0019138931529596448 | Validation Loss:  0.0018385251751169562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  963 | Train Loss:  0.0018330174498260021 | Validation Loss:  0.0020148169714957476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  964 | Train Loss:  0.0018474333919584751 | Validation Loss:  0.00181281054392457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  965 | Train Loss:  0.0018358512315899134 | Validation Loss:  0.0019920754712074995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  966 | Train Loss:  0.0018140836618840694 | Validation Loss:  0.0018087582429870963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  967 | Train Loss:  0.0018447214970365167 | Validation Loss:  0.0019636854995042086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  968 | Train Loss:  0.001787691144272685 | Validation Loss:  0.0017793059814721346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  969 | Train Loss:  0.0018129360396414995 | Validation Loss:  0.0019188408041372895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  970 | Train Loss:  0.0017475291388109326 | Validation Loss:  0.001744561130180955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  971 | Train Loss:  0.001764446496963501 | Validation Loss:  0.001875604735687375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  972 | Train Loss:  0.0017129709012806416 | Validation Loss:  0.001712544122710824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  973 | Train Loss:  0.001716526341624558 | Validation Loss:  0.001839308300986886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  974 | Train Loss:  0.0016835520509630442 | Validation Loss:  0.001690855948254466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  975 | Train Loss:  0.0016830145614221692 | Validation Loss:  0.0018125656060874462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  976 | Train Loss:  0.0016628545708954334 | Validation Loss:  0.001677399268373847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  977 | Train Loss:  0.0016606831923127174 | Validation Loss:  0.0017952063353732228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  978 | Train Loss:  0.0016494161682203412 | Validation Loss:  0.0016696482198312879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  979 | Train Loss:  0.0016474692383781075 | Validation Loss:  0.001785392058081925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  980 | Train Loss:  0.0016416856087744236 | Validation Loss:  0.001666053431108594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  981 | Train Loss:  0.001641081995330751 | Validation Loss:  0.0017823989037424326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  982 | Train Loss:  0.0016390013042837381 | Validation Loss:  0.0016659206012263894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  983 | Train Loss:  0.0016405920032411814 | Validation Loss:  0.0017856864724308252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  984 | Train Loss:  0.0016409915406256914 | Validation Loss:  0.0016691337805241346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  985 | Train Loss:  0.0016456381417810917 | Validation Loss:  0.0017949920147657394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  986 | Train Loss:  0.0016476139426231384 | Validation Loss:  0.001675861538387835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  987 | Train Loss:  0.001656160457059741 | Validation Loss:  0.001809818553738296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  988 | Train Loss:  0.001658712630160153 | Validation Loss:  0.0016862208722159266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  989 | Train Loss:  0.0016718585975468159 | Validation Loss:  0.001829083077609539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  990 | Train Loss:  0.001673700287938118 | Validation Loss:  0.0016998591599985957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  991 | Train Loss:  0.001691648387350142 | Validation Loss:  0.0018508537905290723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  992 | Train Loss:  0.0016912779537960887 | Validation Loss:  0.0017156059620901942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  993 | Train Loss:  0.0017132856883108616 | Validation Loss:  0.0018725728150457144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  994 | Train Loss:  0.0017095409566536546 | Validation Loss:  0.0017315145814791322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  995 | Train Loss:  0.0017336348537355661 | Validation Loss:  0.0018916924018412828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  996 | Train Loss:  0.0017264625057578087 | Validation Loss:  0.0017453859327360988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  997 | Train Loss:  0.0017496144864708185 | Validation Loss:  0.0019062139326706529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  998 | Train Loss:  0.001740326639264822 | Validation Loss:  0.0017553339712321758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  999 | Train Loss:  0.0017591114155948162 | Validation Loss:  0.0019146293634548783\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Test Loss:  0.0015805650036782026\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"model_weights_best.pth\"\n",
    "\n",
    "setups = [\n",
    "    {\"n_epochs\": 300, \"learning_rate\": 0.01, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.01, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.1, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"SGD\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"RMSprop\"}\n",
    "]\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 3\n",
    "\n",
    "for setup in setups:\n",
    "    n_epochs = setup[\"n_epochs\"]\n",
    "    learning_rate = setup[\"learning_rate\"]\n",
    "    optimizer_name = setup[\"optimizer\"]\n",
    "\n",
    "    rnn = RNN(input_size, hidden_size, num_layers, output_size)\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(rnn.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer name: {optimizer_name}\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnn.train()\n",
    "        hidden_state = None\n",
    "        output, hidden_state = rnn(X_train, hidden_state)\n",
    "        loss = criterion(output.view(-1), y_train)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        rnn.eval()\n",
    "        val_hidden_state = None\n",
    "        test_output, _ = rnn(X_val, val_hidden_state)\n",
    "        val_loss = criterion(test_output.view(-1), y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "        print('Setup: ', setup, 'Epoch: ', epoch, '| Train Loss: ', loss.item(), '| Validation Loss: ', val_loss.item())\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(rnn.state_dict(), best_model_path)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"Loss Curves for setup: {setup}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    rnn.eval()\n",
    "    test_hidden_state = None\n",
    "    pred_test, _ = rnn(X_test, test_hidden_state)\n",
    "    test_loss = criterion(pred_test.view(-1), y_test)\n",
    "    print('Setup: ', setup, 'Test Loss: ', test_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fTkKpLe5Vc28",
   "metadata": {
    "id": "fTkKpLe5Vc28"
   },
   "source": [
    "### Step 8: Model Traning : **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "GmPUsjY4VgP7",
   "metadata": {
    "id": "GmPUsjY4VgP7"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, i_size, h_size, n_layers, o_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=i_size,\n",
    "            hidden_size=h_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True)\n",
    "        self.out = nn.Linear(h_size, o_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.lstm(x, None)\n",
    "        outs = self.out(r_out[:, -1, :])\n",
    "        return outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e3982",
   "metadata": {},
   "source": [
    "### Step 9: Hyperparameters setups  - 6 Setups (BY changing 3 parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UT3Uosf4Vp7S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UT3Uosf4Vp7S",
    "outputId": "5013140c-46f0-4eea-edaf-bcb312975f10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.08273070305585861 | Validation Loss:  0.04841369763016701\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.0530138798058033 | Validation Loss:  0.025594227015972137\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.029277393594384193 | Validation Loss:  0.012143105268478394\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.014545761980116367 | Validation Loss:  0.023148594424128532\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.02382880635559559 | Validation Loss:  0.025113968178629875\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.02571435086429119 | Validation Loss:  0.016970865428447723\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.01819903776049614 | Validation Loss:  0.012595933862030506\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.014524719677865505 | Validation Loss:  0.012607560493052006\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.01507289707660675 | Validation Loss:  0.01408793218433857\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.01687781699001789 | Validation Loss:  0.015093930065631866\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.018020236864686012 | Validation Loss:  0.015083717182278633\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.018001781776547432 | Validation Loss:  0.014267760328948498\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.01706993393599987 | Validation Loss:  0.013090084306895733\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.015694476664066315 | Validation Loss:  0.011994298547506332\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.014336695894598961 | Validation Loss:  0.011325540021061897\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.013356919400393963 | Validation Loss:  0.011252985335886478\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.012944096699357033 | Validation Loss:  0.011660420335829258\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.013013680465519428 | Validation Loss:  0.012073712423443794\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.013136441819369793 | Validation Loss:  0.011861937120556831\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.012724792584776878 | Validation Loss:  0.010752310045063496\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.01152031496167183 | Validation Loss:  0.0090873334556818\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.009838679805397987 | Validation Loss:  0.007446313742548227\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.008203783072531223 | Validation Loss:  0.006126181222498417\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.006849760189652443 | Validation Loss:  0.005015108268707991\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.0056145465932786465 | Validation Loss:  0.0038633637595921755\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.004234416410326958 | Validation Loss:  0.00271710567176342\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.0028083862271159887 | Validation Loss:  0.002191255334764719\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.0020873439498245716 | Validation Loss:  0.0025502629578113556\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.0024513087701052427 | Validation Loss:  0.002428274368867278\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.002396906493231654 | Validation Loss:  0.0024302040692418814\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.002403652761131525 | Validation Loss:  0.003233548253774643\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.0032014918979257345 | Validation Loss:  0.0035539830569177866\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.003535747528076172 | Validation Loss:  0.0031478542368859053\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.003162769367918372 | Validation Loss:  0.0028297158423811197\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.0028782752342522144 | Validation Loss:  0.002695710165426135\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.0027363260742276907 | Validation Loss:  0.0022615273483097553\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.0022821801248937845 | Validation Loss:  0.0018541041063144803\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.0019174086628481746 | Validation Loss:  0.0017708390951156616\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.0019336797995492816 | Validation Loss:  0.0017861610976979136\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.0020369545090943575 | Validation Loss:  0.0017489383462816477\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.0020291688852012157 | Validation Loss:  0.0017794705927371979\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.0020332089625298977 | Validation Loss:  0.0019595823250710964\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.0021671478170901537 | Validation Loss:  0.002106415806338191\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.002290140837430954 | Validation Loss:  0.0020452130120247602\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.0022461030166596174 | Validation Loss:  0.001894393120892346\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.0021371720358729362 | Validation Loss:  0.0018154314020648599\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.002088651293888688 | Validation Loss:  0.0017811032012104988\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.0020434383768588305 | Validation Loss:  0.0017197707202285528\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.001924742478877306 | Validation Loss:  0.0016729972558096051\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.0017960325349122286 | Validation Loss:  0.0017066276632249355\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.0017577300313860178 | Validation Loss:  0.001752162235789001\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.0017635662807151675 | Validation Loss:  0.0017286622896790504\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.00172939442563802 | Validation Loss:  0.0017152620712295175\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.001717930193990469 | Validation Loss:  0.0017639799043536186\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.0017659013392403722 | Validation Loss:  0.001798462588340044\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.0017888486618176103 | Validation Loss:  0.0017923442646861076\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.0017637917771935463 | Validation Loss:  0.0018022505100816488\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.0017572409706190228 | Validation Loss:  0.001814963179640472\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.0017608386697247624 | Validation Loss:  0.0017778432229533792\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.001721923821605742 | Validation Loss:  0.0017313087591901422\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.00168135529384017 | Validation Loss:  0.0017132600769400597\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0016724868910387158 | Validation Loss:  0.0016962123336270452\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.0016583818942308426 | Validation Loss:  0.0016791181406006217\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.0016346058109775186 | Validation Loss:  0.0016891378909349442\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.0016345563344657421 | Validation Loss:  0.0017064640996977687\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.0016471631824970245 | Validation Loss:  0.0016988638089969754\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.001644529984332621 | Validation Loss:  0.001685247989371419\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.0016426276415586472 | Validation Loss:  0.001684833667241037\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0016513541340827942 | Validation Loss:  0.0016852756962180138\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0016499683260917664 | Validation Loss:  0.0016846382059156895\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0016370131634175777 | Validation Loss:  0.0016924215015023947\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0016303174197673798 | Validation Loss:  0.0016960699576884508\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.001625226577743888 | Validation Loss:  0.0016836646245792508\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0016115815378725529 | Validation Loss:  0.0016717120306566358\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.0016019754111766815 | Validation Loss:  0.0016704455483704805\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.001600604853592813 | Validation Loss:  0.0016716445097699761\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.0015961015596985817 | Validation Loss:  0.0016757241683080792\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.0015914340037852526 | Validation Loss:  0.0016844341298565269\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0015933900140225887 | Validation Loss:  0.0016864011995494366\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0015932434471324086 | Validation Loss:  0.0016801378224045038\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0015888959169387817 | Validation Loss:  0.0016754546668380499\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.0015873885713517666 | Validation Loss:  0.0016716055106371641\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.001584610785357654 | Validation Loss:  0.0016667713643983006\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.001577746239490807 | Validation Loss:  0.0016648360760882497\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0015730045270174742 | Validation Loss:  0.001661838497966528\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0015695466427132487 | Validation Loss:  0.0016532045556232333\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0015640908386558294 | Validation Loss:  0.001644565723836422\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0015606716042384505 | Validation Loss:  0.0016394713893532753\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.001559263444505632 | Validation Loss:  0.0016367409843951464\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.001556411967612803 | Validation Loss:  0.0016370135126635432\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.0015540783060714602 | Validation Loss:  0.001637582783587277\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.0015528356889262795 | Validation Loss:  0.0016337574925273657\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.0015499204164370894 | Validation Loss:  0.0016278272960335016\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.0015466008335351944 | Validation Loss:  0.001623910153284669\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.0015440728748217225 | Validation Loss:  0.001622206182219088\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.0015405736630782485 | Validation Loss:  0.0016226050211116672\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.0015369633911177516 | Validation Loss:  0.0016235867515206337\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.0015344652347266674 | Validation Loss:  0.001621982199139893\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0015317002544179559 | Validation Loss:  0.0016186415450647473\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0015290055889636278 | Validation Loss:  0.0016163825057446957\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0015271559823304415 | Validation Loss:  0.0016155113698914647\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0015249296557158232 | Validation Loss:  0.0016156875062733889\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0015224684029817581 | Validation Loss:  0.0016156731871888041\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.001520375139079988 | Validation Loss:  0.0016133470926433802\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0015178150497376919 | Validation Loss:  0.0016093963058665395\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.001515075913630426 | Validation Loss:  0.0016060001216828823\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0015126902144402266 | Validation Loss:  0.0016038796165958047\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.0015101009048521519 | Validation Loss:  0.0016029643593356013\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0015075983246788383 | Validation Loss:  0.0016020681941881776\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0015054730465635657 | Validation Loss:  0.0015997341834008694\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0015032185474410653 | Validation Loss:  0.0015967026120051742\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0015010337810963392 | Validation Loss:  0.0015945202903822064\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0014990142080932856 | Validation Loss:  0.0015936323907226324\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0014967777533456683 | Validation Loss:  0.0015936256386339664\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0014945517759770155 | Validation Loss:  0.0015932404203340411\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.0014923946000635624 | Validation Loss:  0.0015916936099529266\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0014900988899171352 | Validation Loss:  0.0015898863784968853\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0014879138907417655 | Validation Loss:  0.00158886075951159\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0014858212089166045 | Validation Loss:  0.0015887448098510504\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.0014836877817288041 | Validation Loss:  0.0015889182686805725\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.001481686602346599 | Validation Loss:  0.0015883486485108733\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.0014797123149037361 | Validation Loss:  0.0015868721529841423\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.001477692974731326 | Validation Loss:  0.0015853546792641282\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0014757447643205523 | Validation Loss:  0.0015844008885324001\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0014737577876076102 | Validation Loss:  0.0015839309198781848\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0014717556769028306 | Validation Loss:  0.0015832954086363316\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0014698212035000324 | Validation Loss:  0.0015819371910765767\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0014678731095045805 | Validation Loss:  0.001580229727551341\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0014659728622063994 | Validation Loss:  0.0015789107419550419\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0014641322195529938 | Validation Loss:  0.0015782237751409411\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.0014622852904722095 | Validation Loss:  0.0015778050292283297\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.0014604856260120869 | Validation Loss:  0.0015770309837535024\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.001458698883652687 | Validation Loss:  0.0015758211957290769\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.0014569043414667249 | Validation Loss:  0.0015747196739539504\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0014551469357684255 | Validation Loss:  0.0015741167590022087\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0014533904613927007 | Validation Loss:  0.0015738732181489468\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0014516558730974793 | Validation Loss:  0.0015734812477603555\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.0014499615645036101 | Validation Loss:  0.001572655513882637\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0014482788974419236 | Validation Loss:  0.0015716972993686795\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.001446633250452578 | Validation Loss:  0.001571011496707797\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0014450093731284142 | Validation Loss:  0.001570619991980493\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0014433953911066055 | Validation Loss:  0.001570170628838241\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0014418099308386445 | Validation Loss:  0.0015693563036620617\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.0014402334345504642 | Validation Loss:  0.0015683333622291684\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0014386788243427873 | Validation Loss:  0.0015674837632104754\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0014371502911671996 | Validation Loss:  0.0015669323038309813\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0014356381725519896 | Validation Loss:  0.001566446153447032\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0014341571368277073 | Validation Loss:  0.0015657421899959445\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0014326964737847447 | Validation Loss:  0.0015648840926587582\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0014312572311609983 | Validation Loss:  0.0015641775680705905\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.0014298416208475828 | Validation Loss:  0.0015637538162991405\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0014284411445260048 | Validation Loss:  0.0015634286683052778\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0014270639512687922 | Validation Loss:  0.0015629471745342016\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0014257067814469337 | Validation Loss:  0.001562326680868864\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.001424371381290257 | Validation Loss:  0.0015618052566424012\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.001423061010427773 | Validation Loss:  0.001561490586027503\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0014217706630006433 | Validation Loss:  0.001561229582875967\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0014205042971298099 | Validation Loss:  0.0015608144458383322\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0014192587696015835 | Validation Loss:  0.0015602685743942857\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.001418033498339355 | Validation Loss:  0.001559794065542519\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.0014168305788189173 | Validation Loss:  0.001559479278512299\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0014156466349959373 | Validation Loss:  0.001559196156449616\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.001414485857822001 | Validation Loss:  0.0015587937086820602\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.001413346384651959 | Validation Loss:  0.001558323041535914\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.001412229030393064 | Validation Loss:  0.0015579551691189408\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.0014111336786299944 | Validation Loss:  0.0015577359590679407\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0014100588159635663 | Validation Loss:  0.0015575357247143984\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.0014090058393776417 | Validation Loss:  0.0015572455013170838\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0014079727698117495 | Validation Loss:  0.0015569347888231277\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0014069611206650734 | Validation Loss:  0.0015567316440865397\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0014059704262763262 | Validation Loss:  0.0015566269867122173\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.0014050006866455078 | Validation Loss:  0.00155649334192276\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0014040522510185838 | Validation Loss:  0.0015562736662104726\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.001403124537318945 | Validation Loss:  0.001556058065034449\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.0014022180112078786 | Validation Loss:  0.001555930939503014\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.001401331159286201 | Validation Loss:  0.0015558445593342185\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.001400464796461165 | Validation Loss:  0.0015556997386738658\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.001399618573486805 | Validation Loss:  0.001555503229610622\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0013987923739477992 | Validation Loss:  0.001555351889692247\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0013979864306747913 | Validation Loss:  0.001555279828608036\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.0013972001615911722 | Validation Loss:  0.0015552141703665257\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.0013964340323582292 | Validation Loss:  0.0015550984535366297\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.001395686878822744 | Validation Loss:  0.0015549814561381936\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0013949592830613256 | Validation Loss:  0.0015549324452877045\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.0013942503137513995 | Validation Loss:  0.0015549269737675786\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.0013935602037236094 | Validation Loss:  0.0015548907686024904\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.0013928889529779553 | Validation Loss:  0.0015548196388408542\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.0013922362122684717 | Validation Loss:  0.0015547765651717782\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.001391601632349193 | Validation Loss:  0.0015547809889540076\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.0013909846311435103 | Validation Loss:  0.001554777380079031\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0013903856743127108 | Validation Loss:  0.0015547308139503002\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0013898039469495416 | Validation Loss:  0.001554683898575604\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0013892394490540028 | Validation Loss:  0.0015546795912086964\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.001388691714964807 | Validation Loss:  0.0015546911163255572\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.0013881607446819544 | Validation Loss:  0.0015546747017651796\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.0013876459561288357 | Validation Loss:  0.0015546459471806884\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.001387147232890129 | Validation Loss:  0.0015546506037935615\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0013866642257198691 | Validation Loss:  0.0015546835493296385\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0013861965853720903 | Validation Loss:  0.0015547007787972689\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0013857438461855054 | Validation Loss:  0.0015546982176601887\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0013853061245754361 | Validation Loss:  0.0015547130024060607\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.0013848827220499516 | Validation Loss:  0.001554753165692091\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0013844735221937299 | Validation Loss:  0.0015547832008451223\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0013840778265148401 | Validation Loss:  0.0015547890216112137\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0013836957514286041 | Validation Loss:  0.0015547997318208218\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.0013833269476890564 | Validation Loss:  0.0015548319788649678\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0013829706003889441 | Validation Loss:  0.001554860849864781\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.001382626942358911 | Validation Loss:  0.0015548691153526306\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.0013822950422763824 | Validation Loss:  0.0015548786614090204\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0013819752493873239 | Validation Loss:  0.0015549088129773736\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.0013816667487844825 | Validation Loss:  0.0015549398958683014\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.001381369074806571 | Validation Loss:  0.001554954331368208\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0013810823438689113 | Validation Loss:  0.001554967137053609\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.0013808058574795723 | Validation Loss:  0.0015549955423921347\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0013805393828079104 | Validation Loss:  0.0015550238313153386\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.001380282687023282 | Validation Loss:  0.0015550352400168777\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.0013800354208797216 | Validation Loss:  0.0015550417592749\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0013797968858852983 | Validation Loss:  0.0015550580574199557\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0013795675477012992 | Validation Loss:  0.0015550728421658278\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0013793461257591844 | Validation Loss:  0.0015550724929198623\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.0013791328528895974 | Validation Loss:  0.0015550673706457019\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0013789273798465729 | Validation Loss:  0.0015550707466900349\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.0013787293573841453 | Validation Loss:  0.0015550728421658278\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.001378538552671671 | Validation Loss:  0.0015550621319562197\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0013783543836325407 | Validation Loss:  0.0015550489770248532\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0013781769666820765 | Validation Loss:  0.0015550422249361873\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.001378005719743669 | Validation Loss:  0.0015550319803878665\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0013778404099866748 | Validation Loss:  0.0015550099778920412\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0013776810374110937 | Validation Loss:  0.0015549848321825266\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0013775270199403167 | Validation Loss:  0.0015549635281786323\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.0013773781247437 | Validation Loss:  0.0015549364034086466\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.0013772340025752783 | Validation Loss:  0.0015548981027677655\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.001377094886265695 | Validation Loss:  0.0015548584051430225\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0013769600773230195 | Validation Loss:  0.001554821035824716\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0013768295757472515 | Validation Loss:  0.0015547767980024219\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0013767030322924256 | Validation Loss:  0.0015547249931842089\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.0013765804469585419 | Validation Loss:  0.0015546733047813177\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0013764614704996347 | Validation Loss:  0.0015546223148703575\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0013763457536697388 | Validation Loss:  0.0015545643400400877\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0013762334128841758 | Validation Loss:  0.0015544999623671174\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0013761239824816585 | Validation Loss:  0.001554436283186078\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.0013760175788775086 | Validation Loss:  0.001554371090605855\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0013759136199951172 | Validation Loss:  0.001554298447445035\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0013758124550804496 | Validation Loss:  0.0015542207984253764\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.0013757136184722185 | Validation Loss:  0.0015541438478976488\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.001375617110170424 | Validation Loss:  0.0015540639869868755\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0013755225809291005 | Validation Loss:  0.0015539771411567926\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.001375430030748248 | Validation Loss:  0.0015538884326815605\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0013753396924585104 | Validation Loss:  0.0015538003062829375\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0013752508675679564 | Validation Loss:  0.0015537075232714415\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0013751634396612644 | Validation Loss:  0.0015536099672317505\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.0013750777579843998 | Validation Loss:  0.0015535117127001286\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.0013749937061220407 | Validation Loss:  0.001553412526845932\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0013749105855822563 | Validation Loss:  0.001553308335132897\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.0013748289784416556 | Validation Loss:  0.0015532008837908506\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.001374748651869595 | Validation Loss:  0.0015530931996181607\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0013746691402047873 | Validation Loss:  0.0015529824886471033\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0013745907926931977 | Validation Loss:  0.0015528675867244601\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.0013745133765041828 | Validation Loss:  0.0015527511714026332\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0013744368916377425 | Validation Loss:  0.0015526341740041971\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0013743609888479114 | Validation Loss:  0.001552514499053359\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0013742860173806548 | Validation Loss:  0.0015523913316428661\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0013742117444053292 | Validation Loss:  0.0015522678149864078\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0013741380535066128 | Validation Loss:  0.0015521431341767311\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0013740650610998273 | Validation Loss:  0.0015520149609073997\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0013739925343543291 | Validation Loss:  0.0015518850414082408\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.00137392058968544 | Validation Loss:  0.0015517546562477946\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0013738491106778383 | Validation Loss:  0.0015516224084421992\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0013737782137468457 | Validation Loss:  0.0015514872502535582\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0013737076660618186 | Validation Loss:  0.001551351509988308\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.001373637467622757 | Validation Loss:  0.0015512151876464486\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0013735677348449826 | Validation Loss:  0.001551076420582831\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.001373498234897852 | Validation Loss:  0.0015509362565353513\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.001373428967781365 | Validation Loss:  0.0015507957432419062\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0013733601663261652 | Validation Loss:  0.0015506538329645991\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.0013732915977016091 | Validation Loss:  0.0015505101764574647\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.0013732232619076967 | Validation Loss:  0.0015503659378737211\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.001373155158944428 | Validation Loss:  0.001550221350044012\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.0013730872888118029 | Validation Loss:  0.0015500748995691538\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.001373019884340465 | Validation Loss:  0.0015499272849410772\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0013729521306231618 | Validation Loss:  0.0015497793210670352\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.0013728848425671458 | Validation Loss:  0.0015496305422857404\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0013728176709264517 | Validation Loss:  0.0015494804829359055\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.001372750848531723 | Validation Loss:  0.0015493298415094614\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0013726841425523162 | Validation Loss:  0.0015491786180064082\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0013726173201575875 | Validation Loss:  0.0015490269288420677\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0013725509634241462 | Validation Loss:  0.0015488743083551526\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.001372484490275383 | Validation Loss:  0.0015487211057916284\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.0013724181335419416 | Validation Loss:  0.0015485674375668168\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0013723522424697876 | Validation Loss:  0.001548412605188787\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.001372286002151668 | Validation Loss:  0.0015482576563954353\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0013722202274948359 | Validation Loss:  0.0015481021255254745\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.0013721543364226818 | Validation Loss:  0.0015479458961635828\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0013720884453505278 | Validation Loss:  0.0015477893175557256\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0013720229035243392 | Validation Loss:  0.0015476320404559374\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0013719572452828288 | Validation Loss:  0.0015474746469408274\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.001371891936287284 | Validation Loss:  0.0015473164385184646\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.0013718265108764172 | Validation Loss:  0.00154715811368078\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.0013717610854655504 | Validation Loss:  0.0015469995560124516\n",
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0013716958928853273 | Validation Loss:  0.0015468401834368706\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 300, 'learning_rate': 0.01, 'optimizer': 'Adam'} Test Loss:  0.0011688509257510304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.06149870157241821 | Validation Loss:  0.03264636546373367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.036699000746011734 | Validation Loss:  0.01674584671854973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.019861580803990364 | Validation Loss:  0.013334487564861774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.015215486288070679 | Validation Loss:  0.024187926203012466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.025000466033816338 | Validation Loss:  0.021160099655389786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.022197086364030838 | Validation Loss:  0.015045895241200924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.01669277995824814 | Validation Loss:  0.012821338139474392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.015069789253175259 | Validation Loss:  0.013499375432729721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.016165170818567276 | Validation Loss:  0.014486052095890045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.017337895929813385 | Validation Loss:  0.014574513770639896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.017427533864974976 | Validation Loss:  0.013823759742081165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.016552552580833435 | Validation Loss:  0.012692578136920929\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.015212928876280785 | Validation Loss:  0.011624260805547237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.013875932432711124 | Validation Loss:  0.010931629687547684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.01286917645484209 | Validation Loss:  0.010753002017736435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.012347718700766563 | Validation Loss:  0.010953000746667385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.012206848710775375 | Validation Loss:  0.011038684286177158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.012001053430140018 | Validation Loss:  0.010416895151138306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.011180317960679531 | Validation Loss:  0.008947093039751053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.009611223824322224 | Validation Loss:  0.0070926654152572155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.007718175183981657 | Validation Loss:  0.00541799608618021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.005999735556542873 | Validation Loss:  0.0040987273678183556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.004568294156342745 | Validation Loss:  0.0029678104910999537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.0032304672058671713 | Validation Loss:  0.0020289691165089607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.002049562754109502 | Validation Loss:  0.0019227020675316453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.0018381380941718817 | Validation Loss:  0.002628479152917862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.002649538917466998 | Validation Loss:  0.0028616799972951412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.0029207966290414333 | Validation Loss:  0.0035531942266970873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.0035716379061341286 | Validation Loss:  0.0038918836507946253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.003903035307303071 | Validation Loss:  0.0033433283679187298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.0033745726104825735 | Validation Loss:  0.002716827904805541\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.002760267350822687 | Validation Loss:  0.0023644533939659595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.002389497123658657 | Validation Loss:  0.001975391758605838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.001996223349124193 | Validation Loss:  0.0016982645029202104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.001786508015356958 | Validation Loss:  0.0017042679246515036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.0019034859724342823 | Validation Loss:  0.0017995787784457207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.0020801322534680367 | Validation Loss:  0.0018596525769680738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.0021537342108786106 | Validation Loss:  0.001961849629878998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.0022157025523483753 | Validation Loss:  0.0021183034405112267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.002321213483810425 | Validation Loss:  0.0021600162144750357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.002340158447623253 | Validation Loss:  0.002027536276727915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.0022204502020031214 | Validation Loss:  0.0018793876515701413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.0020951279439032078 | Validation Loss:  0.0018004599260166287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.002014156198129058 | Validation Loss:  0.0017334113363176584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.0019029841059818864 | Validation Loss:  0.0016728253103792667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.0017696430440992117 | Validation Loss:  0.0016853191191330552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.001714590354822576 | Validation Loss:  0.0017336809542030096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.0017255200073122978 | Validation Loss:  0.0017328013200312853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.0017142202705144882 | Validation Loss:  0.001746072550304234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.0017272391123697162 | Validation Loss:  0.001807940425351262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.0017856977647170424 | Validation Loss:  0.0018374001374468207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.0018037406262010336 | Validation Loss:  0.0018337422516196966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.0017863295506685972 | Validation Loss:  0.001843523234128952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.0017868780996650457 | Validation Loss:  0.0018233469454571605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.0017618772108107805 | Validation Loss:  0.0017681955359876156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.0017073610797524452 | Validation Loss:  0.0017339966725558043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.0016798311844468117 | Validation Loss:  0.0017093693604692817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.0016599248629063368 | Validation Loss:  0.001684758230112493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.0016317303525283933 | Validation Loss:  0.0016891113482415676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.0016285562887787819 | Validation Loss:  0.0017038153018802404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.001640751026570797 | Validation Loss:  0.0016967267729341984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0016410165699198842 | Validation Loss:  0.0016894711880013347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.0016470224363729358 | Validation Loss:  0.0016930940328165889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.0016585795674473047 | Validation Loss:  0.0016938121989369392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.0016542843077331781 | Validation Loss:  0.0016973698511719704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.0016441203188151121 | Validation Loss:  0.0017036792123690248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0016385501949116588 | Validation Loss:  0.001693584956228733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.0016244444996118546 | Validation Loss:  0.0016753898235037923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0016078812768682837 | Validation Loss:  0.001668107113800943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0016008587554097176 | Validation Loss:  0.0016667908057570457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0015935354167595506 | Validation Loss:  0.0016698737163096666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0015866986941546202 | Validation Loss:  0.0016792507376521826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.001588410814292729 | Validation Loss:  0.0016822114121168852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0015889581991359591 | Validation Loss:  0.0016786199994385242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.001586900558322668 | Validation Loss:  0.0016775659751147032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.001587824895977974 | Validation Loss:  0.001674896222539246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.001584443962201476 | Validation Loss:  0.0016711578937247396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.0015777194639667869 | Validation Loss:  0.0016685905866324902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0015731077874079347 | Validation Loss:  0.00166028062812984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0015662007499486208 | Validation Loss:  0.0016488580731675029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0015593867283314466 | Validation Loss:  0.001641046372242272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.0015560834435746074 | Validation Loss:  0.0016362234018743038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.0015523252077400684 | Validation Loss:  0.001635296386666596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.0015496201813220978 | Validation Loss:  0.0016356182750314474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0015489181969314814 | Validation Loss:  0.0016313043888658285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0015466253971680999 | Validation Loss:  0.0016252625500783324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0015441933646798134 | Validation Loss:  0.0016215599607676268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0015419453848153353 | Validation Loss:  0.001620034803636372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.001537873176857829 | Validation Loss:  0.0016205592546612024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0015339942183345556 | Validation Loss:  0.0016199371311813593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.0015305251581594348 | Validation Loss:  0.0016161731909960508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.001526497770100832 | Validation Loss:  0.001612923457287252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.0015236289473250508 | Validation Loss:  0.0016119827050715685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.0015212480211630464 | Validation Loss:  0.00161279970780015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.001518680714070797 | Validation Loss:  0.0016140998341143131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.0015168482204899192 | Validation Loss:  0.0016129116993397474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.001514631207101047 | Validation Loss:  0.0016095471801236272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.00151201116386801 | Validation Loss:  0.0016065506497398019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0015095651615411043 | Validation Loss:  0.001604582299478352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0015065635088831186 | Validation Loss:  0.0016035449225455523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0015036588301882148 | Validation Loss:  0.0016018141759559512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0015010504284873605 | Validation Loss:  0.0015983306802809238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0014983409782871604 | Validation Loss:  0.0015949306543916464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.0014960906701162457 | Validation Loss:  0.0015930227236822248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0014939476968720555 | Validation Loss:  0.0015925837215036154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.0014917318476364017 | Validation Loss:  0.0015923676546663046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0014896965585649014 | Validation Loss:  0.0015907792840152979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.0014874248299747705 | Validation Loss:  0.0015885328175500035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0014851032756268978 | Validation Loss:  0.0015871903160586953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0014828300336375833 | Validation Loss:  0.0015870870556682348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0014804351376369596 | Validation Loss:  0.0015874659875407815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.00147822848521173 | Validation Loss:  0.0015869718044996262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0014760962221771479 | Validation Loss:  0.0015856139361858368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0014740193728357553 | Validation Loss:  0.0015845716698095202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0014720996841788292 | Validation Loss:  0.001584298675879836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.001470118877477944 | Validation Loss:  0.0015843171859160066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0014681717148050666 | Validation Loss:  0.0015835855156183243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.001466214656829834 | Validation Loss:  0.001581915537826717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0014641991583630443 | Validation Loss:  0.0015802857233211398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.0014622585149481893 | Validation Loss:  0.0015793375205248594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0014603137969970703 | Validation Loss:  0.0015788478776812553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.001458442653529346 | Validation Loss:  0.001578001189045608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.0014566462486982346 | Validation Loss:  0.0015765518182888627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.001454863348044455 | Validation Loss:  0.0015752376057207584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.001453145407140255 | Validation Loss:  0.0015745986020192504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0014514130307361484 | Validation Loss:  0.0015743979020044208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0014496934600174427 | Validation Loss:  0.0015739178052172065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0014479930978268385 | Validation Loss:  0.0015729547012597322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0014462938997894526 | Validation Loss:  0.0015720970695838332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0014446506975218654 | Validation Loss:  0.001571753527969122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.0014430293813347816 | Validation Loss:  0.0015716671478003263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.0014414553297683597 | Validation Loss:  0.0015712392050772905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.0014399174833670259 | Validation Loss:  0.0015703681856393814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.0014383948873728514 | Validation Loss:  0.0015695474576205015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0014369033742696047 | Validation Loss:  0.0015690734144300222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0014354155864566565 | Validation Loss:  0.0015686964616179466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0014339550398290157 | Validation Loss:  0.0015679867938160896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.001432517310604453 | Validation Loss:  0.0015670100692659616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0014311079867184162 | Validation Loss:  0.0015662232181057334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.0014297354500740767 | Validation Loss:  0.0015657973708584905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.001428385847248137 | Validation Loss:  0.0015654347371309996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0014270690735429525 | Validation Loss:  0.0015648235566914082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.001425769180059433 | Validation Loss:  0.0015641303034499288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.001424492453224957 | Validation Loss:  0.0015637011965736747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0014232357498258352 | Validation Loss:  0.0015635339077562094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0014220001175999641 | Validation Loss:  0.0015633006114512682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0014207932399585843 | Validation Loss:  0.0015628418186679482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0014196096453815699 | Validation Loss:  0.001562391989864409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0014184555038809776 | Validation Loss:  0.0015621599741280079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0014173226663842797 | Validation Loss:  0.0015620101476088166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.0014162133447825909 | Validation Loss:  0.0015616798773407936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0014151245122775435 | Validation Loss:  0.001561195240356028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0014140565181151032 | Validation Loss:  0.0015608133981004357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0014130112249404192 | Validation Loss:  0.0015606088563799858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.0014119874686002731 | Validation Loss:  0.0015603868523612618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.001410988625138998 | Validation Loss:  0.0015600203769281507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0014100115513429046 | Validation Loss:  0.0015596643788740039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0014090575277805328 | Validation Loss:  0.0015594890573993325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0014081234112381935 | Validation Loss:  0.0015594124561175704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.0014072097837924957 | Validation Loss:  0.0015592474956065416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.0014063160633668303 | Validation Loss:  0.0015590132679790258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0014054430648684502 | Validation Loss:  0.0015588782262057066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0014045911375433207 | Validation Loss:  0.001558861113153398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.0014037598157301545 | Validation Loss:  0.0015588016249239445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0014029493322595954 | Validation Loss:  0.001558630377985537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.0014021579409018159 | Validation Loss:  0.0015584713546559215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0014013864565640688 | Validation Loss:  0.0015584130305796862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.0014006330166012049 | Validation Loss:  0.001558362622745335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0013998986687511206 | Validation Loss:  0.001558220130391419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0013991828309372067 | Validation Loss:  0.0015580578474327922\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0013984860852360725 | Validation Loss:  0.0015579877654090524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.0013978076167404652 | Validation Loss:  0.0015579721657559276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0013971474254503846 | Validation Loss:  0.001557904644869268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.0013965042307972908 | Validation Loss:  0.0015578039456158876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.0013958782656118274 | Validation Loss:  0.0015577706508338451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.0013952695298939943 | Validation Loss:  0.00155779963824898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.0013946773251518607 | Validation Loss:  0.0015577933518216014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.0013941016513854265 | Validation Loss:  0.0015577356098219752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.00139354239217937 | Validation Loss:  0.001557710231281817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0013929990818724036 | Validation Loss:  0.0015577345620840788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.001392471487633884 | Validation Loss:  0.0015577315352857113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.0013919590273872018 | Validation Loss:  0.0015576760051771998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0013914612354710698 | Validation Loss:  0.001557637588120997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0013909783447161317 | Validation Loss:  0.001557648298330605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.0013905097730457783 | Validation Loss:  0.0015576499281451106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.001390055287629366 | Validation Loss:  0.0015576138393953443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.0013896147720515728 | Validation Loss:  0.0015575940487906337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.0013891877606511116 | Validation Loss:  0.0015576222212985158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.0013887734385207295 | Validation Loss:  0.001557648298330605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.001388372271321714 | Validation Loss:  0.0015576414298266172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0013879833277314901 | Validation Loss:  0.0015576435253024101\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0013876068405807018 | Validation Loss:  0.001557679963298142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.001387242111377418 | Validation Loss:  0.001557708252221346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.0013868891401216388 | Validation Loss:  0.0015577031299471855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.001386547228321433 | Validation Loss:  0.0015577019657939672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.001386216375976801 | Validation Loss:  0.001557726296596229\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.0013858958845958114 | Validation Loss:  0.0015577421290799975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0013855858705937862 | Validation Loss:  0.0015577326994389296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0013852858683094382 | Validation Loss:  0.0015577320009469986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0013849957613274455 | Validation Loss:  0.0015577555168420076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0013847145019099116 | Validation Loss:  0.0015577716985717416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.0013844427885487676 | Validation Loss:  0.0015577694866806269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0013841798063367605 | Validation Loss:  0.0015577779849991202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.001383925206027925 | Validation Loss:  0.001557802432216704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0013836788712069392 | Validation Loss:  0.001557814422994852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.0013834405690431595 | Validation Loss:  0.0015578089514747262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0013832097174599767 | Validation Loss:  0.0015578115126118064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.001382986200042069 | Validation Loss:  0.0015578215243294835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.0013827701332047582 | Validation Loss:  0.0015578154707327485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0013825604692101479 | Validation Loss:  0.0015577974263578653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.0013823574408888817 | Validation Loss:  0.0015577891608700156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.001382160815410316 | Validation Loss:  0.0015577846206724644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0013819701271131635 | Validation Loss:  0.0015577655285596848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.001381785492412746 | Validation Loss:  0.0015577420126646757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0013816060964018106 | Validation Loss:  0.001557728392072022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.0013814320554956794 | Validation Loss:  0.001557712908834219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.0013812632532790303 | Validation Loss:  0.0015576835721731186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0013810992240905762 | Validation Loss:  0.0015576528385281563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0013809397350996733 | Validation Loss:  0.0015576271107420325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0013807847863063216 | Validation Loss:  0.0015575933502987027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.0013806341448798776 | Validation Loss:  0.0015575478319078684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0013804875779896975 | Validation Loss:  0.0015575033612549305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.0013803445035591722 | Validation Loss:  0.0015574607532471418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.001380205387249589 | Validation Loss:  0.0015574085991829634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.001380069530569017 | Validation Loss:  0.0015573500422760844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0013799371663480997 | Validation Loss:  0.001557295210659504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.0013798079453408718 | Validation Loss:  0.0015572389820590615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0013796815183013678 | Validation Loss:  0.0015571737894788384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0013795580016449094 | Validation Loss:  0.0015571062685921788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0013794373953714967 | Validation Loss:  0.001557040843181312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.0013793190009891987 | Validation Loss:  0.0015569694805890322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.0013792031677439809 | Validation Loss:  0.0015568905510008335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0013790895463898778 | Validation Loss:  0.001556810806505382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0013789781369268894 | Validation Loss:  0.0015567305963486433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.001378868706524372 | Validation Loss:  0.0015566428191959858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0013787612551823258 | Validation Loss:  0.0015565510839223862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.001378655550070107 | Validation Loss:  0.0015564605128020048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0013785515911877155 | Validation Loss:  0.0015563665656372905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0013784492621198297 | Validation Loss:  0.0015562669141218066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0013783484464511275 | Validation Loss:  0.0015561663312837481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0013782489113509655 | Validation Loss:  0.0015560656320303679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.001378151006065309 | Validation Loss:  0.0015559596940875053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0013780542649328709 | Validation Loss:  0.001555849565193057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.001377958687953651 | Validation Loss:  0.0015557389706373215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.0013778642751276493 | Validation Loss:  0.0015556259313598275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.0013777707936242223 | Validation Loss:  0.0015555074205622077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0013776787091046572 | Validation Loss:  0.0015553871635347605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.001377587323077023 | Validation Loss:  0.001555266440846026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0013774968683719635 | Validation Loss:  0.0015551416436210275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0013774072285741568 | Validation Loss:  0.0015550135867670178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0013773186365142465 | Validation Loss:  0.0015548855299130082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.0013772303937003016 | Validation Loss:  0.0015547556104138494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.0013771430822089314 | Validation Loss:  0.0015546217327937484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0013770564692094922 | Validation Loss:  0.0015544865746051073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.001376970438286662 | Validation Loss:  0.0015543503686785698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.001376885105855763 | Validation Loss:  0.0015542109031230211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.001376800355501473 | Validation Loss:  0.0015540687600150704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0013767158379778266 | Validation Loss:  0.0015539259184151888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.001376632135361433 | Validation Loss:  0.0015537814470008016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.001376548781991005 | Validation Loss:  0.0015536337159574032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0013764658942818642 | Validation Loss:  0.0015534852864220738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0013763835886493325 | Validation Loss:  0.001553335809148848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0013763015158474445 | Validation Loss:  0.0015531840035691857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0013762199087068439 | Validation Loss:  0.0015530308010056615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0013761390000581741 | Validation Loss:  0.001552877016365528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0013760579749941826 | Validation Loss:  0.0015527214854955673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0013759772991761565 | Validation Loss:  0.0015525633934885263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0013758972054347396 | Validation Loss:  0.0015524050686508417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0013758174609392881 | Validation Loss:  0.0015522458124905825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0013757379492744803 | Validation Loss:  0.0015520841116085649\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.0013756584376096725 | Validation Loss:  0.0015519213629886508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0013755795080214739 | Validation Loss:  0.0015517582651227713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.0013755008112639189 | Validation Loss:  0.0015515935374423862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.0013754224637523293 | Validation Loss:  0.0015514276456087828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0013753441162407398 | Validation Loss:  0.0015512611716985703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.0013752662343904376 | Validation Loss:  0.001551093882881105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.0013751883525401354 | Validation Loss:  0.0015509248478338122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.0013751107035204768 | Validation Loss:  0.0015507553471252322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.0013750334037467837 | Validation Loss:  0.0015505849150940776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.0013749564532190561 | Validation Loss:  0.0015504132024943829\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0013748793862760067 | Validation Loss:  0.0015502405585721135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.001374802552163601 | Validation Loss:  0.0015500674489885569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0013747259508818388 | Validation Loss:  0.001549893175251782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0013746495824307203 | Validation Loss:  0.0015497178537771106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0013745733303949237 | Validation Loss:  0.001549542066641152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0013744973111897707 | Validation Loss:  0.0015493659302592278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0013744214083999395 | Validation Loss:  0.0015491880476474762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0013743458548560739 | Validation Loss:  0.0015490101650357246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.0013742700684815645 | Validation Loss:  0.0015488314675167203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0013741945149376988 | Validation Loss:  0.0015486514894291759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0013741191942244768 | Validation Loss:  0.001548471162095666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0013740441063418984 | Validation Loss:  0.0015482902526855469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.001373969134874642 | Validation Loss:  0.0015481081791222095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0013738940469920635 | Validation Loss:  0.0015479254070669413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0013738193083554506 | Validation Loss:  0.001547741936519742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.001373744453303516 | Validation Loss:  0.0015475581167265773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.0013736699474975467 | Validation Loss:  0.001547373365610838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.0013735955581068993 | Validation Loss:  0.0015471880324184895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.00137352105230093 | Validation Loss:  0.00154700200073421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0013734467793256044 | Validation Loss:  0.0015468153869733214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.0013733727391809225 | Validation Loss:  0.0015466278418898582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.0013732985826209188 | Validation Loss:  0.001546439598314464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.0013732245424762368 | Validation Loss:  0.0015462510054931045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.0013731506187468767 | Validation Loss:  0.0015460614813491702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.0013730769278481603 | Validation Loss:  0.0015458711422979832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.001373003120534122 | Validation Loss:  0.0015456803375855088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.0013729294296354055 | Validation Loss:  0.0015454887179657817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.0013728559715673327 | Validation Loss:  0.0015452963998541236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.0013727822806686163 | Validation Loss:  0.0015451034996658564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.0013727088226005435 | Validation Loss:  0.0015449097845703363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.0013726354809477925 | Validation Loss:  0.0015447153709828854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.0013725622557103634 | Validation Loss:  0.0015445203753188252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.0013724887976422906 | Validation Loss:  0.0015443244483321905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.0013724155724048615 | Validation Loss:  0.001544127706438303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.0013723424635827541 | Validation Loss:  0.0015439303824678063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.0013722693547606468 | Validation Loss:  0.001543732243590057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.0013721961295232177 | Validation Loss:  0.0015435331733897328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.0013721231371164322 | Validation Loss:  0.0015433337539434433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.0013720501447096467 | Validation Loss:  0.0015431336360052228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.0013719770358875394 | Validation Loss:  0.0015429325867444277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.0013719041598960757 | Validation Loss:  0.0015427304897457361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.0013718311674892902 | Validation Loss:  0.001542528043501079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.0013717582914978266 | Validation Loss:  0.0015423244331032038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.0013716855319216847 | Validation Loss:  0.0015421200077980757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.0013716124230995774 | Validation Loss:  0.0015419151168316603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.0013715397799387574 | Validation Loss:  0.0015417090617120266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.0013714669039472938 | Validation Loss:  0.0015415021916851401\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.0013713939115405083 | Validation Loss:  0.0015412947395816445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.0013713211519643664 | Validation Loss:  0.0015410861233249307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.0013712482759729028 | Validation Loss:  0.0015408769249916077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.0013711753999814391 | Validation Loss:  0.0015406666789203882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.0013711025239899755 | Validation Loss:  0.0015404557343572378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.0013710297644138336 | Validation Loss:  0.001540243742056191\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.0013709570048376918 | Validation Loss:  0.0015400309348478913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.0013708840124309063 | Validation Loss:  0.001539817312732339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.0013708111364394426 | Validation Loss:  0.0015396027592942119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.001370738260447979 | Validation Loss:  0.0015393870417028666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.0013706652680411935 | Validation Loss:  0.0015391707420349121\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.0013705923920497298 | Validation Loss:  0.0015389531617984176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.0013705192832276225 | Validation Loss:  0.0015387347666546702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.0013704465236514807 | Validation Loss:  0.0015385157894343138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.0013703732984140515 | Validation Loss:  0.0015382954152300954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.001370300306007266 | Validation Loss:  0.0015380741097033024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.0013702271971851587 | Validation Loss:  0.0015378518728539348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.0013701540883630514 | Validation Loss:  0.0015376287046819925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.0013700808631256223 | Validation Loss:  0.0015374046051874757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.0013700076378881931 | Validation Loss:  0.0015371793415397406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.0013699342962354422 | Validation Loss:  0.0015369531465694308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.0013698609545826912 | Validation Loss:  0.0015367259038612247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.0013697876129299402 | Validation Loss:  0.0015364978462457657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.0013697140384465456 | Validation Loss:  0.001536268275231123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.0013696405803784728 | Validation Loss:  0.0015360377728939056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.0013695670058950782 | Validation Loss:  0.0015358069213107228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.0013694933149963617 | Validation Loss:  0.0015355739742517471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.0013694197405129671 | Validation Loss:  0.0015353407943621278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.0013693460496142507 | Validation Loss:  0.0015351064503192902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.0013692720094695687 | Validation Loss:  0.001534870476461947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.0013691980857402086 | Validation Loss:  0.0015346338041126728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.0013691239291802049 | Validation Loss:  0.0015343959676101804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.0013690500054508448 | Validation Loss:  0.0015341569669544697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.0013689756160601974 | Validation Loss:  0.0015339169185608625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.0013689014595001936 | Validation Loss:  0.001533675822429359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.001368827186524868 | Validation Loss:  0.001533433562144637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.0013687526807188988 | Validation Loss:  0.0015331897884607315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.0013686781749129295 | Validation Loss:  0.0015329455491155386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.0013686034362763166 | Validation Loss:  0.0015326999127864838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.0013685288140550256 | Validation Loss:  0.0015324529958888888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.0013684539590030909 | Validation Loss:  0.0015322050312533975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.0013683788711205125 | Validation Loss:  0.0015319560188800097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.0013683036668226123 | Validation Loss:  0.0015317053766921163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.0013682286953553557 | Validation Loss:  0.001531454036012292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.0013681532582268119 | Validation Loss:  0.0015312014147639275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.001368077821098268 | Validation Loss:  0.001530947512947023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.0013680021511390805 | Validation Loss:  0.0015306924469769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.001367926481179893 | Validation Loss:  0.001530436216853559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.0013678506948053837 | Validation Loss:  0.0015301788225769997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.0013677749084308743 | Validation Loss:  0.0015299203805625439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.0013676987728103995 | Validation Loss:  0.0015296603087335825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.001367622404359281 | Validation Loss:  0.0015293993055820465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.0013675460359081626 | Validation Loss:  0.001529136672616005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.0013674695510417223 | Validation Loss:  0.0015288733411580324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.001367393066175282 | Validation Loss:  0.0015286087291315198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.001367316348478198 | Validation Loss:  0.0015283424872905016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.0013672393979504704 | Validation Loss:  0.0015280753141269088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.0013671622145920992 | Validation Loss:  0.0015278068603947759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.001367084914818406 | Validation Loss:  0.0015275368932634592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.0013670074986293912 | Validation Loss:  0.0015272661112248898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.0013669299660250545 | Validation Loss:  0.0015269936993718147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.001366852200590074 | Validation Loss:  0.0015267202397808433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.0013667743187397718 | Validation Loss:  0.0015264457324519753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.0013666963204741478 | Validation Loss:  0.0015261693624779582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.0013666179729625583 | Validation Loss:  0.0015258921775966883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.001366539509035647 | Validation Loss:  0.0015256137121468782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.0013664610451087356 | Validation Loss:  0.0015253335004672408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.0013663822319358587 | Validation Loss:  0.0015250524738803506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.0013663031859323382 | Validation Loss:  0.0015247699338942766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.001366224023513496 | Validation Loss:  0.0015244862297549844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.0013661445118486881 | Validation Loss:  0.0015242013614624739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.0013660650001838803 | Validation Loss:  0.0015239150961861014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.0013659852556884289 | Validation Loss:  0.001523627550341189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.0013659052783623338 | Validation Loss:  0.0015233387239277363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.001365825068205595 | Validation Loss:  0.0015230485005304217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.0013657446252182126 | Validation Loss:  0.001522756996564567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.001365664298646152 | Validation Loss:  0.0015224640956148505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.0013655833899974823 | Validation Loss:  0.0015221700305119157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.0013655024813488126 | Validation Loss:  0.0015218749176710844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.0013654211070388556 | Validation Loss:  0.0015215782914310694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.0013653398491442204 | Validation Loss:  0.0015212800353765488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.001365258009172976 | Validation Loss:  0.001520981197245419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.0013651760527864099 | Validation Loss:  0.0015206807292997837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.0013650938635692 | Validation Loss:  0.0015203787479549646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.0013650115579366684 | Validation Loss:  0.0015200759517028928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.0013649287866428494 | Validation Loss:  0.0015197711763903499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.0013648460153490305 | Validation Loss:  0.0015194658190011978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.0013647630112245679 | Validation Loss:  0.0015191588317975402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.0013646796578541398 | Validation Loss:  0.0015188503311946988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.001364596071653068 | Validation Loss:  0.0015185411320999265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.0013645123690366745 | Validation Loss:  0.0015182304196059704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.0013644280843436718 | Validation Loss:  0.001517918542958796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.001364343799650669 | Validation Loss:  0.001517605152912438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.001364259049296379 | Validation Loss:  0.0015172907151281834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.001364174415357411 | Validation Loss:  0.0015169751131907105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.0013640891993418336 | Validation Loss:  0.0015166578814387321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.0013640038669109344 | Validation Loss:  0.0015163399511948228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.001363918068818748 | Validation Loss:  0.0015160203911364079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.0013638322707265615 | Validation Loss:  0.0015156997833400965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.0013637460069730878 | Validation Loss:  0.001515377894975245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.0013636595103889704 | Validation Loss:  0.001515054958872497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.0013635726645588875 | Validation Loss:  0.0015147309750318527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.001363485585898161 | Validation Loss:  0.001514405244961381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.0013633982744067907 | Validation Loss:  0.0015140786999836564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.0013633107300847769 | Validation Loss:  0.0015137511072680354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.0013632228365167975 | Validation Loss:  0.0015134223503991961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.0013631345937028527 | Validation Loss:  0.0015130919637158513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.001363046234473586 | Validation Loss:  0.0015127611113712192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.0013629574095830321 | Validation Loss:  0.001512428862042725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.0013628683518618345 | Validation Loss:  0.0015120954485610127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.0013627789448946714 | Validation Loss:  0.0015117612201720476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.0013626893050968647 | Validation Loss:  0.0015114257112145424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.0013625993160530925 | Validation Loss:  0.0015110892709344625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.0013625090941786766 | Validation Loss:  0.0015107516665011644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.0013624184066429734 | Validation Loss:  0.0015104132471606135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.0013623276026919484 | Validation Loss:  0.0015100736636668444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.001362236333079636 | Validation Loss:  0.0015097330324351788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.001362144947052002 | Validation Loss:  0.0015093915862962604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.0013620530953630805 | Validation Loss:  0.0015090493252500892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.0013619610108435154 | Validation Loss:  0.0015087059000506997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.001361868460662663 | Validation Loss:  0.0015083615435287356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.0013617759104818106 | Validation Loss:  0.0015080167213454843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.0013616826618090272 | Validation Loss:  0.0015076702693477273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.0013615894131362438 | Validation Loss:  0.001507324050180614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.0013614956988021731 | Validation Loss:  0.0015069758519530296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.0013614017516374588 | Validation Loss:  0.001506627770140767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.001361307455226779 | Validation Loss:  0.0015062784077599645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.0013612126931548119 | Validation Loss:  0.0015059286961331964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.0013611179310828447 | Validation Loss:  0.0015055779367685318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.0013610224705189466 | Validation Loss:  0.0015052263624966145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.0013609270099550486 | Validation Loss:  0.0015048745553940535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.0013608310837298632 | Validation Loss:  0.0015045215841382742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.001360735041089356 | Validation Loss:  0.0015041687292978168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.001360638183541596 | Validation Loss:  0.00150381401181221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.001360541325993836 | Validation Loss:  0.0015034603420644999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.0013604442356154323 | Validation Loss:  0.001503104460425675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.0013603467959910631 | Validation Loss:  0.00150274985935539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.0013602490071207285 | Validation Loss:  0.0015023928135633469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.0013601509854197502 | Validation Loss:  0.0015020371647551656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.0013600523816421628 | Validation Loss:  0.001501679071225226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.0013599538942798972 | Validation Loss:  0.0015013228403404355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.0013598547084257007 | Validation Loss:  0.0015009641647338867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.0013597554061561823 | Validation Loss:  0.0015006066532805562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.0013596557546406984 | Validation Loss:  0.00150024751201272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.001359555753879249 | Validation Loss:  0.0014998895348981023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.0013594554038718343 | Validation Loss:  0.0014995296951383352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.0013593550538644195 | Validation Loss:  0.0014991710195317864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.0013592542381957173 | Validation Loss:  0.0014988110633566976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.001359152956865728 | Validation Loss:  0.0014984518056735396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.0013590517919510603 | Validation Loss:  0.0014980912674218416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.00135894981212914 | Validation Loss:  0.0014977318933233619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.0013588478323072195 | Validation Loss:  0.0014973710058256984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.0013587455032393336 | Validation Loss:  0.0014970117481425405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.001358643057756126 | Validation Loss:  0.0014966502785682678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.001358540030196309 | Validation Loss:  0.0014962912537157536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.0013584367698058486 | Validation Loss:  0.0014959292020648718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.0013583333930000663 | Validation Loss:  0.0014955711085349321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.0013582296669483185 | Validation Loss:  0.0014952074270695448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.0013581259408965707 | Validation Loss:  0.0014948522439226508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.00135802139993757 | Validation Loss:  0.0014944844879209995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.0013579169753938913 | Validation Loss:  0.0014941355912014842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.001357812201604247 | Validation Loss:  0.0014937585219740868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.0013577070785686374 | Validation Loss:  0.001493425341323018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.0013576018391177058 | Validation Loss:  0.0014930240577086806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.0013574964832514524 | Validation Loss:  0.0014927303418517113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.0013573906617239118 | Validation Loss:  0.0014922668924555182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.001357284840196371 | Validation Loss:  0.001492076669819653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.0013571793679147959 | Validation Loss:  0.00149144500028342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.0013570745941251516 | Validation Loss:  0.001491542556323111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.0013569724978879094 | Validation Loss:  0.001490444177761674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.001356878667138517 | Validation Loss:  0.0014913834165781736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.0013568089343607426 | Validation Loss:  0.0014889804879203439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.0013568128924816847 | Validation Loss:  0.0014925923896953464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.001357034663669765 | Validation Loss:  0.001486794906668365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.0013579409569501877 | Validation Loss:  0.0015001223655417562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.0013608195586130023 | Validation Loss:  0.001489022746682167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.0013699564151465893 | Validation Loss:  0.0015404507284983993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.001391964964568615 | Validation Loss:  0.0015382720157504082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.00144532963167876 | Validation Loss:  0.0016377769643440843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.0014795769238844514 | Validation Loss:  0.0015789446188136935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.0015005945460870862 | Validation Loss:  0.001552421716041863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.001401913003064692 | Validation Loss:  0.0014890601160004735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.0013568721478804946 | Validation Loss:  0.001504808897152543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.0013990558218210936 | Validation Loss:  0.0015800729161128402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.0014271273976191878 | Validation Loss:  0.0015094648115336895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.0014031636528670788 | Validation Loss:  0.0014929919270798564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.0013574787881225348 | Validation Loss:  0.0015216887695714831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.0013767058262601495 | Validation Loss:  0.0015125871868804097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.001409569988027215 | Validation Loss:  0.001522946055047214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.0013773533282801509 | Validation Loss:  0.0014897220535203815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.001356137334369123 | Validation Loss:  0.0014931996120139956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.0013780571753159165 | Validation Loss:  0.0015295922057703137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.0013825498754158616 | Validation Loss:  0.0014865989796817303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.0013621952384710312 | Validation Loss:  0.0014859159709885716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.001357742235995829 | Validation Loss:  0.0015176264569163322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.001373077742755413 | Validation Loss:  0.0014903569826856256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.0013717805268242955 | Validation Loss:  0.001491666305810213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.0013562625972554088 | Validation Loss:  0.001501254504546523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.0013615434290841222 | Validation Loss:  0.0014888623263686895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.0013707674806937575 | Validation Loss:  0.0014991055941209197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.0013603753177449107 | Validation Loss:  0.001489367219619453\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.0013554964680224657 | Validation Loss:  0.001485223532654345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.0013635697541758418 | Validation Loss:  0.00150207313708961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.0013627568259835243 | Validation Loss:  0.001483535161241889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.0013554986799135804 | Validation Loss:  0.0014824426034465432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.0013568894937634468 | Validation Loss:  0.0014990029158070683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.0013611066387966275 | Validation Loss:  0.0014817432966083288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.0013578522484749556 | Validation Loss:  0.001483912579715252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.0013544969260692596 | Validation Loss:  0.0014930054312571883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.0013576875207945704 | Validation Loss:  0.0014815598260611296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.0013588351430371404 | Validation Loss:  0.001487447996623814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.0013550660805776715 | Validation Loss:  0.001486682565882802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.0013547873822972178 | Validation Loss:  0.0014806335093453526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.0013573061442002654 | Validation Loss:  0.001489618793129921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.001356125925667584 | Validation Loss:  0.001482137362472713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.0013539626961573958 | Validation Loss:  0.0014801985817030072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.0013549417490139604 | Validation Loss:  0.001488942070864141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.0013559621293097734 | Validation Loss:  0.0014798485208302736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.0013545615365728736 | Validation Loss:  0.0014811811270192266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.0013536157784983516 | Validation Loss:  0.0014855926856398582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.001354628591798246 | Validation Loss:  0.0014784646918997169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.0013549053110182285 | Validation Loss:  0.0014826083788648248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.0013536815531551838 | Validation Loss:  0.0014813104644417763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.001353392843157053 | Validation Loss:  0.0014775536255910993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.001354139531031251 | Validation Loss:  0.001483102678321302\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.0013539692154154181 | Validation Loss:  0.0014781436184421182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.0013531410368159413 | Validation Loss:  0.001477697747759521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.0013531093718484044 | Validation Loss:  0.0014816540060564876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.0013535545440390706 | Validation Loss:  0.0014763568760827184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.0013533380115404725 | Validation Loss:  0.0014785861130803823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.001352765946649015 | Validation Loss:  0.0014788031112402678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.0013527729315683246 | Validation Loss:  0.0014755490701645613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.0013530419673770666 | Validation Loss:  0.0014791677240282297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.0013528337003663182 | Validation Loss:  0.0014762335922569036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.0013524373061954975 | Validation Loss:  0.001475667697377503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.0013524169335141778 | Validation Loss:  0.0014782216167077422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.0013525673421099782 | Validation Loss:  0.001474596792832017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.0013524320675060153 | Validation Loss:  0.0014762115897610784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.001352135674096644 | Validation Loss:  0.0014759417390450835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.0013520654756575823 | Validation Loss:  0.001473799697123468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.0013521466171368957 | Validation Loss:  0.001476195058785379\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.0013520632637664676 | Validation Loss:  0.0014737695455551147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.001351843704469502 | Validation Loss:  0.001473793527111411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.0013517328770831227 | Validation Loss:  0.0014748581452295184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.0013517545303329825 | Validation Loss:  0.001472424017265439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.0013517159968614578 | Validation Loss:  0.0014739419566467404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.0013515576720237732 | Validation Loss:  0.0014728112146258354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.0013514243764802814 | Validation Loss:  0.0014719543978571892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.001351392362266779 | Validation Loss:  0.0014733002753928304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.0013513682642951608 | Validation Loss:  0.001471277792006731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.0013512670993804932 | Validation Loss:  0.0014720638282597065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.0013511361321434379 | Validation Loss:  0.0014717509038746357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.0013510589487850666 | Validation Loss:  0.0014705947833135724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.0013510242570191622 | Validation Loss:  0.0014717442682012916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.0013509619748219848 | Validation Loss:  0.0014702016487717628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.0013508565025404096 | Validation Loss:  0.0014705039793625474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.0013507558032870293 | Validation Loss:  0.0014705011853948236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.001350692706182599 | Validation Loss:  0.0014693571720272303\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.0013506421819329262 | Validation Loss:  0.001470225746743381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.0013505675597116351 | Validation Loss:  0.0014690273674204946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.0013504717499017715 | Validation Loss:  0.0014690919779241085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.0013503852533176541 | Validation Loss:  0.0014691009419038892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.0013503206428140402 | Validation Loss:  0.0014680895255878568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.0013502603396773338 | Validation Loss:  0.001468715607188642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.0013501852517947555 | Validation Loss:  0.0014677195576950908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.0013500984059646726 | Validation Loss:  0.0014677473809570074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.00135001668240875 | Validation Loss:  0.001467663561925292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.0013499476481229067 | Validation Loss:  0.0014668580843135715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.0013498823391273618 | Validation Loss:  0.0014672994147986174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.0013498096959665418 | Validation Loss:  0.0014664159389212728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.0013497292529791594 | Validation Loss:  0.0014664961490780115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.001349649392068386 | Validation Loss:  0.0014662526082247496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.0013495759340003133 | Validation Loss:  0.0014656782150268555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.0013495071325451136 | Validation Loss:  0.0014659418957307935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.0013494367012754083 | Validation Loss:  0.0014651494566351175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.0013493611477315426 | Validation Loss:  0.0014652909012511373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.0013492833822965622 | Validation Loss:  0.0014648567885160446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.0013492072466760874 | Validation Loss:  0.0014645160408690572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.001349134836345911 | Validation Loss:  0.0014645493356510997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.0013490640558302402 | Validation Loss:  0.0014638875145465136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.001348991529084742 | Validation Loss:  0.0014640326844528317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.0013489167904481292 | Validation Loss:  0.0014634612016379833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.0013488411204889417 | Validation Loss:  0.0014633475802838802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.0013487657997757196 | Validation Loss:  0.0014631162630394101\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.0013486925745382905 | Validation Loss:  0.001462672371417284\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.001348620280623436 | Validation Loss:  0.0014626991469413042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.0013485477538779378 | Validation Loss:  0.0014621266163885593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.0013484737137332559 | Validation Loss:  0.0014621493173763156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.0013483992079272866 | Validation Loss:  0.0014616999542340636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.001348324352875352 | Validation Loss:  0.001461521373130381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.0013482500799000263 | Validation Loss:  0.0014613043749704957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.0013481763890013099 | Validation Loss:  0.0014609097270295024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.0013481032801792026 | Validation Loss:  0.0014608585042878985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.0013480298221111298 | Validation Loss:  0.0014603704912588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.0013479561312124133 | Validation Loss:  0.0014603344025090337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.0013478818582370877 | Validation Loss:  0.0014598984271287918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.0013478072360157967 | Validation Loss:  0.0014597553526982665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.0013477328466251493 | Validation Loss:  0.001459453604184091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.0013476584572345018 | Validation Loss:  0.0014591667568311095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.001347584300674498 | Validation Loss:  0.0014589963247999549\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.0013475106097757816 | Validation Loss:  0.001458604820072651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.0013474366860464215 | Validation Loss:  0.0014585069147869945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.0013473622966557741 | Validation Loss:  0.0014580803690478206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.0013472880236804485 | Validation Loss:  0.0014579835115000606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.0013472131686285138 | Validation Loss:  0.0014575845561921597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.0013471385464072227 | Validation Loss:  0.0014574386877939105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.001347063691355288 | Validation Loss:  0.0014571023639291525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.0013469889527186751 | Validation Loss:  0.001456888159736991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.0013469140976667404 | Validation Loss:  0.0014566219178959727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.0013468391261994839 | Validation Loss:  0.0014563420554623008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.0013467642711475492 | Validation Loss:  0.0014561342541128397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.0013466894160956144 | Validation Loss:  0.0014558034017682076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.0013466140953823924 | Validation Loss:  0.0014556365786120296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.0013465391239151359 | Validation Loss:  0.0014552704524248838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.0013464638032019138 | Validation Loss:  0.001455132500268519\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.0013463882496580482 | Validation Loss:  0.0014547420432791114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.001346313045360148 | Validation Loss:  0.0014546267921105027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.0013462374918162823 | Validation Loss:  0.0014542130520567298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.0013461619382724166 | Validation Loss:  0.0014541231794282794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.001346086384728551 | Validation Loss:  0.001453678822144866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.001346010249108076 | Validation Loss:  0.001453627715818584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.0013459345791488886 | Validation Loss:  0.0014531335327774286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.0013458587927743793 | Validation Loss:  0.0014531492488458753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.0013457832392305136 | Validation Loss:  0.0014525686856359243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.001345707685686648 | Validation Loss:  0.0014527002349495888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.0013456325978040695 | Validation Loss:  0.0014519674004986882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.0013455586740747094 | Validation Loss:  0.0014523058198392391\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.001345486263744533 | Validation Loss:  0.0014513019705191255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.001345416996628046 | Validation Loss:  0.0014520210679620504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.0013453535502776504 | Validation Loss:  0.0014505241997539997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.001345300697721541 | Validation Loss:  0.0014519690303131938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.0013452675193548203 | Validation Loss:  0.0014495578361675143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.0013452740386128426 | Validation Loss:  0.0014524575090035796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.0013453536666929722 | Validation Loss:  0.0014483463019132614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.0013455902226269245 | Validation Loss:  0.0014543572906404734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.001346114557236433 | Validation Loss:  0.0014472275506705046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.0013473047874867916 | Validation Loss:  0.0014604750322178006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.0013496589381247759 | Validation Loss:  0.0014490177854895592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.0013549996074289083 | Validation Loss:  0.0014805194223299623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.00136468093842268 | Validation Loss:  0.0014680060558021069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.001387212541885674 | Validation Loss:  0.001541225821711123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.0014177928678691387 | Validation Loss:  0.0015390767948701978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.0014837691560387611 | Validation Loss:  0.0016298406990244985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.001499435631558299 | Validation Loss:  0.0015642076032236218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.0015181186608970165 | Validation Loss:  0.0015387297607958317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.0014134193770587444 | Validation Loss:  0.0014459190424531698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.001348754856735468 | Validation Loss:  0.0014529719483107328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.0013624376151710749 | Validation Loss:  0.0015360089018940926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.0014133183285593987 | Validation Loss:  0.0015100131276994944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.0014399323845282197 | Validation Loss:  0.001500380109064281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.001379318069666624 | Validation Loss:  0.0014506190782412887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.0013452032580971718 | Validation Loss:  0.0014585519675165415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.0013703828444704413 | Validation Loss:  0.0015138749731704593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.001390146091580391 | Validation Loss:  0.001462549902498722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.00137308647390455 | Validation Loss:  0.00145518418867141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.0013454111758619547 | Validation Loss:  0.0014735247241333127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.0013573329197242856 | Validation Loss:  0.0014658934669569135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.0013782952446490526 | Validation Loss:  0.0014795537572354078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.0013614839408546686 | Validation Loss:  0.0014500570250675082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.0013447131495922804 | Validation Loss:  0.0014503493439406157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.0013544639805331826 | Validation Loss:  0.0014810168650001287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.0013631241163238883 | Validation Loss:  0.001450286596082151\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.00135346211027354 | Validation Loss:  0.0014502528356388211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.0013441488845273852 | Validation Loss:  0.0014657003339380026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.001352015882730484 | Validation Loss:  0.0014515663497149944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.0013577681966125965 | Validation Loss:  0.0014600591966882348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.0013482430949807167 | Validation Loss:  0.0014520680997520685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.001344336080364883 | Validation Loss:  0.0014481410617008805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.0013510022545233369 | Validation Loss:  0.0014646400231868029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.0013513915473595262 | Validation Loss:  0.0014471422182396054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.001345219905488193 | Validation Loss:  0.0014472365146502852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.0013443109346553683 | Validation Loss:  0.0014600789872929454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.0013486130628734827 | Validation Loss:  0.00144632114097476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.0013483363436535 | Validation Loss:  0.001450863666832447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.0013439301401376724 | Validation Loss:  0.001451571355573833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.0013442523777484894 | Validation Loss:  0.001445080037228763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.0013471875572577119 | Validation Loss:  0.001454331330023706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.0013458732282742858 | Validation Loss:  0.0014455097261816263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.001343183103017509 | Validation Loss:  0.0014442381216213107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.0013438157038763165 | Validation Loss:  0.00145301956217736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.0013454831205308437 | Validation Loss:  0.0014431437011808157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.001344553311355412 | Validation Loss:  0.0014460523379966617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.001342782867141068 | Validation Loss:  0.0014479816891252995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.0013433111598715186 | Validation Loss:  0.001442384091205895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.0013444327050819993 | Validation Loss:  0.001448517432436347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.0013436353765428066 | Validation Loss:  0.0014434719923883677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.0013424524804577231 | Validation Loss:  0.0014422567328438163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.0013427505036816 | Validation Loss:  0.0014477617805823684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.0013434559805318713 | Validation Loss:  0.0014412306481972337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.0013430318795144558 | Validation Loss:  0.001443620421923697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.0013421764597296715 | Validation Loss:  0.0014440694358199835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.0013422613264992833 | Validation Loss:  0.0014403397217392921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.0013427670346572995 | Validation Loss:  0.0014447113499045372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.0013425449142232537 | Validation Loss:  0.0014406400732696056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.0013419304741546512 | Validation Loss:  0.0014404233079403639\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.0013418300077319145 | Validation Loss:  0.0014431806048378348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.0013421413023024797 | Validation Loss:  0.0014389222487807274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.001342119532637298 | Validation Loss:  0.0014414854813367128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.0013416993897408247 | Validation Loss:  0.001440174994058907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.001341486582532525 | Validation Loss:  0.0014385293470695615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.001341629889793694 | Validation Loss:  0.0014414028264582157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.0013416943838819861 | Validation Loss:  0.001437996281310916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.001341466442681849 | Validation Loss:  0.0014391100266948342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.0013412139378488064 | Validation Loss:  0.0014393030432984233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.0013411965919658542 | Validation Loss:  0.001437118393369019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.0013412722619250417 | Validation Loss:  0.0014393962919712067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.0013411897234618664 | Validation Loss:  0.0014370298013091087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.0013409826206043363 | Validation Loss:  0.001437247497960925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.0013408546801656485 | Validation Loss:  0.00143795448821038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.0013408620143309236 | Validation Loss:  0.0014358318876475096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.0013408593367785215 | Validation Loss:  0.001437500468455255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.0013407421065494418 | Validation Loss:  0.0014358868356794119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.0013405880890786648 | Validation Loss:  0.0014357242034748197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.0013405093923211098 | Validation Loss:  0.0014364819508045912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.0013404948404058814 | Validation Loss:  0.0014346748357638717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.0013404511846601963 | Validation Loss:  0.0014359012711793184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.0013403425691649318 | Validation Loss:  0.0014346964890137315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.0013402245240285993 | Validation Loss:  0.001434453995898366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.001340151997283101 | Validation Loss:  0.0014350193087011576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.0013401132309809327 | Validation Loss:  0.0014335400192067027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.0013400580501183867 | Validation Loss:  0.0014344649389386177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.0013399652671068907 | Validation Loss:  0.0014334142906591296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.0013398632872849703 | Validation Loss:  0.0014332561986520886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.0013397855218499899 | Validation Loss:  0.0014335334999486804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.001339730923064053 | Validation Loss:  0.0014323717914521694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.0013396715512499213 | Validation Loss:  0.0014330933336168528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.0013395908754318953 | Validation Loss:  0.0014320965856313705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.0013394994894042611 | Validation Loss:  0.0014321209164336324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.0013394163688644767 | Validation Loss:  0.0014320902992039919\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.0013393479166552424 | Validation Loss:  0.0014312660787254572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.001339284353889525 | Validation Loss:  0.0014317879686132073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.0013392120599746704 | Validation Loss:  0.0014308360405266285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.0013391290558502078 | Validation Loss:  0.0014310356928035617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.0013390440726652741 | Validation Loss:  0.0014306841185316443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.0013389659579843283 | Validation Loss:  0.001430205418728292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.0013388945953920484 | Validation Loss:  0.0014304451178759336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.001338823582045734 | Validation Loss:  0.0014296133304014802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.0013387480285018682 | Validation Loss:  0.001429891330190003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.001338666770607233 | Validation Loss:  0.0014292810810729861\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.0013385842321440578 | Validation Loss:  0.001429145340807736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.00133850472047925 | Validation Loss:  0.0014290270628407598\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.0013384288176894188 | Validation Loss:  0.0014284630306065083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.0013383534969761968 | Validation Loss:  0.001428640796802938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.0013382765464484692 | Validation Loss:  0.0014279637252911925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.0013381964527070522 | Validation Loss:  0.0014280626783147454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.001338114612735808 | Validation Loss:  0.0014276060974225402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.0013380328891798854 | Validation Loss:  0.0014273974811658263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.001337952446192503 | Validation Loss:  0.001427265116944909\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.0013378732837736607 | Validation Loss:  0.0014267769875004888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.0013377940049394965 | Validation Loss:  0.0014268327504396439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.0013377139111980796 | Validation Loss:  0.001426257542334497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.001337632187642157 | Validation Loss:  0.0014262832701206207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.0013375492999330163 | Validation Loss:  0.0014258199371397495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.00133746606297791 | Validation Loss:  0.001425669644959271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.0013373828260228038 | Validation Loss:  0.0014254074776545167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.001337299938313663 | Validation Loss:  0.001425060909241438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.0013372169341892004 | Validation Loss:  0.001424965332262218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.0013371342793107033 | Validation Loss:  0.001424496411345899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.0013370505766943097 | Validation Loss:  0.0014244669582694769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.0013369661755859852 | Validation Loss:  0.001423979178071022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.0013368810759857297 | Validation Loss:  0.0014239164302125573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.001336795394308865 | Validation Loss:  0.0014234917471185327\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.0013367092469707131 | Validation Loss:  0.0014233373804017901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.001336622517555952 | Validation Loss:  0.001423011184670031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.0013365356717258692 | Validation Loss:  0.0014227512292563915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.0013364485930651426 | Validation Loss:  0.0014225196791812778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.0013363611651584506 | Validation Loss:  0.0014221712481230497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.001336273388005793 | Validation Loss:  0.0014220101293176413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.00133618526160717 | Validation Loss:  0.0014216020936146379\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.0013360966695472598 | Validation Loss:  0.0014214836992323399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.0013360074954107404 | Validation Loss:  0.0014210400404408574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.0013359178556129336 | Validation Loss:  0.001420944114215672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.0013358276337385178 | Validation Loss:  0.0014204781036823988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.001335736713372171 | Validation Loss:  0.0014203967293724418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.0013356456765905023 | Validation Loss:  0.0014199098804965615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.0013355540577322245 | Validation Loss:  0.0014198481803759933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.0013354619732126594 | Validation Loss:  0.0014193286187946796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.0013353695394471288 | Validation Loss:  0.0014193046372383833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.0013352766400203109 | Validation Loss:  0.001418726285919547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.0013351832749322057 | Validation Loss:  0.001418776111677289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.001335090259090066 | Validation Loss:  0.0014180932193994522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.0013349970104172826 | Validation Loss:  0.0014182808808982372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.001334904576651752 | Validation Loss:  0.00141741125844419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.0013348134234547615 | Validation Loss:  0.0014178542187437415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.0013347252970561385 | Validation Loss:  0.001416647108271718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.0013346427585929632 | Validation Loss:  0.0014175721444189548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.001334571628831327 | Validation Loss:  0.0014157473342493176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.0013345213374122977 | Validation Loss:  0.0014176223194226623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.0013345115585252643 | Validation Loss:  0.001414654077962041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.0013345847837626934 | Validation Loss:  0.0014185202307999134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.001334815053269267 | Validation Loss:  0.0014134821249172091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.0013353925896808505 | Validation Loss:  0.0014218941796571016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.0013366148341447115 | Validation Loss:  0.001413604593835771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.0013394063571467996 | Validation Loss:  0.0014336182503029704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.0013448874233290553 | Validation Loss:  0.0014233384281396866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.0013577943900600076 | Validation Loss:  0.001475171884521842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.0013801493914797902 | Validation Loss:  0.0014791556168347597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.001433912431821227 | Validation Loss:  0.0015923652099445462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.0014894044725224376 | Validation Loss:  0.0016171014867722988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.0016047419048845768 | Validation Loss:  0.00165340187959373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.0015446781180799007 | Validation Loss:  0.0015075596747919917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.0014714290155097842 | Validation Loss:  0.001439318060874939\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.0013476149179041386 | Validation Loss:  0.0014503875281661749\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.0013573942705988884 | Validation Loss:  0.0015018449630588293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.0014541256241500378 | Validation Loss:  0.0015471101505681872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.0014466461725533009 | Validation Loss:  0.0014426936395466328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.0013822552282363176 | Validation Loss:  0.0014180467696860433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.0013357377611100674 | Validation Loss:  0.0014825885882601142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.001382049871608615 | Validation Loss:  0.0014719911850988865\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.0014195132534950972 | Validation Loss:  0.0014533273642882705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.001358571695163846 | Validation Loss:  0.0014250465901568532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.0013369194930419326 | Validation Loss:  0.0014409375144168735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.0013758837012574077 | Validation Loss:  0.0014728836249560118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.0013735817046836019 | Validation Loss:  0.0014184419997036457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.0013407449005171657 | Validation Loss:  0.0014186514308676124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.0013410454848781228 | Validation Loss:  0.0014605099568143487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.0013628146843984723 | Validation Loss:  0.0014269461389631033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.0013549614232033491 | Validation Loss:  0.0014206174528226256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.0013342248275876045 | Validation Loss:  0.001439163344912231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.0013461015187203884 | Validation Loss:  0.0014274179702624679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.0013574091717600822 | Validation Loss:  0.0014305138029158115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.0013395198620855808 | Validation Loss:  0.0014243547338992357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.0013358744326978922 | Validation Loss:  0.0014212699607014656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.0013488286640495062 | Validation Loss:  0.0014348754193633795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.0013433252461254597 | Validation Loss:  0.0014162135776132345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.0013334278482943773 | Validation Loss:  0.0014158339472487569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.0013387914514169097 | Validation Loss:  0.0014339538756757975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.001342867617495358 | Validation Loss:  0.0014143009902909398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.0013360389275476336 | Validation Loss:  0.001414370839484036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.001333492691628635 | Validation Loss:  0.0014278996968641877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.00133896479383111 | Validation Loss:  0.0014139482518658042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.001338354079052806 | Validation Loss:  0.0014162787701934576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.0013329146895557642 | Validation Loss:  0.001419673440977931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.0013345639454200864 | Validation Loss:  0.0014123698929324746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.0013377121649682522 | Validation Loss:  0.0014190521324053407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.0013344066683202982 | Validation Loss:  0.001413522637449205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.001332268351688981 | Validation Loss:  0.0014103059656918049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.0013348028296604753 | Validation Loss:  0.0014192051021382213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.0013350483495742083 | Validation Loss:  0.00140991504304111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.0013324127066880465 | Validation Loss:  0.0014095766237005591\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.00133224215824157 | Validation Loss:  0.0014165592147037387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.0013339354190975428 | Validation Loss:  0.001408287906087935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.0013332997914403677 | Validation Loss:  0.0014105677837505937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.0013315826654434204 | Validation Loss:  0.0014122066786512733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.0013321086298674345 | Validation Loss:  0.0014070605393499136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.0013330456567928195 | Validation Loss:  0.0014115292578935623\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.0013320515863597393 | Validation Loss:  0.0014080475084483624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.00133112957701087 | Validation Loss:  0.0014061094261705875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.0013317203847691417 | Validation Loss:  0.0014110802439972758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.001332069979980588 | Validation Loss:  0.0014055940555408597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.0013312720693647861 | Validation Loss:  0.0014061954570934176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.0013307753251865506 | Validation Loss:  0.0014085228322073817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.0013312187511473894 | Validation Loss:  0.0014042326947674155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.0013313527451828122 | Validation Loss:  0.001406869268976152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.001330728642642498 | Validation Loss:  0.0014054697239771485\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.0013304173480719328 | Validation Loss:  0.0014035332715138793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.0013307012850418687 | Validation Loss:  0.0014067224692553282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.0013307241024449468 | Validation Loss:  0.0014032041653990746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.0013302875449880958 | Validation Loss:  0.001403470174409449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.001330049242824316 | Validation Loss:  0.001404837123118341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.0013301968574523926 | Validation Loss:  0.0014018635265529156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.0013302096631377935 | Validation Loss:  0.0014037815853953362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.0013299009297043085 | Validation Loss:  0.0014024932170286775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.0013296788092702627 | Validation Loss:  0.001401303568854928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.001329720253124833 | Validation Loss:  0.001403232105076313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.001329728402197361 | Validation Loss:  0.0014006971614435315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.001329526654444635 | Validation Loss:  0.001401316374540329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.0013293132651597261 | Validation Loss:  0.0014015125343576074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.0013292706571519375 | Validation Loss:  0.0013997473288327456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.0013292731018736959 | Validation Loss:  0.0014012547908350825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.0013291462091729045 | Validation Loss:  0.0013996436027809978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.0013289544731378555 | Validation Loss:  0.0013994055334478617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.001328850630670786 | Validation Loss:  0.001400113571435213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.0013288237387314439 | Validation Loss:  0.0013983595417812467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.0013287479523569345 | Validation Loss:  0.0013993072789162397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.0013285954482853413 | Validation Loss:  0.0013984311372041702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.0013284580782055855 | Validation Loss:  0.0013978236820548773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.001328385784290731 | Validation Loss:  0.0013985721161589026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.0013283254811540246 | Validation Loss:  0.0013970800209790468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.0013282190775498748 | Validation Loss:  0.0013976184418424964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.001328082988038659 | Validation Loss:  0.0013970941072329879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.001327970647253096 | Validation Loss:  0.0013963965466246009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.0013278931146487594 | Validation Loss:  0.001396997831761837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.001327812671661377 | Validation Loss:  0.0013957683695480227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.0013277024263516068 | Validation Loss:  0.001396083040162921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.0013275790261104703 | Validation Loss:  0.0013956428738310933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.0013274725060909986 | Validation Loss:  0.0013950059656053782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.0013273849617689848 | Validation Loss:  0.001395412371493876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.0013272945070639253 | Validation Loss:  0.001394387916661799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.0013271878706291318 | Validation Loss:  0.0013946312246844172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.0013270719209685922 | Validation Loss:  0.0013941471697762609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.0013269629562273622 | Validation Loss:  0.0013936621835455298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.0013268658658489585 | Validation Loss:  0.0013938498450443149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.0013267708709463477 | Validation Loss:  0.0013929727720096707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.001326667726971209 | Validation Loss:  0.0013931839494034648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.0013265566667541862 | Validation Loss:  0.0013925887178629637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.0013264452572911978 | Validation Loss:  0.0013923050137236714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.0013263400178402662 | Validation Loss:  0.001392242731526494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.0013262389693409204 | Validation Loss:  0.0013915367890149355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.001326136989519 | Validation Loss:  0.0013916842872276902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.001326030120253563 | Validation Loss:  0.0013910002307966352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.0013259192928671837 | Validation Loss:  0.0013909217668697238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.00132580881472677 | Validation Loss:  0.001390583347529173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.0013257008977234364 | Validation Loss:  0.0013901281636208296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.0013255949597805738 | Validation Loss:  0.0013900964986532927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.0013254891382530332 | Validation Loss:  0.0013894443400204182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.001325380988419056 | Validation Loss:  0.0013894580770283937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.001325270626693964 | Validation Loss:  0.0013888886896893382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.001325158984400332 | Validation Loss:  0.0013887156965211034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.0013250475749373436 | Validation Loss:  0.0013883793726563454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.0013249370967969298 | Validation Loss:  0.0013879613252356648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.0013248272007331252 | Validation Loss:  0.0013878284953534603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.0013247173046693206 | Validation Loss:  0.0013872676063328981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.0013246064772829413 | Validation Loss:  0.0013872046256437898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.0013244944857433438 | Validation Loss:  0.0013866424560546875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.0013243815628811717 | Validation Loss:  0.001386512303724885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.001324267708696425 | Validation Loss:  0.00138604745734483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.001324153970927 | Validation Loss:  0.001385784475132823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.0013240400003269315 | Validation Loss:  0.0013854524586349726\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.0013239261461421847 | Validation Loss:  0.0013850630493834615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.0013238121755421162 | Validation Loss:  0.0013848412781953812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.0013236978556960821 | Validation Loss:  0.0013843587366864085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.0013235833030194044 | Validation Loss:  0.0013842005282640457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.0013234681682661176 | Validation Loss:  0.0013836657162755728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.001323352800682187 | Validation Loss:  0.0013835364952683449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.0013232366181910038 | Validation Loss:  0.0013829806121066213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.0013231202028691769 | Validation Loss:  0.0013828636147081852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.0013230033218860626 | Validation Loss:  0.0013822924811393023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.0013228862080723047 | Validation Loss:  0.0013821865431964397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.0013227687450125813 | Validation Loss:  0.0013815868878737092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.001322651281952858 | Validation Loss:  0.001381516456604004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.0013225334696471691 | Validation Loss:  0.0013808589428663254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.001322415773756802 | Validation Loss:  0.0013808723306283355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.001322298776358366 | Validation Loss:  0.0013800952583551407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.001322182477451861 | Validation Loss:  0.0013802776811644435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.0013220686232671142 | Validation Loss:  0.0013792666140943766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.0013219582615420222 | Validation Loss:  0.0013797908322885633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.0013218565145507455 | Validation Loss:  0.001378330634906888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.0013217703672125936 | Validation Loss:  0.0013795698760077357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.0013217166997492313 | Validation Loss:  0.0013772365637123585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.0013217305531725287 | Validation Loss:  0.0013800690649077296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.0013218829408288002 | Validation Loss:  0.00137609604280442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.00132235256023705 | Validation Loss:  0.0013828914379701018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.0013234690995886922 | Validation Loss:  0.0013764477334916592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.0013262222055345774 | Validation Loss:  0.0013946895487606525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.0013321376172825694 | Validation Loss:  0.001389039563946426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.001347079873085022 | Validation Loss:  0.001444200286641717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.0013754410902038217 | Validation Loss:  0.0014689827803522348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.0014481233665719628 | Validation Loss:  0.0016009900718927383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.0015247439732775092 | Validation Loss:  0.0016700448468327522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.0016838443698361516 | Validation Loss:  0.0016437422018498182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.0015625180676579475 | Validation Loss:  0.0014575397362932563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.0014316416345536709 | Validation Loss:  0.0013826332287862897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.0013223588466644287 | Validation Loss:  0.001465259469114244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.0013968802522867918 | Validation Loss:  0.001535705290734768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.0015209323028102517 | Validation Loss:  0.0014923642156645656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.001418826635926962 | Validation Loss:  0.001378198736347258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.0013264764565974474 | Validation Loss:  0.0013992964522913098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.0013618324883282185 | Validation Loss:  0.001482065417803824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.0014059508685022593 | Validation Loss:  0.0014094815123826265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.0013697943650186062 | Validation Loss:  0.001381609938107431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.001322113792411983 | Validation Loss:  0.001434943056665361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.0013637634692713618 | Validation Loss:  0.0014277978334575891\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.001389360404573381 | Validation Loss:  0.0014018530491739511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.0013320024590939283 | Validation Loss:  0.0014083937276154757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.0013369922526180744 | Validation Loss:  0.001415023347362876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.0013737569097429514 | Validation Loss:  0.0014086731243878603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.0013389801606535912 | Validation Loss:  0.0013885374646633863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.0013242854038253427 | Validation Loss:  0.0013982615200802684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.0013519893400371075 | Validation Loss:  0.0014122907305136323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.0013416606234386563 | Validation Loss:  0.0013823723420500755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.0013217962114140391 | Validation Loss:  0.0013863700442016125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.0013348731445148587 | Validation Loss:  0.0014087628806009889\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.01, 'optimizer': 'Adam'} Test Loss:  0.0011564893648028374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.026276126503944397 | Validation Loss:  1.6759430170059204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  1.6768746376037598 | Validation Loss:  0.029952386394143105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.0290809515863657 | Validation Loss:  1.483262062072754\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  1.4944303035736084 | Validation Loss:  0.5804784297943115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.5923298001289368 | Validation Loss:  0.06254680454730988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.06756748259067535 | Validation Loss:  0.2539026737213135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.2508716583251953 | Validation Loss:  0.2046077698469162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.2022627294063568 | Validation Loss:  0.01808933913707733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.019863314926624298 | Validation Loss:  0.030828000977635384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.03426811844110489 | Validation Loss:  0.020584329962730408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.020686261355876923 | Validation Loss:  0.014107395894825459\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.01421535573899746 | Validation Loss:  0.10691387951374054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.11158555746078491 | Validation Loss:  0.025554046034812927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.026062490418553352 | Validation Loss:  0.9138948917388916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.9123953580856323 | Validation Loss:  0.6139445304870605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.6159926056861877 | Validation Loss:  0.5251874923706055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.5310023427009583 | Validation Loss:  0.031496815383434296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.035175759345293045 | Validation Loss:  0.08495137095451355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.08498091250658035 | Validation Loss:  0.14040213823318481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.13947561383247375 | Validation Loss:  0.06725723296403885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.06711645424365997 | Validation Loss:  0.014833944849669933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.01569056324660778 | Validation Loss:  0.010067317634820938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.011720839887857437 | Validation Loss:  0.018261635676026344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.020497621968388557 | Validation Loss:  0.024625055491924286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.027247313410043716 | Validation Loss:  0.02713879756629467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.029979178681969643 | Validation Loss:  0.026727838441729546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.029650848358869553 | Validation Loss:  0.02455790527164936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.0274589192122221 | Validation Loss:  0.021575290709733963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.02437642216682434 | Validation Loss:  0.01848212257027626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.021127961575984955 | Validation Loss:  0.015764469280838966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.018218381330370903 | Validation Loss:  0.013718360103666782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.01595934107899666 | Validation Loss:  0.012474083341658115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.014494070783257484 | Validation Loss:  0.012023200280964375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.013824730180203915 | Validation Loss:  0.012249741703271866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.013843868859112263 | Validation Loss:  0.012964965775609016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.014369359239935875 | Validation Loss:  0.013943617232143879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.015180782414972782 | Validation Loss:  0.014958892948925495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.01605449989438057 | Validation Loss:  0.015812912955880165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.016794204711914062 | Validation Loss:  0.016359619796276093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.017253907397389412 | Validation Loss:  0.01651776395738125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.01735103130340576 | Validation Loss:  0.016273071989417076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.01706872507929802 | Validation Loss:  0.015670228749513626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.016448020935058594 | Validation Loss:  0.014797123149037361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.015572242438793182 | Validation Loss:  0.013764576986432076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.014546964317560196 | Validation Loss:  0.012685407884418964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.013479266315698624 | Validation Loss:  0.011656055226922035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.012459609657526016 | Validation Loss:  0.010743258520960808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.01154873427003622 | Validation Loss:  0.009977086447179317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.010771000757813454 | Validation Loss:  0.009350616484880447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.010114396922290325 | Validation Loss:  0.00882560946047306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.009536664932966232 | Validation Loss:  0.008343040943145752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.008976380340754986 | Validation Loss:  0.00783697422593832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.008367495611310005 | Validation Loss:  0.007250300142914057\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.007655861787497997 | Validation Loss:  0.0065508680418133736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.0068161264061927795 | Validation Loss:  0.005746300797909498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.005867011845111847 | Validation Loss:  0.004894415847957134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.0048814075998961926 | Validation Loss:  0.004103102721273899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.003984423354268074 | Validation Loss:  0.0035095217172056437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.003328680992126465 | Validation Loss:  0.0032288196962326765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.0030373611953109503 | Validation Loss:  0.003278813324868679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.0031235311180353165 | Validation Loss:  0.0035250901710242033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0034338899422436953 | Validation Loss:  0.003719670930877328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.003693025792017579 | Validation Loss:  0.003654863452538848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.003668509889394045 | Validation Loss:  0.0033217102754861116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.0033398966770619154 | Validation Loss:  0.0029136461671441793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.002905464032664895 | Validation Loss:  0.002665379550307989\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0026152613572776318 | Validation Loss:  0.002684403210878372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.0025929564144462347 | Validation Loss:  0.0029069315642118454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0027849366888403893 | Validation Loss:  0.0031699123792350292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0030311562586575747 | Validation Loss:  0.0033160774037241936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0031729312613606453 | Validation Loss:  0.0032679017167538404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0031301884446293116 | Validation Loss:  0.0030495268292725086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.0029250341467559338 | Validation Loss:  0.0027615430299192667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0026562337297946215 | Validation Loss:  0.002524338196963072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.0024413377977907658 | Validation Loss:  0.002414245391264558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.002352176234126091 | Validation Loss:  0.0024247507099062204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.002376990159973502 | Validation Loss:  0.002479370916262269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.0024355528876185417 | Validation Loss:  0.002491193590685725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0024407822638750076 | Validation Loss:  0.0024243297521024942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.002360499696806073 | Validation Loss:  0.00230892444960773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.0022306665778160095 | Validation Loss:  0.002205388154834509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.0021169150713831186 | Validation Loss:  0.002156563801690936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.0020647025667130947 | Validation Loss:  0.002165082609280944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.0020762288477271795 | Validation Loss:  0.0022022435441613197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0021202508360147476 | Validation Loss:  0.0022320044226944447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0021575677674263716 | Validation Loss:  0.0022319029085338116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0021632055286318064 | Validation Loss:  0.0022016323637217283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.002135632326826453 | Validation Loss:  0.0021586420480161905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.0020923702977597713 | Validation Loss:  0.0021253565791994333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0020568061154335737 | Validation Loss:  0.002115199575200677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.0020436758641153574 | Validation Loss:  0.002124960534274578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.00205097533762455 | Validation Loss:  0.0021379778627306223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.002062891609966755 | Validation Loss:  0.0021361103281378746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.00206170161254704 | Validation Loss:  0.0021120738238096237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.0020401517394930124 | Validation Loss:  0.0020732872653752565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.002005369868129492 | Validation Loss:  0.0020351437851786613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.001972203142940998 | Validation Loss:  0.002009875839576125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.0019521649228408933 | Validation Loss:  0.0019995151087641716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0019465271616354585 | Validation Loss:  0.001996926963329315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0019475326407700777 | Validation Loss:  0.0019925704691559076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0019453206332400441 | Validation Loss:  0.001981444889679551\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.0019349543144926429 | Validation Loss:  0.0019657164812088013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0019189923768863082 | Validation Loss:  0.00195196398999542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.00190456653945148 | Validation Loss:  0.0019455442670732737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.001897551235742867 | Validation Loss:  0.0019463354256004095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.0018981787143275142 | Validation Loss:  0.0019490047125145793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.0019012849079445004 | Validation Loss:  0.001947383163496852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.0019007448572665453 | Validation Loss:  0.001939173205755651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0018942459719255567 | Validation Loss:  0.001927099539898336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0018844516016542912 | Validation Loss:  0.0019159660441800952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.0018760230159386992 | Validation Loss:  0.0019086223328486085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0018715522019192576 | Validation Loss:  0.0019042770145460963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0018699020147323608 | Validation Loss:  0.0019000411266461015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0018678329652175307 | Validation Loss:  0.0018938221037387848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.001863014535047114 | Validation Loss:  0.0018860239069908857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.0018558038864284754 | Validation Loss:  0.001878881361335516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0018486010376363993 | Validation Loss:  0.0018743100808933377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0018436465179547668 | Validation Loss:  0.0018722068052738905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0018412386998534203 | Validation Loss:  0.0018706003902480006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.001839782577008009 | Validation Loss:  0.0018673615995794535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0018374088685959578 | Validation Loss:  0.0018618772737681866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.001833581831306219 | Validation Loss:  0.0018552805995568633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.0018293040338903666 | Validation Loss:  0.001849259133450687\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0018259503412991762 | Validation Loss:  0.0018446797039359808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0018239745404571295 | Validation Loss:  0.0018412066856399179\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0018226387910544872 | Validation Loss:  0.001837999909184873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0018208244582638144 | Validation Loss:  0.001834652735851705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.0018180427141487598 | Validation Loss:  0.0018314861226826906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0018147412920370698 | Validation Loss:  0.0018290475709363818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.001811753842048347 | Validation Loss:  0.0018273970345035195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.0018094982951879501 | Validation Loss:  0.0018259059870615602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.0018076732521876693 | Validation Loss:  0.0018237396143376827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.0018056597327813506 | Validation Loss:  0.001820541569031775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.0018031588988378644 | Validation Loss:  0.0018166769295930862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0018004359444603324 | Validation Loss:  0.0018128646770492196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0017979900585487485 | Validation Loss:  0.00180960597936064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0017960526747629046 | Validation Loss:  0.0018069347133859992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.001794413081370294 | Validation Loss:  0.0018046223558485508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0017926926957443357 | Validation Loss:  0.0018025257159024477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.0017907250439748168 | Validation Loss:  0.0018007047474384308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0017886738060042262 | Validation Loss:  0.001799234887585044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0017868068534880877 | Validation Loss:  0.0017979755066335201\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0017852059099823236 | Validation Loss:  0.001796571072191\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.001783704967238009 | Validation Loss:  0.0017946987645700574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0017820874927565455 | Validation Loss:  0.0017923134146258235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0017803088994696736 | Validation Loss:  0.0017896561184898019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0017785107484087348 | Validation Loss:  0.0017870466690510511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0017768467077985406 | Validation Loss:  0.0017846755217760801\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0017753253923729062 | Validation Loss:  0.0017825700342655182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.001773830852471292 | Validation Loss:  0.0017806992400437593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.001772266230545938 | Validation Loss:  0.0017790605779737234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.001770655158907175 | Validation Loss:  0.0017776488093659282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.0017690977547317743 | Validation Loss:  0.0017763773212209344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0017676525749266148 | Validation Loss:  0.0017750683473423123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.001766279456205666 | Validation Loss:  0.0017735518049448729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.0017648967914283276 | Validation Loss:  0.001771781942807138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0017634723335504532 | Validation Loss:  0.0017698609735816717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0017620464786887169 | Validation Loss:  0.001767955138348043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.0017606737092137337 | Validation Loss:  0.0017661928432062268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.0017593574011698365 | Validation Loss:  0.0017646262422204018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.001758053433150053 | Validation Loss:  0.001763258595019579\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0017567259492352605 | Validation Loss:  0.0017620718572288752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0017553839134052396 | Validation Loss:  0.0017610228387638927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.001754064462147653 | Validation Loss:  0.0017600255087018013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0017527861054986715 | Validation Loss:  0.0017589702038094401\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.001751530566252768 | Validation Loss:  0.0017577777616679668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0017502710688859224 | Validation Loss:  0.0017564462032169104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.001749004004523158 | Validation Loss:  0.0017550447955727577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.001747749513015151 | Validation Loss:  0.0017536679515615106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0017465243581682444 | Validation Loss:  0.001752384821884334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0017453221371397376 | Validation Loss:  0.0017512260237708688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.001744124572724104 | Validation Loss:  0.0017501900438219309\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0017429239815101027 | Validation Loss:  0.0017492513870820403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.001741729793138802 | Validation Loss:  0.0017483595293015242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.001740553299896419 | Validation Loss:  0.0017474477645009756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.0017393933376297355 | Validation Loss:  0.0017464595148339868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.0017382384976372123 | Validation Loss:  0.0017453761538490653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.0017370828427374363 | Validation Loss:  0.0017442256212234497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0017359309131279588 | Validation Loss:  0.0017430648440495133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0017347915563732386 | Validation Loss:  0.0017419483046978712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.0017336646560579538 | Validation Loss:  0.0017409106949344277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.0017325448570773005 | Validation Loss:  0.001739961444400251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0017314273864030838 | Validation Loss:  0.0017390872817486525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0017303151544183493 | Validation Loss:  0.0017382557271048427\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.0017292125849053264 | Validation Loss:  0.0017374244052916765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.001728120376355946 | Validation Loss:  0.0017365560634061694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.001727033988572657 | Validation Loss:  0.001735638128593564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.0017259507440030575 | Validation Loss:  0.0017346855020150542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.001724872039631009 | Validation Loss:  0.0017337319441139698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.0017238006694242358 | Validation Loss:  0.001732813660055399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0017227366333827376 | Validation Loss:  0.0017319534672424197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0017216777196153998 | Validation Loss:  0.0017311562551185489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0017206218326464295 | Validation Loss:  0.0017304102657362819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.0017195706022903323 | Validation Loss:  0.001729690353386104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.0017185256583616138 | Validation Loss:  0.0017289674142375588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.001717486884444952 | Validation Loss:  0.0017282188637182117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.0017164526507258415 | Validation Loss:  0.001727439695969224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0017154219094663858 | Validation Loss:  0.0017266422510147095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0017143955919891596 | Validation Loss:  0.0017258492298424244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0017133752116933465 | Validation Loss:  0.001725082052871585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0017123599536716938 | Validation Loss:  0.0017243532929569483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.001711348770186305 | Validation Loss:  0.00172366201877594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0017103413119912148 | Validation Loss:  0.0017229968216270208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0017093382775783539 | Validation Loss:  0.0017223386093974113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0017083397833630443 | Validation Loss:  0.0017216700362041593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.0017073461785912514 | Validation Loss:  0.0017209810903295875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0017063560662791133 | Validation Loss:  0.0017202752642333508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.00170536944642663 | Validation Loss:  0.0017195650143548846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.0017043874831870198 | Validation Loss:  0.0017188664060086012\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.00170340936165303 | Validation Loss:  0.0017181913135573268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.0017024354310706258 | Validation Loss:  0.001717544044367969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.00170146522577852 | Validation Loss:  0.0017169201746582985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0017004983965307474 | Validation Loss:  0.0017163081793114543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.0016995357582345605 | Validation Loss:  0.0017156954854726791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0016985773108899593 | Validation Loss:  0.001715074060484767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.0016976220067590475 | Validation Loss:  0.0017144433222711086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.0016966706607490778 | Validation Loss:  0.0017138097900897264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.001695722690783441 | Validation Loss:  0.0017131840577349067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0016947784461081028 | Validation Loss:  0.0017125748563557863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0016938376938924193 | Validation Loss:  0.0017119860276579857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.0016929005505517125 | Validation Loss:  0.0017114151269197464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.001691966550424695 | Validation Loss:  0.0017108545871451497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.001691036275587976 | Validation Loss:  0.0017102956771850586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.001690109260380268 | Validation Loss:  0.0017097318777814507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0016891855048015714 | Validation Loss:  0.0017091627232730389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.001688265590928495 | Validation Loss:  0.0017085917061194777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.0016873485874384642 | Validation Loss:  0.0017080256948247552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.00168643519282341 | Validation Loss:  0.0017074703937396407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0016855248250067234 | Validation Loss:  0.0017069289460778236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0016846178332343698 | Validation Loss:  0.0017063989071175456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.001683714217506349 | Validation Loss:  0.0017058755038306117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.001682813628576696 | Validation Loss:  0.0017053529154509306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0016819164156913757 | Validation Loss:  0.0017048269510269165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0016810219967737794 | Validation Loss:  0.0017042980762198567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0016801313031464815 | Validation Loss:  0.0017037694342434406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0016792432870715857 | Validation Loss:  0.0017032453324645758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.001678358530625701 | Validation Loss:  0.0017027303110808134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.0016774769173935056 | Validation Loss:  0.0017022248357534409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0016765983309596777 | Validation Loss:  0.0017017277423292398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0016757227713242173 | Validation Loss:  0.001701235305517912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.0016748503549024463 | Validation Loss:  0.0017007434507831931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.0016739810816943645 | Validation Loss:  0.0017002505483105779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.0016731146024540067 | Validation Loss:  0.0016997571801766753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0016722511500120163 | Validation Loss:  0.0016992662567645311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.0016713908407837152 | Validation Loss:  0.0016987810377031565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.0016705332091078162 | Validation Loss:  0.0016983029199764132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0016696787206456065 | Validation Loss:  0.0016978324856609106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.0016688271425664425 | Validation Loss:  0.0016973669407889247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.001667978591285646 | Validation Loss:  0.0016969041898846626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0016671326011419296 | Validation Loss:  0.0016964420210570097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0016662896377965808 | Validation Loss:  0.0016959793865680695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.0016654495848342776 | Validation Loss:  0.0016955180326476693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.0016646123258396983 | Validation Loss:  0.0016950599383562803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0016637779772281647 | Validation Loss:  0.0016946063842624426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.0016629465389996767 | Validation Loss:  0.0016941590001806617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.0016621177783235908 | Validation Loss:  0.0016937158070504665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0016612919280305505 | Validation Loss:  0.001693275524303317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.001660468871705234 | Validation Loss:  0.0016928365221247077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.0016596484929323196 | Validation Loss:  0.0016923976363614202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0016588307917118073 | Validation Loss:  0.001691959798336029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0016580161172896624 | Validation Loss:  0.0016915238229557872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0016572040040045977 | Validation Loss:  0.001691092038527131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0016563944518566132 | Validation Loss:  0.0016906640958040953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0016555879265069962 | Validation Loss:  0.0016902403440326452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0016547838458791375 | Validation Loss:  0.0016898195026442409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0016539825592190027 | Validation Loss:  0.0016894001746550202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0016531837172806263 | Validation Loss:  0.001688981894403696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0016523877857252955 | Validation Loss:  0.001688564894720912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0016515945317223668 | Validation Loss:  0.0016881497576832771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0016508036060258746 | Validation Loss:  0.0016877376474440098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.001650015590712428 | Validation Loss:  0.001687329146079719\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0016492301365360618 | Validation Loss:  0.0016869239043444395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.0016484471270814538 | Validation Loss:  0.0016865214565768838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.0016476667951792479 | Validation Loss:  0.00168612040579319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0016468892572447658 | Validation Loss:  0.0016857207519933581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.0016461139312013984 | Validation Loss:  0.0016853220295161009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.001645341282710433 | Validation Loss:  0.0016849254025146365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.001644571078941226 | Validation Loss:  0.0016845309874042869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.001643803552724421 | Validation Loss:  0.0016841397155076265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.0016430382383987308 | Validation Loss:  0.0016837511211633682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0016422758344560862 | Validation Loss:  0.0016833647387102246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.0016415155259892344 | Validation Loss:  0.0016829794039949775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0016407578950747848 | Validation Loss:  0.001682595699094236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0016400028252974153 | Validation Loss:  0.001682212925516069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0016392500838264823 | Validation Loss:  0.001681832130998373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0016385000199079514 | Validation Loss:  0.00168145343195647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0016377520514652133 | Validation Loss:  0.0016810771776363254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0016370064113289118 | Validation Loss:  0.0016807031352072954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.0016362634487450123 | Validation Loss:  0.0016803308390080929\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0016355228144675493 | Validation Loss:  0.001679960056208074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0016347842756658792 | Validation Loss:  0.0016795903211459517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0016340482980012894 | Validation Loss:  0.0016792220994830132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.0016333147650584579 | Validation Loss:  0.0016788558568805456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0016325835604220629 | Validation Loss:  0.0016784913605079055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0016318546840921044 | Validation Loss:  0.0016781289596110582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0016311281360685825 | Validation Loss:  0.001677768537774682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.0016304035671055317 | Validation Loss:  0.0016774098621681333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.001629681559279561 | Validation Loss:  0.001677052234299481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.001628961879760027 | Validation Loss:  0.001676695654168725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0016282444121316075 | Validation Loss:  0.0016763408202677965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.0016275293892249465 | Validation Loss:  0.0016759874997660518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.0016268163453787565 | Validation Loss:  0.0016756360419094563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.0016261053970083594 | Validation Loss:  0.00167528644669801\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.0016253967769443989 | Validation Loss:  0.0016749384813010693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.0016246906016021967 | Validation Loss:  0.0016745914472267032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.0016239865217357874 | Validation Loss:  0.001674245810136199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.0016232846537604928 | Validation Loss:  0.001673901453614235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.0016225847648456693 | Validation Loss:  0.001673558377660811\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.0016218872042372823 | Validation Loss:  0.0016732171643525362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.0016211919719353318 | Validation Loss:  0.0016728773480281234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.0016204986022785306 | Validation Loss:  0.0016725389286875725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.001619807444512844 | Validation Loss:  0.0016722019063308835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.0016191182658076286 | Validation Loss:  0.0016718662809580564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.0016184312989935279 | Validation Loss:  0.0016715317033231258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.0016177465440705419 | Validation Loss:  0.0016711985226720572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.0016170637682080269 | Validation Loss:  0.001670866389758885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.0016163830878213048 | Validation Loss:  0.0016705357702448964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.0016157043864950538 | Validation Loss:  0.0016702066641300917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.0016150280134752393 | Validation Loss:  0.0016698790714144707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.0016143532702699304 | Validation Loss:  0.0016695524100214243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.0016136809717863798 | Validation Loss:  0.0016692269127815962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.0016130103031173348 | Validation Loss:  0.0016689025796949863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.0016123419627547264 | Validation Loss:  0.0016685794107615948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.0016116752522066236 | Validation Loss:  0.0016682575223967433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.0016110107535496354 | Validation Loss:  0.0016679372638463974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.0016103481175377965 | Validation Loss:  0.001667618053033948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.0016096876934170723 | Validation Loss:  0.001667299773544073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.0016090288991108537 | Validation Loss:  0.0016669825417920947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.001608372200280428 | Validation Loss:  0.0016666665906086564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.0016077173640951514 | Validation Loss:  0.0016663518035784364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.0016070642741397023 | Validation Loss:  0.0016660380642861128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.001606413396075368 | Validation Loss:  0.0016657256055623293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.0016057644970715046 | Validation Loss:  0.001665414427407086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.0016051172278821468 | Validation Loss:  0.0016651040641590953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.0016044718213379383 | Validation Loss:  0.0016647946322336793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.0016038283938542008 | Validation Loss:  0.0016644864808768034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.001603186596184969 | Validation Loss:  0.0016641794936731458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.0016025471268221736 | Validation Loss:  0.001663873321376741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.001601909170858562 | Validation Loss:  0.0016635681968182325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.0016012726118788123 | Validation Loss:  0.0016632641199976206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.0016006386140361428 | Validation Loss:  0.0016629615565761924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.0016000060131773353 | Validation Loss:  0.0016626595752313733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.0015993749257177114 | Validation Loss:  0.001662358408793807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.0015987460501492023 | Validation Loss:  0.0016620582900941372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.001598118687979877 | Validation Loss:  0.0016617594519630075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.0015974933048710227 | Validation Loss:  0.0016614614287391305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.0015968694351613522 | Validation Loss:  0.0016611642204225063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.0015962474280968308 | Validation Loss:  0.001660868525505066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.0015956270508468151 | Validation Loss:  0.001660573179833591\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.0015950084198266268 | Validation Loss:  0.0016602791147306561\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.0015943915350362659 | Validation Loss:  0.0016599857481196523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.0015937762800604105 | Validation Loss:  0.0016596934292465448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.0015931628877297044 | Validation Loss:  0.0016594019252806902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.0015925508923828602 | Validation Loss:  0.001659111469052732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.0015919405268505216 | Validation Loss:  0.0016588218277320266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.0015913319075480103 | Validation Loss:  0.0016585328849032521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.0015907251508906484 | Validation Loss:  0.0016582449898123741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.0015901196748018265 | Validation Loss:  0.0016579576767981052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.001589515944942832 | Validation Loss:  0.0016576715279370546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.001588913961313665 | Validation Loss:  0.0016573863103985786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.0015883133746683598 | Validation Loss:  0.00165710155852139\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.001587714534252882 | Validation Loss:  0.0016568180872127414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.0015871170908212662 | Validation Loss:  0.00165653508156538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.001586521277204156 | Validation Loss:  0.0016562530072405934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.0015859270934015512 | Validation Loss:  0.0016559716314077377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.0015853343065828085 | Validation Loss:  0.0016556913033127785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.0015847432659938931 | Validation Loss:  0.0016554114408791065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.001584153505973518 | Validation Loss:  0.0016551325097680092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.00158356549218297 | Validation Loss:  0.0016548546263948083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.0015829786425456405 | Validation Loss:  0.001654576975852251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.0015823936555534601 | Validation Loss:  0.0016543002566322684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.0015818100655451417 | Validation Loss:  0.0016540244687348604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.0015812277561053634 | Validation Loss:  0.0016537492629140615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.0015806470764800906 | Validation Loss:  0.0016534748720005155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.0015800677938386798 | Validation Loss:  0.0016532012959942222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.0015794900245964527 | Validation Loss:  0.0016529281856492162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.001578913419507444 | Validation Loss:  0.0016526561230421066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.0015783385606482625 | Validation Loss:  0.001652384176850319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.0015777652151882648 | Validation Loss:  0.0016521136276423931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.0015771930338814855 | Validation Loss:  0.001651843311265111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.0015766224823892117 | Validation Loss:  0.001651573576964438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.0015760528622195125 | Validation Loss:  0.0016513047739863396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.0015754849882796407 | Validation Loss:  0.0016510364366695285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.001574918394908309 | Validation Loss:  0.001650769030675292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.0015743531985208392 | Validation Loss:  0.0016505018575116992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.0015737892827019095 | Validation Loss:  0.001650235615670681\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.00157322664745152 | Validation Loss:  0.001649969955906272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.0015726654091849923 | Validation Loss:  0.0016497051110491157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.0015721055679023266 | Validation Loss:  0.0016494407318532467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.001571547007188201 | Validation Loss:  0.001649176818318665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.0015709897270426154 | Validation Loss:  0.0016489134868606925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.0015704338438808918 | Validation Loss:  0.001648650853894651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.0015698791248723865 | Validation Loss:  0.0016483889194205403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.001569325802847743 | Validation Loss:  0.0016481275670230389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.0015687737613916397 | Validation Loss:  0.0016478665638715029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.0015682227676734328 | Validation Loss:  0.0016476063756272197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.0015676731709390879 | Validation Loss:  0.0016473463037982583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.001567124854773283 | Validation Loss:  0.0016470872797071934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.0015665777027606964 | Validation Loss:  0.001646828604862094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.001566031714901328 | Validation Loss:  0.0016465706285089254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.0015654872404411435 | Validation Loss:  0.0016463128849864006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.0015649435808882117 | Validation Loss:  0.0016460559563711286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.0015644014347344637 | Validation Loss:  0.001645799377001822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.001563860336318612 | Validation Loss:  0.0016455432632938027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.0015633204020559788 | Validation Loss:  0.0016452876152470708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.00156278139911592 | Validation Loss:  0.0016450328985229135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.001562244025990367 | Validation Loss:  0.001644778298214078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.0015617077006027102 | Validation Loss:  0.001644524047151208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.00156117242295295 | Validation Loss:  0.0016442706109955907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.001560638309456408 | Validation Loss:  0.001644017407670617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.0015601051272824407 | Validation Loss:  0.0016437649028375745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.001559573458507657 | Validation Loss:  0.0016435128636658192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.0015590426046401262 | Validation Loss:  0.0016432610573247075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.0015585129149258137 | Validation Loss:  0.0016430097166448832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.0015579845057800412 | Validation Loss:  0.0016427586087957025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.0015574570279568434 | Validation Loss:  0.0016425083158537745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.0015569308307021856 | Validation Loss:  0.001642258488573134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.0015564053319394588 | Validation Loss:  0.0016420090105384588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.001555881230160594 | Validation Loss:  0.001641759998165071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.00155535782687366 | Validation Loss:  0.0016415112186223269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.00155483593698591 | Validation Loss:  0.00164126290474087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.0015543148620054126 | Validation Loss:  0.0016410149401053786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.001553794601932168 | Validation Loss:  0.0016407674411311746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.0015532758552581072 | Validation Loss:  0.001640520291402936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.0015527576906606555 | Validation Loss:  0.0016402736073359847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.001552240806631744 | Validation Loss:  0.001640027272514999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.0015517249703407288 | Validation Loss:  0.0016397815197706223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.0015512099489569664 | Validation Loss:  0.0016395356506109238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.0015506958588957787 | Validation Loss:  0.0016392902471125126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.001550183049403131 | Validation Loss:  0.0016390453092753887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.0015496710548177361 | Validation Loss:  0.0016388010699301958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.001549159875139594 | Validation Loss:  0.001638556714169681\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.0015486496267840266 | Validation Loss:  0.0016383128240704536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.0015481406589969993 | Validation Loss:  0.0016380692832171917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.0015476323897019029 | Validation Loss:  0.0016378258587792516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.001547125168144703 | Validation Loss:  0.0016375831328332424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.0015466188779100776 | Validation Loss:  0.001637340639717877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.001546113402582705 | Validation Loss:  0.001637098379433155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.0015456087421625853 | Validation Loss:  0.0016368563519790769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.0015451053623110056 | Validation Loss:  0.0016366145573556423\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.0015446026809513569 | Validation Loss:  0.001636373344808817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.0015441006980836391 | Validation Loss:  0.0016361323650926352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.0015435999957844615 | Validation Loss:  0.0016358915017917752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.0015430999919772148 | Validation Loss:  0.001635650871321559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.001542600686661899 | Validation Loss:  0.0016354108229279518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.0015421025454998016 | Validation Loss:  0.0016351707745343447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.0015416048699989915 | Validation Loss:  0.0016349311918020248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.0015411084750667214 | Validation Loss:  0.0016346917254850268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.0015406126622110605 | Validation Loss:  0.0016344524919986725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.001540117897093296 | Validation Loss:  0.0016342137241736054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.0015396238304674625 | Validation Loss:  0.0016339749563485384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.0015391305787488818 | Validation Loss:  0.0016337365377694368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.001538638025522232 | Validation Loss:  0.0016334984684363008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.0015381465200334787 | Validation Loss:  0.0016332603991031647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.0015376559458673 | Validation Loss:  0.0016330230282619596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.0015371654881164432 | Validation Loss:  0.0016327851917594671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.0015366766601800919 | Validation Loss:  0.001632547820918262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.0015361880650743842 | Validation Loss:  0.0016323107993230224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.0015357002848759294 | Validation Loss:  0.0016320737777277827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.001535213552415371 | Validation Loss:  0.0016318372217938304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.0015347275184467435 | Validation Loss:  0.0016316008986905217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.0015342420665547252 | Validation Loss:  0.0016313649248331785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.0015337575459852815 | Validation Loss:  0.001631128485314548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.001533273607492447 | Validation Loss:  0.0016308928607031703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.0015327903674915433 | Validation Loss:  0.0016306571196764708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.0015323081752285361 | Validation Loss:  0.0016304217278957367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.0015318264486268163 | Validation Loss:  0.0016301865689456463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.0015313455369323492 | Validation Loss:  0.0016299514099955559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.001530865323729813 | Validation Loss:  0.0016297162510454655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.001530385809019208 | Validation Loss:  0.0016294814413413405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.0015299071092158556 | Validation Loss:  0.001629246980883181\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.001529429224319756 | Validation Loss:  0.0016290125204250216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.0015289515722543001 | Validation Loss:  0.001628778176382184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.0015284748515114188 | Validation Loss:  0.00162854406516999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.0015279988292604685 | Validation Loss:  0.001628309953957796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.0015275232726708055 | Validation Loss:  0.0016280761919915676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.001527048647403717 | Validation Loss:  0.001627842546440661\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.0015265746042132378 | Validation Loss:  0.0016276089008897543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.0015261013759300113 | Validation Loss:  0.0016273756045848131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.001525628613308072 | Validation Loss:  0.0016271421918645501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.0015251563163474202 | Validation Loss:  0.001626908895559609\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.001524684950709343 | Validation Loss:  0.001626676064915955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.0015242141671478748 | Validation Loss:  0.0016264431178569794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.0015237440820783377 | Validation Loss:  0.0016262104036286473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.0015232745790854096 | Validation Loss:  0.0016259775729849935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.001522805541753769 | Validation Loss:  0.0016257449751719832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.001522337319329381 | Validation Loss:  0.0016255126101896167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.0015218695625662804 | Validation Loss:  0.0016252802452072501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.0015214025042951107 | Validation Loss:  0.0016250481130555272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.0015209359116852283 | Validation Loss:  0.0016248158644884825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.0015204699011519551 | Validation Loss:  0.0016245837323367596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.0015200047055259347 | Validation Loss:  0.0016243518330156803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.0015195399755612016 | Validation Loss:  0.0016241200501099229\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.0015190758276730776 | Validation Loss:  0.0016238881507888436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.0015186122618615627 | Validation Loss:  0.0016236566007137299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.001518149278126657 | Validation Loss:  0.001623425050638616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.0015176868764683604 | Validation Loss:  0.0016231935005635023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.0015172248240560293 | Validation Loss:  0.0016229619504883885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.0015167637029662728 | Validation Loss:  0.0016227307496592402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.0015163029311224818 | Validation Loss:  0.001622499548830092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.0015158426249399781 | Validation Loss:  0.0016222683480009437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.0015153831336647272 | Validation Loss:  0.001622037380002439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.0015149237588047981 | Validation Loss:  0.0016218061791732907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.0015144651988521218 | Validation Loss:  0.001621575327590108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.0015140071045607328 | Validation Loss:  0.001621344592422247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.0015135493595153093 | Validation Loss:  0.0016211133915930986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.0015130924293771386 | Validation Loss:  0.0016208826564252377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.001512636081315577 | Validation Loss:  0.0016206520376726985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.001512179966084659 | Validation Loss:  0.0016204215353354812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.0015117244329303503 | Validation Loss:  0.0016201906837522984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.0015112694818526506 | Validation Loss:  0.0016199599485844374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.0015108149964362383 | Validation Loss:  0.0016197297954931855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.0015103609766811132 | Validation Loss:  0.0016194990603253245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.0015099075390025973 | Validation Loss:  0.0016192687908187509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.001509454334154725 | Validation Loss:  0.0016190384048968554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.0015090018277987838 | Validation Loss:  0.0016188082518056035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.0015085497871041298 | Validation Loss:  0.0016185780987143517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.001508098328486085 | Validation Loss:  0.001618347829207778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.0015076471026986837 | Validation Loss:  0.0016181175597012043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.0015071965754032135 | Validation Loss:  0.0016178875230252743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.001506746280938387 | Validation Loss:  0.0016176573699340224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.0015062966849654913 | Validation Loss:  0.001617427566088736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.0015058474382385612 | Validation Loss:  0.0016171972965821624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.0015053987735882401 | Validation Loss:  0.0016169673763215542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.0015049504581838846 | Validation Loss:  0.001616737456060946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.0015045023756101727 | Validation Loss:  0.0016165075358003378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.0015040549915283918 | Validation Loss:  0.0016162777319550514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.0015036080731078982 | Validation Loss:  0.0016160478116944432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.001503161620348692 | Validation Loss:  0.0016158180078491569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.0015027154004201293 | Validation Loss:  0.0016155883204191923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.0015022697625681758 | Validation Loss:  0.0016153587494045496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.0015018245903775096 | Validation Loss:  0.0016151288291439414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.0015013798838481307 | Validation Loss:  0.0016148993745446205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.0015009354101493955 | Validation Loss:  0.001614669687114656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.0015004915185272694 | Validation Loss:  0.0016144401161000133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.0015000482089817524 | Validation Loss:  0.0016142105450853705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.001499605132266879 | Validation Loss:  0.0016139809740707278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.0014991622883826494 | Validation Loss:  0.0016137516358867288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.0014987201429903507 | Validation Loss:  0.0016135219484567642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.0014982783468440175 | Validation Loss:  0.0016132924938574433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.001497836783528328 | Validation Loss:  0.0016130631556734443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.0014973959187045693 | Validation Loss:  0.0016128338174894452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.0014969554031267762 | Validation Loss:  0.0016126044793054461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.001496515003964305 | Validation Loss:  0.001612375257536769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.0014960753032937646 | Validation Loss:  0.0016121459193527699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.0014956359518691897 | Validation Loss:  0.0016119165811687708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.0014951968332752585 | Validation Loss:  0.0016116874758154154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.0014947582967579365 | Validation Loss:  0.0016114581376314163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.00149432010948658 | Validation Loss:  0.0016112291486933827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.0014938825042918324 | Validation Loss:  0.0016110000433400273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.0014934451319277287 | Validation Loss:  0.00161077082157135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.001493008341640234 | Validation Loss:  0.0016105417162179947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.0014925715513527393 | Validation Loss:  0.0016103126108646393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.0014921353431418538 | Validation Loss:  0.0016100837383419275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.0014916997170075774 | Validation Loss:  0.0016098548658192158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.001491264090873301 | Validation Loss:  0.001609625993296504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.0014908290468156338 | Validation Loss:  0.0016093968879431486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.0014903945848345757 | Validation Loss:  0.0016091680154204369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.0014899603556841612 | Validation Loss:  0.0016089394921436906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.0014895264757797122 | Validation Loss:  0.0016087106196209788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.0014890930615365505 | Validation Loss:  0.001608481863513589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.0014886599965393543 | Validation Loss:  0.0016082533402368426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.0014882273972034454 | Validation Loss:  0.0016080247005447745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.0014877950306981802 | Validation Loss:  0.0016077959444373846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.001487363246269524 | Validation Loss:  0.0016075675375759602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.0014869315782561898 | Validation Loss:  0.0016073393635451794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.0014865004923194647 | Validation Loss:  0.0016071107238531113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.001486069755628705 | Validation Loss:  0.0016068824334070086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.0014856393681839108 | Validation Loss:  0.001606654142960906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.001485209446400404 | Validation Loss:  0.0016064258525148034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.0014847799902781844 | Validation Loss:  0.0016061979113146663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.0014843505341559649 | Validation Loss:  0.0016059697372838855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.0014839217765256763 | Validation Loss:  0.0016057415632531047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.0014834932517260313 | Validation Loss:  0.0016055135056376457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.0014830654254183173 | Validation Loss:  0.0016052856808528304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.0014826375991106033 | Validation Loss:  0.001605057972483337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.0014822105877101421 | Validation Loss:  0.0016048301476985216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.0014817836927250028 | Validation Loss:  0.0016046024393290281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.0014813571469858289 | Validation Loss:  0.0016043747309595346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.0014809311833232641 | Validation Loss:  0.0016041473718360066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.0014805053360760212 | Validation Loss:  0.0016039198962971568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.0014800800709053874 | Validation Loss:  0.0016036926535889506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.001479655271396041 | Validation Loss:  0.0016034652944654226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.0014792305883020163 | Validation Loss:  0.0016032381681725383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.0014788066036999226 | Validation Loss:  0.001603011041879654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.0014783829683437943 | Validation Loss:  0.0016027840320020914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.0014779596822336316 | Validation Loss:  0.0016025572549551725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.0014775367453694344 | Validation Loss:  0.0016023303614929318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.0014771142741665244 | Validation Loss:  0.001602103584446013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.0014766920357942581 | Validation Loss:  0.0016018773894757032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.0014762702630832791 | Validation Loss:  0.0016016506124287844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.001475849305279553 | Validation Loss:  0.0016014244174584746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.0014754284638911486 | Validation Loss:  0.001601198106072843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.0014750080881640315 | Validation Loss:  0.0016009720275178552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.0014745878288522363 | Validation Loss:  0.0016007459489628673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.001474168268032372 | Validation Loss:  0.0016005202196538448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.0014737490564584732 | Validation Loss:  0.0016002943739295006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.0014733304269611835 | Validation Loss:  0.0016000691102817655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.0014729119138792157 | Validation Loss:  0.0015998434973880649\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.0014724942157045007 | Validation Loss:  0.0015996182337403297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.0014720767503604293 | Validation Loss:  0.0015993930865079165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.0014716596342623234 | Validation Loss:  0.001599168055690825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.0014712431002408266 | Validation Loss:  0.0015989432577043772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.0014708270318806171 | Validation Loss:  0.001598718692548573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.0014704111963510513 | Validation Loss:  0.001598494010977447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.0014699959428980947 | Validation Loss:  0.0015982696786522865\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.0014695810386911035 | Validation Loss:  0.0015980456955730915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.0014691668329760432 | Validation Loss:  0.0015978215960785747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.0014687529765069485 | Validation Loss:  0.0015975977294147015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.0014683392364531755 | Validation Loss:  0.001597374095581472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.0014679264277219772 | Validation Loss:  0.0015971508109942079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.0014675138518214226 | Validation Loss:  0.0015969275264069438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.001467101857997477 | Validation Loss:  0.0015967043582350016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.0014666903298348188 | Validation Loss:  0.0015964817721396685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.0014662790345028043 | Validation Loss:  0.0015962589532136917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.0014658685540780425 | Validation Loss:  0.0015960364835336804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.0014654581900686026 | Validation Loss:  0.0015958143630996346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.001465048873797059 | Validation Loss:  0.0015955922426655889\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.0014646395575255156 | Validation Loss:  0.0015953703550621867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.0014642310561612248 | Validation Loss:  0.00159514881670475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.0014638230204582214 | Validation Loss:  0.001594927511177957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.0014634153340011835 | Validation Loss:  0.0015947066713124514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.0014630082296207547 | Validation Loss:  0.0015944857150316238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.0014626014744862914 | Validation Loss:  0.00159426499158144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.0014621955342590809 | Validation Loss:  0.0015940446173772216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.0014617899432778358 | Validation Loss:  0.0015938245924189687\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.0014613849343732 | Validation Loss:  0.0015936046838760376\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.0014609802747145295 | Validation Loss:  0.0015933853574097157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.0014605765463784337 | Validation Loss:  0.0015931660309433937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.0014601730508729815 | Validation Loss:  0.0015929470537230372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.0014597702538594604 | Validation Loss:  0.0015927283093333244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.0014593678060919046 | Validation Loss:  0.001592509914189577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.001458965940400958 | Validation Loss:  0.0015922918682917953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.0014585646567866206 | Validation Loss:  0.0015920737059786916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.0014581643044948578 | Validation Loss:  0.0015918562421575189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.0014577641850337386 | Validation Loss:  0.0015916393604129553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.0014573646476492286 | Validation Loss:  0.001591422245837748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.0014569658087566495 | Validation Loss:  0.0015912055969238281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.0014565675519406796 | Validation Loss:  0.0015909892972558737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.0014561699936166406 | Validation Loss:  0.0015907734632492065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.001455772900953889 | Validation Loss:  0.0015905576292425394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.0014553761575371027 | Validation Loss:  0.0015903424937278032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.001454980461858213 | Validation Loss:  0.001590127358213067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.0014545852318406105 | Validation Loss:  0.00158991280477494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.0014541905838996172 | Validation Loss:  0.0015896986005827785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.001453796518035233 | Validation Loss:  0.001589484978467226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.0014534031506627798 | Validation Loss:  0.0015892712399363518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.0014530104817822576 | Validation Loss:  0.0015890580834820867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.0014526183949783444 | Validation Loss:  0.0015888455091044307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.0014522270066663623 | Validation Loss:  0.0015886331675574183\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.0014518365496769547 | Validation Loss:  0.0015884211752563715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.0014514463255181909 | Validation Loss:  0.0015882094157859683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.0014510569162666798 | Validation Loss:  0.0015879978891462088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.0014506682055070996 | Validation Loss:  0.0015877872938290238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.0014502800768241286 | Validation Loss:  0.0015875769313424826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.0014498926466330886 | Validation Loss:  0.001587366801686585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.0014495061477646232 | Validation Loss:  0.0015871572541072965\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.001449120114557445 | Validation Loss:  0.0015869482886046171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.0014487350126728415 | Validation Loss:  0.0015867395559325814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.0014483504928648472 | Validation Loss:  0.0015865314053371549\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.0014479666715487838 | Validation Loss:  0.0015863231383264065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.0014475836651399732 | Validation Loss:  0.0015861161518841982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.0014472013572230935 | Validation Loss:  0.00158590916544199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.0014468197477981448 | Validation Loss:  0.001585702528245747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.0014464390696957707 | Validation Loss:  0.0015854964731261134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.001446058857254684 | Validation Loss:  0.0015852911164984107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.0014456796925514936 | Validation Loss:  0.0015850858762860298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.0014453011099249125 | Validation Loss:  0.0015848813345655799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.001444923342205584 | Validation Loss:  0.0015846772585064173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.0014445465058088303 | Validation Loss:  0.0015844734152778983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.0014441702514886856 | Validation Loss:  0.0015842703869566321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.0014437950449064374 | Validation Loss:  0.0015840678242966533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.0014434203039854765 | Validation Loss:  0.0015838656108826399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.001443046610802412 | Validation Loss:  0.0015836640959605575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.0014426738489419222 | Validation Loss:  0.0015834628138691187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.0014423015527427197 | Validation Loss:  0.0015832622302696109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.0014419303042814136 | Validation Loss:  0.0015830621123313904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.0014415598707273602 | Validation Loss:  0.0015828623436391354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.0014411902520805597 | Validation Loss:  0.0015826636226847768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.0014408215647563338 | Validation Loss:  0.0015824651345610619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.0014404536923393607 | Validation Loss:  0.0015822671120986342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.0014400867512449622 | Validation Loss:  0.0015820699045434594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.0014397206250578165 | Validation Loss:  0.0015818726969882846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.0014393554301932454 | Validation Loss:  0.0015816765371710062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.0014389909338206053 | Validation Loss:  0.001581480959430337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.0014386276016011834 | Validation Loss:  0.0015812856145203114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.0014382649678736925 | Validation Loss:  0.0015810913173481822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.001437903381884098 | Validation Loss:  0.0015808971365913749\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.0014375424943864346 | Validation Loss:  0.0015807035379111767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.0014371828874573112 | Validation Loss:  0.001580510986968875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.0014368240954354405 | Validation Loss:  0.001580318552441895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.0014364661183208227 | Validation Loss:  0.001580126816406846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.0014361091889441013 | Validation Loss:  0.0015799356624484062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.0014357530744746327 | Validation Loss:  0.0015797455562278628\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.0014353980077430606 | Validation Loss:  0.0015795555664226413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.001435043872334063 | Validation Loss:  0.0015793662751093507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.001434690784662962 | Validation Loss:  0.0015791774494573474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.0014343385118991137 | Validation Loss:  0.0015789895551279187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.0014339872868731618 | Validation Loss:  0.001578802359290421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.0014336372260004282 | Validation Loss:  0.0015786152798682451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.0014332879800349474 | Validation Loss:  0.0015784291317686439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.0014329398982226849 | Validation Loss:  0.0015782436821609735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.0014325926313176751 | Validation Loss:  0.0015780589310452342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.0014322465285658836 | Validation Loss:  0.001577874762006104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.0014319013571366668 | Validation Loss:  0.0015776908257976174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.0014315573498606682 | Validation Loss:  0.001577508170157671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.0014312142739072442 | Validation Loss:  0.0015773259801790118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.0014308721292763948 | Validation Loss:  0.00157714425586164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.0014305311487987638 | Validation Loss:  0.0015769629972055554\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.001430191216059029 | Validation Loss:  0.0015767826698720455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.0014298524474725127 | Validation Loss:  0.0015766031574457884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.0014295147266238928 | Validation Loss:  0.0015764243435114622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.0014291780535131693 | Validation Loss:  0.0015762458788231015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.0014288424281403422 | Validation Loss:  0.0015760682290419936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.0014285077340900898 | Validation Loss:  0.0015758912777528167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.0014281743206083775 | Validation Loss:  0.0015757149085402489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.0014278419548645616 | Validation Loss:  0.0015755393542349339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.0014275108696892858 | Validation Loss:  0.0015753644984215498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.0014271807158365846 | Validation Loss:  0.0015751904575154185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.0014268517261371017 | Validation Loss:  0.001575016649439931\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.0014265237841755152 | Validation Loss:  0.0015748438891023397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.001426197006367147 | Validation Loss:  0.0015746719436720014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.0014258712762966752 | Validation Loss:  0.0015745003474876285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.0014255469432100654 | Validation Loss:  0.001574329799041152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.0014252235414460301 | Validation Loss:  0.0015741594834253192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.0014249013038352132 | Validation Loss:  0.0015739903319627047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.0014245803467929363 | Validation Loss:  0.001573821878992021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.0014242605539038777 | Validation Loss:  0.0015736536588519812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.0014239418087527156 | Validation Loss:  0.0015734864864498377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.0014236243441700935 | Validation Loss:  0.001573320128954947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.0014233080437406898 | Validation Loss:  0.0015731543535366654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.0014229926746338606 | Validation Loss:  0.0015729890437796712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.0014226788189262152 | Validation Loss:  0.0015728248981758952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.0014223660109564662 | Validation Loss:  0.001572661567479372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.0014220543671399355 | Validation Loss:  0.0015724984696134925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.0014217440038919449 | Validation Loss:  0.001572335953824222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.0014214348047971725 | Validation Loss:  0.0015721747186034918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.0014211267698556185 | Validation Loss:  0.0015720142982900143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.0014208201318979263 | Validation Loss:  0.0015718542272225022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.0014205145416781306 | Validation Loss:  0.0015716943889856339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.001420210232026875 | Validation Loss:  0.001571536180563271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.0014199072029441595 | Validation Loss:  0.0015713785542175174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.0014196052215993404 | Validation Loss:  0.0015712210442870855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.0014193047536537051 | Validation Loss:  0.001571064698509872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.0014190052170306444 | Validation Loss:  0.0015709094004705548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.0014187070773914456 | Validation Loss:  0.0015707543352618814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.0014184104511514306 | Validation Loss:  0.0015706002013757825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.001418114872649312 | Validation Loss:  0.0015704466495662928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.001417820225469768 | Validation Loss:  0.0015702941454946995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.0014175274409353733 | Validation Loss:  0.0015701423399150372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.0014172354713082314 | Validation Loss:  0.0015699909999966621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.0014169450150802732 | Validation Loss:  0.001569840358570218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.0014166554901748896 | Validation Loss:  0.0015696907648816705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.0014163674786686897 | Validation Loss:  0.0015695416368544102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.00141608074773103 | Validation Loss:  0.0015693934401497245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.0014157952973619103 | Validation Loss:  0.001569245709106326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.001415511011146009 | Validation Loss:  0.0015690990258008242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.0014152281219139695 | Validation Loss:  0.0015689529245719314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.0014149465132504702 | Validation Loss:  0.001568807172589004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.001414666068740189 | Validation Loss:  0.0015686627011746168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.00141438702121377 | Validation Loss:  0.0015685189282521605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.0014141093706712127 | Validation Loss:  0.0015683758538216352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.0014138328842818737 | Validation Loss:  0.0015682332450523973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.0014135577948763967 | Validation Loss:  0.0015680913347750902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.0014132839860394597 | Validation Loss:  0.001567950821481645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.001413011341355741 | Validation Loss:  0.0015678103081882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.0014127400936558843 | Validation Loss:  0.0015676707262173295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.0014124701265245676 | Validation Loss:  0.0015675319591537118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.001412201439961791 | Validation Loss:  0.0015673937741667032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.0014119341503828764 | Validation Loss:  0.0015672564040869474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.0014116681413725019 | Validation Loss:  0.0015671198489144444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.0014114035293459892 | Validation Loss:  0.0015669839922338724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.001411139965057373 | Validation Loss:  0.0015668487176299095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.0014108779141679406 | Validation Loss:  0.0015667141415178776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.0014106171438470483 | Validation Loss:  0.0015665804967284203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.0014103577705100179 | Validation Loss:  0.0015664473176002502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.0014100996777415276 | Validation Loss:  0.001566314953379333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.0014098427491262555 | Validation Loss:  0.0015661832876503468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.001409587450325489 | Validation Loss:  0.001566052553243935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.001409333199262619 | Validation Loss:  0.0015659224009141326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.001409080228768289 | Validation Loss:  0.0015657927142456174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.0014088290045037866 | Validation Loss:  0.0015656640753149986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.0014085785951465368 | Validation Loss:  0.0015655361348763108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.0014083296991884708 | Validation Loss:  0.0015654084272682667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.0014080822002142668 | Validation Loss:  0.0015652818838134408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.0014078359818086028 | Validation Loss:  0.001565155922435224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.0014075912768021226 | Validation Loss:  0.00156503077596426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.001407347503118217 | Validation Loss:  0.0015649058623239398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.0014071052428334951 | Validation Loss:  0.001564781996421516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.0014068641467019916 | Validation Loss:  0.001564658829011023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.00140662444755435 | Validation Loss:  0.0015645362436771393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.001406386261805892 | Validation Loss:  0.0015644142404198647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.0014061492402106524 | Validation Loss:  0.0015642931684851646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.0014059134991839528 | Validation Loss:  0.001564172562211752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.0014056790387257934 | Validation Loss:  0.0015640526544302702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.0014054460916668177 | Validation Loss:  0.0015639334451407194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.0014052143087610602 | Validation Loss:  0.0015638151671737432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.001404983806423843 | Validation Loss:  0.0015636971220374107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.0014047549339011312 | Validation Loss:  0.0015635800082236528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.0014045271091163158 | Validation Loss:  0.001563463476486504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.0014043004484847188 | Validation Loss:  0.0015633475268259645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.0014040753012523055 | Validation Loss:  0.0015632322756573558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.0014038512017577887 | Validation Loss:  0.001563117839396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.0014036287320777774 | Validation Loss:  0.0015630038687959313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.0014034073101356626 | Validation Loss:  0.001562890480272472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.0014031875180080533 | Validation Loss:  0.0015627777902409434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.0014029686572030187 | Validation Loss:  0.0015626659151166677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.001402751193381846 | Validation Loss:  0.0015625542728230357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.0014025351265445352 | Validation Loss:  0.0015624439110979438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.0014023202238604426 | Validation Loss:  0.0015623338986188173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.001402106718160212 | Validation Loss:  0.0015622240025550127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.0014018943766131997 | Validation Loss:  0.0015621152706444263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.0014016834320500493 | Validation Loss:  0.0015620070043951273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.001401473768055439 | Validation Loss:  0.0015618993202224374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.001401265268214047 | Validation Loss:  0.0015617922181263566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.0014010581653565168 | Validation Loss:  0.0015616860473528504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.001400852226652205 | Validation Loss:  0.0015615802258253098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.0014006475685164332 | Validation Loss:  0.001561474520713091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.0014004441909492016 | Validation Loss:  0.0015613703290000558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.00140024209395051 | Validation Loss:  0.0015612664865329862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.0014000411611050367 | Validation Loss:  0.0015611626440659165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.0013998416252434254 | Validation Loss:  0.0015610597329214215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.0013996432535350323 | Validation Loss:  0.0015609575202688575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.0013994459295645356 | Validation Loss:  0.0015608560061082244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.0013992500025779009 | Validation Loss:  0.0015607543755322695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.00139905558899045 | Validation Loss:  0.0015606541419401765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.0013988621067255735 | Validation Loss:  0.0015605543740093708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.0013986699050292373 | Validation Loss:  0.0015604546060785651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.001398478983901441 | Validation Loss:  0.001560355769470334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.0013982889940962195 | Validation Loss:  0.0015602573985233903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.0013981005176901817 | Validation Loss:  0.0015601592604070902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.001397913321852684 | Validation Loss:  0.0015600620536133647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.001397727057337761 | Validation Loss:  0.0015599655453115702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.0013975421898066998 | Validation Loss:  0.0015598693862557411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.0013973584864288568 | Validation Loss:  0.0015597736928611994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.0013971758307889104 | Validation Loss:  0.0015596786979585886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.0013969946885481477 | Validation Loss:  0.0015595838194712996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.0013968143612146378 | Validation Loss:  0.0015594897558912635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.001396635198034346 | Validation Loss:  0.0015593962743878365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.0013964575482532382 | Validation Loss:  0.001559303142130375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.001396280829794705 | Validation Loss:  0.0015592105919495225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.0013961055083200336 | Validation Loss:  0.0015591183910146356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.0013959311181679368 | Validation Loss:  0.0015590270049870014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.0013957578921690583 | Validation Loss:  0.0015589359682053328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.00139558594673872 | Validation Loss:  0.001558845629915595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.001395415049046278 | Validation Loss:  0.0015587552916258574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.0013952451990917325 | Validation Loss:  0.0015586655354127288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.001395076629705727 | Validation Loss:  0.001558576594106853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.0013949093408882618 | Validation Loss:  0.0015584880020469427\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.001394742983393371 | Validation Loss:  0.001558399642817676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.001394577557221055 | Validation Loss:  0.0015583117492496967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.0013944135280326009 | Validation Loss:  0.0015582250198349357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.0013942504301667213 | Validation Loss:  0.0015581377083435655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.00139408849645406 | Validation Loss:  0.0015580513281747699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.0013939276104792953 | Validation Loss:  0.001557965762913227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.0013937678886577487 | Validation Loss:  0.0015578800812363625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.0013936092145740986 | Validation Loss:  0.0015577947488054633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.001393451471813023 | Validation Loss:  0.001557710231281817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.0013932951260358095 | Validation Loss:  0.0015576264122501016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.0013931395951658487 | Validation Loss:  0.0015575423603877425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.0013929851120337844 | Validation Loss:  0.0015574588906019926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.0013928319094702601 | Validation Loss:  0.0015573762357234955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.0013926796382293105 | Validation Loss:  0.001557293813675642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.0013925281818956137 | Validation Loss:  0.0015572113916277885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.001392377889715135 | Validation Loss:  0.0015571297844871879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.0013922288781031966 | Validation Loss:  0.0015570484101772308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.001392080681398511 | Validation Loss:  0.0015569675015285611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.0013919335324317217 | Validation Loss:  0.0015568870585411787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.001391787314787507 | Validation Loss:  0.00155680684838444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.0013916422612965107 | Validation Loss:  0.0015567269874736667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.001391498139128089 | Validation Loss:  0.0015566474758088589\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.0013913550646975636 | Validation Loss:  0.0015565685462206602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.001391212921589613 | Validation Loss:  0.0015564900822937489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.001391071593388915 | Validation Loss:  0.0015564116183668375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.0013909313129261136 | Validation Loss:  0.001556333969347179\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.0013907919637858868 | Validation Loss:  0.0015562563203275204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.0013906538952142 | Validation Loss:  0.0015561789041385055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.001390516641549766 | Validation Loss:  0.0015561020700260997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.001390380086377263 | Validation Loss:  0.0015560255851596594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.0013902446953579783 | Validation Loss:  0.001555949216708541\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.0013901101192459464 | Validation Loss:  0.0015558740124106407\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.001389976474456489 | Validation Loss:  0.0015557982260361314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.0013898438774049282 | Validation Loss:  0.0015557227889075875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.001389712211675942 | Validation Loss:  0.0015556480502709746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.0013895813608542085 | Validation Loss:  0.0015555733116343617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.0013894514413550496 | Validation Loss:  0.001555499155074358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.0013893224531784654 | Validation Loss:  0.001555425114929676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.001389194279909134 | Validation Loss:  0.001555351773276925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.0013890669215470552 | Validation Loss:  0.0015552780823782086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.0013889404945075512 | Validation Loss:  0.0015552050899714231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.0013888151152059436 | Validation Loss:  0.001555132563225925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.001388690434396267 | Validation Loss:  0.0015550601528957486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.001388566684909165 | Validation Loss:  0.0015549880918115377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.0013884437503293157 | Validation Loss:  0.0015549161471426487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.0013883216306567192 | Validation Loss:  0.0015548443188890815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.0013882004423066974 | Validation Loss:  0.0015547731891274452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.0013880799524486065 | Validation Loss:  0.0015547021757811308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.0013879603939130902 | Validation Loss:  0.0015546315116807818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.0013878416502848268 | Validation Loss:  0.0015545610804110765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.0013877236051484942 | Validation Loss:  0.001554490881972015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.0013876066077500582 | Validation Loss:  0.0015544204507023096\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.0013874900760129094 | Validation Loss:  0.001554350834339857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.001387374708428979 | Validation Loss:  0.001554281683638692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.0013872599229216576 | Validation Loss:  0.0015542121836915612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.0013871457194909453 | Validation Loss:  0.0015541431494057178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.0013870325637981296 | Validation Loss:  0.00155407446436584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.0013869201065972447 | Validation Loss:  0.0015540061285719275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.0013868084643036127 | Validation Loss:  0.0015539377927780151\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.0013866975205019116 | Validation Loss:  0.0015538696898147464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.0013865875080227852 | Validation Loss:  0.001553801936097443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.001386477961204946 | Validation Loss:  0.0015537341823801398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.0013863693457096815 | Validation Loss:  0.0015536668943241239\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.0013862613122910261 | Validation Loss:  0.0015535997226834297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.0013861542101949453 | Validation Loss:  0.0015535325510427356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.0013860478065907955 | Validation Loss:  0.001553465728648007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.001385941868647933 | Validation Loss:  0.0015533990226686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.0013858368620276451 | Validation Loss:  0.0015533327823504806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.0013857325538992882 | Validation Loss:  0.0015532667748630047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.0013856288278475404 | Validation Loss:  0.001553200650960207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.0013855258002877235 | Validation Loss:  0.0015531351091340184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.0013854235876351595 | Validation Loss:  0.0015530695673078299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.0013853219570592046 | Validation Loss:  0.0015530040254816413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.0013852210249751806 | Validation Loss:  0.0015529390657320619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.0013851207913830876 | Validation Loss:  0.0015528736403211951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.001385021023452282 | Validation Loss:  0.0015528089134022593\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.001384922070428729 | Validation Loss:  0.0015527443028986454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.0013848236994817853 | Validation Loss:  0.0015526796923950315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.0013847261434420943 | Validation Loss:  0.0015526153147220612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.0013846290530636907 | Validation Loss:  0.0015525514027103782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.001384532661177218 | Validation Loss:  0.0015524871414527297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.0013844368513673544 | Validation Loss:  0.0015524234622716904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.0013843417400494218 | Validation Loss:  0.0015523598995059729\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.0013842472108080983 | Validation Loss:  0.0015522959874942899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.001384153263643384 | Validation Loss:  0.0015522327739745378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.001384059782139957 | Validation Loss:  0.0015521695604547858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.0013839671155437827 | Validation Loss:  0.0015521063469350338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.0013838751474395394 | Validation Loss:  0.001552043599076569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.0013837835285812616 | Validation Loss:  0.0015519809676334262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.0013836926082149148 | Validation Loss:  0.0015519177541136742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.0013836021535098553 | Validation Loss:  0.0015518557047471404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.0013835122808814049 | Validation Loss:  0.0015517929568886757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.00138342275749892 | Validation Loss:  0.0015517306746914983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.0013833342818543315 | Validation Loss:  0.00155166897457093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.0013832461554557085 | Validation Loss:  0.001551606459543109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.0013831586111336946 | Validation Loss:  0.0015515446430072188\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.0013830714160576463 | Validation Loss:  0.0015514831757172942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.0013829850358888507 | Validation Loss:  0.0015514212427660823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.0013828990049660206 | Validation Loss:  0.0015513596590608358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.0013828135561197996 | Validation Loss:  0.0015512985410168767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.001382728572934866 | Validation Loss:  0.0015512369573116302\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.0013826441718265414 | Validation Loss:  0.0015511757228523493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.0013825602363795042 | Validation Loss:  0.0015511148376390338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.0013824771158397198 | Validation Loss:  0.0015510537195950747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.001382394228130579 | Validation Loss:  0.0015509926015511155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.0013823118060827255 | Validation Loss:  0.001550932414829731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.0013822297332808375 | Validation Loss:  0.0015508709475398064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.001382148708216846 | Validation Loss:  0.0015508106444031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.0013820675667375326 | Validation Loss:  0.0015507503412663937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.0013819871237501502 | Validation Loss:  0.0015506895724684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.0013819070300087333 | Validation Loss:  0.0015506293857470155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.0013818277511745691 | Validation Loss:  0.0015505694318562746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.0013817487051710486 | Validation Loss:  0.001550508663058281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.0013816703576594591 | Validation Loss:  0.0015504490584135056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.0013815921265631914 | Validation Loss:  0.0015503891045227647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.001381514361128211 | Validation Loss:  0.0015503288013860583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.0013814371777698398 | Validation Loss:  0.0015502694295719266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.001381360343657434 | Validation Loss:  0.001550209242850542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.0013812842080369592 | Validation Loss:  0.0015501496382057667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.001381208305247128 | Validation Loss:  0.0015500906156376004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.0013811328681185842 | Validation Loss:  0.001550030428916216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.001381057663820684 | Validation Loss:  0.001549971173517406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.0013809832744300365 | Validation Loss:  0.001549911918118596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.0013809092342853546 | Validation Loss:  0.0015498525463044643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.0013808351941406727 | Validation Loss:  0.00154979364015162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.0013807619689032435 | Validation Loss:  0.0015497341519221663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.001380688976496458 | Validation Loss:  0.001549675129354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.0013806164497509599 | Validation Loss:  0.001549615990370512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.001380544388666749 | Validation Loss:  0.001549556851387024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.0013804726768285036 | Validation Loss:  0.0015494978288188577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.0013804011978209019 | Validation Loss:  0.0015494392719119787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.0013803301844745874 | Validation Loss:  0.0015493803657591343\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.0013802595203742385 | Validation Loss:  0.001549321343190968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.0013801895547658205 | Validation Loss:  0.001549262786284089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.0013801194727420807 | Validation Loss:  0.0015492038801312447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.0013800500892102718 | Validation Loss:  0.0015491456724703312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.0013799810549244285 | Validation Loss:  0.0015490867663174868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.0013799123698845506 | Validation Loss:  0.0015490282094106078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.0013798440340906382 | Validation Loss:  0.0015489697689190507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.0013797758147120476 | Validation Loss:  0.0015489112120121717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.001379708293825388 | Validation Loss:  0.0015488530043512583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.0013796407729387283 | Validation Loss:  0.0015487945638597012\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.0013795739505439997 | Validation Loss:  0.001548736123368144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.0013795074773952365 | Validation Loss:  0.0015486779157072306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.0013794410042464733 | Validation Loss:  0.001548619708046317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.0013793751131743193 | Validation Loss:  0.00154856126755476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.0013793094549328089 | Validation Loss:  0.0015485032927244902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.0013792440295219421 | Validation Loss:  0.0015484452014788985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.0013791791861876845 | Validation Loss:  0.0015483871102333069\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.0013791146920993924 | Validation Loss:  0.0015483290189877152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.0013790503144264221 | Validation Loss:  0.0015482708113268018\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.1, 'optimizer': 'Adam'} Test Loss:  0.0011749848490580916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  0 | Train Loss:  0.025992143899202347 | Validation Loss:  0.021261710673570633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  1 | Train Loss:  0.024769198149442673 | Validation Loss:  0.020189842209219933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  2 | Train Loss:  0.023625291883945465 | Validation Loss:  0.019194098189473152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  3 | Train Loss:  0.022557444870471954 | Validation Loss:  0.018272606655955315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  4 | Train Loss:  0.021563587710261345 | Validation Loss:  0.01742403395473957\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  5 | Train Loss:  0.020642179995775223 | Validation Loss:  0.016647791489958763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  6 | Train Loss:  0.019792476668953896 | Validation Loss:  0.01594395935535431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  7 | Train Loss:  0.019014399498701096 | Validation Loss:  0.015313061885535717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  8 | Train Loss:  0.01830836944282055 | Validation Loss:  0.014756005257368088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  9 | Train Loss:  0.01767520047724247 | Validation Loss:  0.01427395548671484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  10 | Train Loss:  0.017116010189056396 | Validation Loss:  0.013868201524019241\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  11 | Train Loss:  0.016632093116641045 | Validation Loss:  0.013539894483983517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  12 | Train Loss:  0.01622469536960125 | Validation Loss:  0.013289685361087322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  13 | Train Loss:  0.01589466631412506 | Validation Loss:  0.013117177411913872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  14 | Train Loss:  0.01564197987318039 | Validation Loss:  0.013020189478993416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  15 | Train Loss:  0.015465017408132553 | Validation Loss:  0.012993797659873962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  16 | Train Loss:  0.015359695069491863 | Validation Loss:  0.013029227964580059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  17 | Train Loss:  0.015318415127694607 | Validation Loss:  0.013112894259393215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  18 | Train Loss:  0.015329115092754364 | Validation Loss:  0.013226190581917763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  19 | Train Loss:  0.015375008806586266 | Validation Loss:  0.013346905820071697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  20 | Train Loss:  0.015435759909451008 | Validation Loss:  0.013452492654323578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  21 | Train Loss:  0.015490435995161533 | Validation Loss:  0.013524052686989307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  22 | Train Loss:  0.015521184541285038 | Validation Loss:  0.013549301773309708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  23 | Train Loss:  0.015516095794737339 | Validation Loss:  0.013523636385798454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  24 | Train Loss:  0.01547025702893734 | Validation Loss:  0.013449358753859997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  25 | Train Loss:  0.015385136008262634 | Validation Loss:  0.01333383284509182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  26 | Train Loss:  0.015266826376318932 | Validation Loss:  0.013187231495976448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  27 | Train Loss:  0.01512399222701788 | Validation Loss:  0.013020449317991734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  28 | Train Loss:  0.01496586762368679 | Validation Loss:  0.012843487784266472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  29 | Train Loss:  0.01480074878782034 | Validation Loss:  0.012664378620684147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  30 | Train Loss:  0.014634991995990276 | Validation Loss:  0.012488674372434616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  31 | Train Loss:  0.014472566545009613 | Validation Loss:  0.012319386005401611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  32 | Train Loss:  0.01431502029299736 | Validation Loss:  0.012157254852354527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  33 | Train Loss:  0.01416178047657013 | Validation Loss:  0.012001201510429382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  34 | Train Loss:  0.014010603539645672 | Validation Loss:  0.011848852038383484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  35 | Train Loss:  0.013858091086149216 | Validation Loss:  0.011697058565914631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  36 | Train Loss:  0.013700196519494057 | Validation Loss:  0.011542361229658127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  37 | Train Loss:  0.013532677665352821 | Validation Loss:  0.011381374672055244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  38 | Train Loss:  0.01335146650671959 | Validation Loss:  0.011211114004254341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  39 | Train Loss:  0.013152983039617538 | Validation Loss:  0.011029215529561043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  40 | Train Loss:  0.012934370897710323 | Validation Loss:  0.01083410531282425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  41 | Train Loss:  0.012693666853010654 | Validation Loss:  0.010625077411532402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  42 | Train Loss:  0.012429908849298954 | Validation Loss:  0.010402278043329716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  43 | Train Loss:  0.012143145315349102 | Validation Loss:  0.010166575200855732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  44 | Train Loss:  0.011834359727799892 | Validation Loss:  0.009919296018779278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  45 | Train Loss:  0.011505262926220894 | Validation Loss:  0.009661801159381866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  46 | Train Loss:  0.01115791779011488 | Validation Loss:  0.00939496885985136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  47 | Train Loss:  0.010794270783662796 | Validation Loss:  0.009118686430156231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  48 | Train Loss:  0.010415634140372276 | Validation Loss:  0.008831583894789219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  49 | Train Loss:  0.010022353380918503 | Validation Loss:  0.008531076833605766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  50 | Train Loss:  0.009613755159080029 | Validation Loss:  0.008213653229176998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  51 | Train Loss:  0.009188353084027767 | Validation Loss:  0.007875338196754456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  52 | Train Loss:  0.00874420441687107 | Validation Loss:  0.007512401789426804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  53 | Train Loss:  0.008279475383460522 | Validation Loss:  0.007122298702597618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  54 | Train Loss:  0.007793198339641094 | Validation Loss:  0.006704685743898153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  55 | Train Loss:  0.007286100182682276 | Validation Loss:  0.006262208800762892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  56 | Train Loss:  0.006761243101209402 | Validation Loss:  0.005800722166895866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  57 | Train Loss:  0.006224211771041155 | Validation Loss:  0.0053287954069674015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  58 | Train Loss:  0.005682698916643858 | Validation Loss:  0.004856604617089033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  59 | Train Loss:  0.00514558469876647 | Validation Loss:  0.004394560120999813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  60 | Train Loss:  0.004621846601366997 | Validation Loss:  0.003952127881348133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  61 | Train Loss:  0.0041197254322469234 | Validation Loss:  0.003537233453243971\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  62 | Train Loss:  0.0036464936565607786 | Validation Loss:  0.0031563937664031982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  63 | Train Loss:  0.003208922455087304 | Validation Loss:  0.002815354848280549\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  64 | Train Loss:  0.002814131323248148 | Validation Loss:  0.002519716043025255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  65 | Train Loss:  0.002470178296789527 | Validation Loss:  0.002274848986417055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  66 | Train Loss:  0.0021855724044144154 | Validation Loss:  0.0020847925916314125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  67 | Train Loss:  0.001967302057892084 | Validation Loss:  0.0019506645621731877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  68 | Train Loss:  0.0018180819461122155 | Validation Loss:  0.0018701087683439255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  69 | Train Loss:  0.0017348111141473055 | Validation Loss:  0.0018387180753052235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  70 | Train Loss:  0.0017099635442718863 | Validation Loss:  0.001851635635830462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  71 | Train Loss:  0.0017344297375530005 | Validation Loss:  0.001902045332826674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  72 | Train Loss:  0.0017978043761104345 | Validation Loss:  0.0019771798979490995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  73 | Train Loss:  0.0018855120288208127 | Validation Loss:  0.002057927893474698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  74 | Train Loss:  0.0019782171584665775 | Validation Loss:  0.002125479280948639\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  75 | Train Loss:  0.0020573269575834274 | Validation Loss:  0.002168821869418025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  76 | Train Loss:  0.0021113157272338867 | Validation Loss:  0.002185633173212409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  77 | Train Loss:  0.002136215567588806 | Validation Loss:  0.0021781218238174915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  78 | Train Loss:  0.0021320946980267763 | Validation Loss:  0.002149870153516531\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  79 | Train Loss:  0.0021012136712670326 | Validation Loss:  0.0021056735422462225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  80 | Train Loss:  0.002048858907073736 | Validation Loss:  0.002051744144409895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  81 | Train Loss:  0.001983389724045992 | Validation Loss:  0.001994416117668152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  82 | Train Loss:  0.0019138101488351822 | Validation Loss:  0.0019386133644729853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  83 | Train Loss:  0.0018470732029527426 | Validation Loss:  0.001887683873064816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  84 | Train Loss:  0.0017874209443107247 | Validation Loss:  0.0018441632855683565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  85 | Train Loss:  0.0017373634036630392 | Validation Loss:  0.0018101175082847476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  86 | Train Loss:  0.0016985206166282296 | Validation Loss:  0.0017866274574771523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  87 | Train Loss:  0.0016715389210730791 | Validation Loss:  0.001773180440068245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  88 | Train Loss:  0.0016556825721636415 | Validation Loss:  0.0017678005388006568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  89 | Train Loss:  0.0016489647096022964 | Validation Loss:  0.0017679333686828613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  90 | Train Loss:  0.0016489020781591535 | Validation Loss:  0.0017713550478219986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  91 | Train Loss:  0.0016532622976228595 | Validation Loss:  0.0017765016527846456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  92 | Train Loss:  0.0016602716641500592 | Validation Loss:  0.0017822635127231479\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  93 | Train Loss:  0.0016684040892869234 | Validation Loss:  0.001787716057151556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  94 | Train Loss:  0.001676214043982327 | Validation Loss:  0.0017920760437846184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  95 | Train Loss:  0.0016824568156152964 | Validation Loss:  0.0017948095919564366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  96 | Train Loss:  0.0016863327473402023 | Validation Loss:  0.0017956671072170138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  97 | Train Loss:  0.0016875757137313485 | Validation Loss:  0.0017945774598047137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  98 | Train Loss:  0.0016863002674654126 | Validation Loss:  0.0017915199277922511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  99 | Train Loss:  0.0016827672952786088 | Validation Loss:  0.0017865252448245883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  100 | Train Loss:  0.0016772706294432282 | Validation Loss:  0.0017798043554648757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  101 | Train Loss:  0.001670184195972979 | Validation Loss:  0.0017718462040647864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  102 | Train Loss:  0.0016620338428765535 | Validation Loss:  0.001763352775014937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  103 | Train Loss:  0.001653454382903874 | Validation Loss:  0.0017550369957461953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  104 | Train Loss:  0.0016450422117486596 | Validation Loss:  0.0017474379856139421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  105 | Train Loss:  0.0016372341196984053 | Validation Loss:  0.001740868203341961\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  106 | Train Loss:  0.001630305196158588 | Validation Loss:  0.0017354702576994896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  107 | Train Loss:  0.0016244432190433145 | Validation Loss:  0.001731278607621789\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  108 | Train Loss:  0.0016197884688153863 | Validation Loss:  0.0017282261978834867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  109 | Train Loss:  0.0016163941472768784 | Validation Loss:  0.0017261363100260496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  110 | Train Loss:  0.00161416782066226 | Validation Loss:  0.0017247612122446299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  111 | Train Loss:  0.0016128845745697618 | Validation Loss:  0.0017238627187907696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  112 | Train Loss:  0.0016122706001624465 | Validation Loss:  0.0017232642276212573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  113 | Train Loss:  0.0016120776999741793 | Validation Loss:  0.0017228315118700266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  114 | Train Loss:  0.0016120898071676493 | Validation Loss:  0.0017224251059815288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  115 | Train Loss:  0.001612097374163568 | Validation Loss:  0.001721893553622067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  116 | Train Loss:  0.0016118998173624277 | Validation Loss:  0.0017211122903972864\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  117 | Train Loss:  0.0016113525489345193 | Validation Loss:  0.0017200218280777335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  118 | Train Loss:  0.0016104024834930897 | Validation Loss:  0.001718616927973926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  119 | Train Loss:  0.0016090716235339642 | Validation Loss:  0.0017169143538922071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  120 | Train Loss:  0.0016074146842584014 | Validation Loss:  0.0017149397172033787\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  121 | Train Loss:  0.0016054962761700153 | Validation Loss:  0.001712743192911148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  122 | Train Loss:  0.0016033971915021539 | Validation Loss:  0.0017104153521358967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  123 | Train Loss:  0.0016012219712138176 | Validation Loss:  0.0017080717952921987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  124 | Train Loss:  0.0015990788815543056 | Validation Loss:  0.0017058192752301693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  125 | Train Loss:  0.0015970508102327585 | Validation Loss:  0.0017037354409694672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  126 | Train Loss:  0.0015951833920553327 | Validation Loss:  0.0017018639482557774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  127 | Train Loss:  0.001593497465364635 | Validation Loss:  0.0017002170206978917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  128 | Train Loss:  0.0015920023433864117 | Validation Loss:  0.0016987728886306286\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  129 | Train Loss:  0.0015906934859231114 | Validation Loss:  0.001697476371191442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  130 | Train Loss:  0.0015895458636805415 | Validation Loss:  0.0016962579684332013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  131 | Train Loss:  0.0015885159373283386 | Validation Loss:  0.0016950606368482113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  132 | Train Loss:  0.001587557140737772 | Validation Loss:  0.00169385748449713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  133 | Train Loss:  0.001586630940437317 | Validation Loss:  0.0016926511889323592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  134 | Train Loss:  0.0015857077669352293 | Validation Loss:  0.0016914549050852656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  135 | Train Loss:  0.0015847613103687763 | Validation Loss:  0.0016902800416573882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  136 | Train Loss:  0.0015837704995647073 | Validation Loss:  0.0016891288105398417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  137 | Train Loss:  0.0015827231109142303 | Validation Loss:  0.0016879938775673509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  138 | Train Loss:  0.0015816203085705638 | Validation Loss:  0.0016868587117642164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  139 | Train Loss:  0.0015804722206667066 | Validation Loss:  0.0016857038717716932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  140 | Train Loss:  0.0015792910708114505 | Validation Loss:  0.001684516784735024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  141 | Train Loss:  0.0015780895482748747 | Validation Loss:  0.0016833001282066107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  142 | Train Loss:  0.0015768810408189893 | Validation Loss:  0.0016820752061903477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  143 | Train Loss:  0.0015756806824356318 | Validation Loss:  0.0016808718210086226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  144 | Train Loss:  0.0015745020937174559 | Validation Loss:  0.001679717330262065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  145 | Train Loss:  0.0015733527252450585 | Validation Loss:  0.0016786264022812247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  146 | Train Loss:  0.0015722360694780946 | Validation Loss:  0.0016775991534814239\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  147 | Train Loss:  0.0015711510786786675 | Validation Loss:  0.0016766227781772614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  148 | Train Loss:  0.0015700961230322719 | Validation Loss:  0.0016756750410422683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  149 | Train Loss:  0.0015690672444179654 | Validation Loss:  0.0016747318441048265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  150 | Train Loss:  0.0015680580399930477 | Validation Loss:  0.0016737750265747309\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  151 | Train Loss:  0.0015670598950237036 | Validation Loss:  0.00167279748711735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  152 | Train Loss:  0.001566066755913198 | Validation Loss:  0.001671803300268948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  153 | Train Loss:  0.0015650735003873706 | Validation Loss:  0.001670803059823811\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  154 | Train Loss:  0.0015640766359865665 | Validation Loss:  0.0016698067774996161\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  155 | Train Loss:  0.0015630738344043493 | Validation Loss:  0.0016688198084011674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  156 | Train Loss:  0.0015620639314875007 | Validation Loss:  0.0016678419196978211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  157 | Train Loss:  0.0015610478585585952 | Validation Loss:  0.001666867290623486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  158 | Train Loss:  0.001560027478262782 | Validation Loss:  0.0016658877721056342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  159 | Train Loss:  0.001559006399475038 | Validation Loss:  0.0016648974269628525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  160 | Train Loss:  0.0015579863684251904 | Validation Loss:  0.0016638950910419226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  161 | Train Loss:  0.0015569697134196758 | Validation Loss:  0.001662887865677476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  162 | Train Loss:  0.0015559587627649307 | Validation Loss:  0.0016618858790025115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  163 | Train Loss:  0.0015549551462754607 | Validation Loss:  0.001660900772549212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  164 | Train Loss:  0.0015539593296125531 | Validation Loss:  0.0016599398804828525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  165 | Train Loss:  0.0015529709635302424 | Validation Loss:  0.001659003784880042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  166 | Train Loss:  0.0015519899316132069 | Validation Loss:  0.0016580884112045169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  167 | Train Loss:  0.0015510147204622626 | Validation Loss:  0.0016571853775531054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  168 | Train Loss:  0.0015500448644161224 | Validation Loss:  0.001656287000514567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  169 | Train Loss:  0.0015490786172449589 | Validation Loss:  0.001655388856306672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  170 | Train Loss:  0.0015481146983802319 | Validation Loss:  0.001654490944929421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  171 | Train Loss:  0.0015471522929146886 | Validation Loss:  0.0016535987379029393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  172 | Train Loss:  0.0015461910516023636 | Validation Loss:  0.001652717706747353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  173 | Train Loss:  0.001545230858027935 | Validation Loss:  0.0016518530901521444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  174 | Train Loss:  0.001544271595776081 | Validation Loss:  0.0016510055866092443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  175 | Train Loss:  0.00154331314843148 | Validation Loss:  0.001650172402150929\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  176 | Train Loss:  0.0015423563309013844 | Validation Loss:  0.001649348414503038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  177 | Train Loss:  0.0015414014924317598 | Validation Loss:  0.0016485280357301235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  178 | Train Loss:  0.0015404492150992155 | Validation Loss:  0.00164770835544914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  179 | Train Loss:  0.0015394994989037514 | Validation Loss:  0.0016468892572447658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  180 | Train Loss:  0.001538553275167942 | Validation Loss:  0.001646073185838759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  181 | Train Loss:  0.001537610194645822 | Validation Loss:  0.0016452644485980272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  182 | Train Loss:  0.0015366702573373914 | Validation Loss:  0.001644465490244329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  183 | Train Loss:  0.0015357336960732937 | Validation Loss:  0.0016436767764389515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  184 | Train Loss:  0.0015348001616075635 | Validation Loss:  0.00164289609529078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  185 | Train Loss:  0.0015338690718635917 | Validation Loss:  0.001642119139432907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  186 | Train Loss:  0.0015329407760873437 | Validation Loss:  0.0016413424164056778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  187 | Train Loss:  0.0015320148086175323 | Validation Loss:  0.001640564063563943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  188 | Train Loss:  0.0015310910530388355 | Validation Loss:  0.0016397838480770588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  189 | Train Loss:  0.001530169160105288 | Validation Loss:  0.0016390044474974275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  190 | Train Loss:  0.0015292492462322116 | Validation Loss:  0.0016382281901314855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  191 | Train Loss:  0.0015283313114196062 | Validation Loss:  0.0016374577535316348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  192 | Train Loss:  0.0015274157049134374 | Validation Loss:  0.0016366944182664156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  193 | Train Loss:  0.0015265019610524178 | Validation Loss:  0.0016359369037672877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  194 | Train Loss:  0.0015255905454978347 | Validation Loss:  0.0016351828817278147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  195 | Train Loss:  0.0015246814582496881 | Validation Loss:  0.001634430605918169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  196 | Train Loss:  0.0015237746993079782 | Validation Loss:  0.0016336798435077071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  197 | Train Loss:  0.0015228703850880265 | Validation Loss:  0.0016329307109117508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  198 | Train Loss:  0.0015219685155898333 | Validation Loss:  0.0016321855364367366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  199 | Train Loss:  0.0015210692072287202 | Validation Loss:  0.001631446066312492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  200 | Train Loss:  0.0015201722271740437 | Validation Loss:  0.0016307140467688441\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  201 | Train Loss:  0.0015192775754258037 | Validation Loss:  0.0016299893613904715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  202 | Train Loss:  0.0015183856012299657 | Validation Loss:  0.001629271195270121\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  203 | Train Loss:  0.0015174956060945988 | Validation Loss:  0.0016285574529320002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  204 | Train Loss:  0.0015166079392656684 | Validation Loss:  0.0016278469702228904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  205 | Train Loss:  0.0015157226007431746 | Validation Loss:  0.001627139514312148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  206 | Train Loss:  0.0015148395905271173 | Validation Loss:  0.0016264352016150951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  207 | Train Loss:  0.001513958559371531 | Validation Loss:  0.0016257351962849498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  208 | Train Loss:  0.001513080089353025 | Validation Loss:  0.0016250411281362176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  209 | Train Loss:  0.0015122038312256336 | Validation Loss:  0.001624352764338255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  210 | Train Loss:  0.0015113299014046788 | Validation Loss:  0.00162366998847574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  211 | Train Loss:  0.001510457950644195 | Validation Loss:  0.0016229917528107762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  212 | Train Loss:  0.0015095887938514352 | Validation Loss:  0.0016223166603595018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  213 | Train Loss:  0.00150872184894979 | Validation Loss:  0.0016216440126299858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  214 | Train Loss:  0.0015078573487699032 | Validation Loss:  0.0016209733439609408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  215 | Train Loss:  0.0015069948276504874 | Validation Loss:  0.0016203055856749415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  216 | Train Loss:  0.0015061351004987955 | Validation Loss:  0.0016196410870179534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  217 | Train Loss:  0.0015052777016535401 | Validation Loss:  0.0016189808957278728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  218 | Train Loss:  0.0015044225146993995 | Validation Loss:  0.0016183246625587344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  219 | Train Loss:  0.0015035696560516953 | Validation Loss:  0.0016176721546798944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  220 | Train Loss:  0.001502719591371715 | Validation Loss:  0.0016170223243534565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  221 | Train Loss:  0.0015018712729215622 | Validation Loss:  0.0016163747059181333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  222 | Train Loss:  0.0015010256320238113 | Validation Loss:  0.001615729066543281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  223 | Train Loss:  0.001500182319432497 | Validation Loss:  0.0016150856390595436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  224 | Train Loss:  0.0014993413351476192 | Validation Loss:  0.0016144452383741736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  225 | Train Loss:  0.0014985030284151435 | Validation Loss:  0.0016138083301484585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  226 | Train Loss:  0.0014976668171584606 | Validation Loss:  0.0016131751472130418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  227 | Train Loss:  0.0014968330506235361 | Validation Loss:  0.0016125459223985672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  228 | Train Loss:  0.0014960020780563354 | Validation Loss:  0.001611919840797782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  229 | Train Loss:  0.0014951730845496058 | Validation Loss:  0.0016112967859953642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  230 | Train Loss:  0.0014943466521799564 | Validation Loss:  0.0016106765251606703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  231 | Train Loss:  0.0014935227809473872 | Validation Loss:  0.0016100590582937002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  232 | Train Loss:  0.0014927011216059327 | Validation Loss:  0.0016094447346404195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  233 | Train Loss:  0.0014918821398168802 | Validation Loss:  0.0016088341362774372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  234 | Train Loss:  0.0014910658355802298 | Validation Loss:  0.0016082272632047534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  235 | Train Loss:  0.0014902515104040504 | Validation Loss:  0.0016076242318376899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  236 | Train Loss:  0.0014894399791955948 | Validation Loss:  0.0016070250421762466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  237 | Train Loss:  0.001488630659878254 | Validation Loss:  0.001606428879313171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  238 | Train Loss:  0.0014878239016979933 | Validation Loss:  0.001605835510417819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  239 | Train Loss:  0.0014870199374854565 | Validation Loss:  0.001605244935490191\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  240 | Train Loss:  0.001486218417994678 | Validation Loss:  0.001604657736606896\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  241 | Train Loss:  0.0014854189939796925 | Validation Loss:  0.0016040736809372902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  242 | Train Loss:  0.001484622247517109 | Validation Loss:  0.0016034931177273393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  243 | Train Loss:  0.0014838282950222492 | Validation Loss:  0.001602916163392365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  244 | Train Loss:  0.001483036787249148 | Validation Loss:  0.0016023422358557582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  245 | Train Loss:  0.001482247607782483 | Validation Loss:  0.0016017711022868752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  246 | Train Loss:  0.0014814611058682203 | Validation Loss:  0.0016012029955163598\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  247 | Train Loss:  0.0014806772815063596 | Validation Loss:  0.0016006375662982464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  248 | Train Loss:  0.0014798957854509354 | Validation Loss:  0.001600074814632535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  249 | Train Loss:  0.0014791171997785568 | Validation Loss:  0.0015995148569345474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  250 | Train Loss:  0.0014783409424126148 | Validation Loss:  0.0015989582752808928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  251 | Train Loss:  0.001477567246183753 | Validation Loss:  0.0015984048368409276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  252 | Train Loss:  0.0014767962275072932 | Validation Loss:  0.0015978545416146517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  253 | Train Loss:  0.0014760277699679136 | Validation Loss:  0.0015973070403560996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  254 | Train Loss:  0.001475261989980936 | Validation Loss:  0.0015967624494805932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  255 | Train Loss:  0.0014744988875463605 | Validation Loss:  0.001596220419742167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  256 | Train Loss:  0.0014737383462488651 | Validation Loss:  0.0015956814168021083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  257 | Train Loss:  0.0014729804825037718 | Validation Loss:  0.0015951453242450953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  258 | Train Loss:  0.0014722250634804368 | Validation Loss:  0.001594612724147737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  259 | Train Loss:  0.0014714726712554693 | Validation Loss:  0.0015940829180181026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  260 | Train Loss:  0.0014707227237522602 | Validation Loss:  0.0015935564879328012\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  261 | Train Loss:  0.0014699753373861313 | Validation Loss:  0.0015930328518152237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  262 | Train Loss:  0.0014692307449877262 | Validation Loss:  0.0015925124753266573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  263 | Train Loss:  0.0014684888301417232 | Validation Loss:  0.0015919948928058147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  264 | Train Loss:  0.0014677499420940876 | Validation Loss:  0.0015914804534986615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  265 | Train Loss:  0.0014670133823528886 | Validation Loss:  0.0015909692738205194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  266 | Train Loss:  0.0014662796165794134 | Validation Loss:  0.0015904611209407449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  267 | Train Loss:  0.0014655485283583403 | Validation Loss:  0.0015899559948593378\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  268 | Train Loss:  0.0014648201176896691 | Validation Loss:  0.0015894542448222637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  269 | Train Loss:  0.0014640946174040437 | Validation Loss:  0.0015889557544142008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  270 | Train Loss:  0.0014633717946708202 | Validation Loss:  0.0015884600579738617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  271 | Train Loss:  0.0014626518823206425 | Validation Loss:  0.00158796738833189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  272 | Train Loss:  0.001461934414692223 | Validation Loss:  0.0015874778619036078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  273 | Train Loss:  0.001461220090277493 | Validation Loss:  0.0015869912458583713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  274 | Train Loss:  0.0014605083269998431 | Validation Loss:  0.001586507773026824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  275 | Train Loss:  0.001459799474105239 | Validation Loss:  0.0015860276762396097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  276 | Train Loss:  0.0014590932987630367 | Validation Loss:  0.0015855504898354411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  277 | Train Loss:  0.0014583900338038802 | Validation Loss:  0.0015850764466449618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  278 | Train Loss:  0.0014576895628124475 | Validation Loss:  0.00158460543025285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  279 | Train Loss:  0.001456991769373417 | Validation Loss:  0.001584137324243784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  280 | Train Loss:  0.0014562972355633974 | Validation Loss:  0.0015836723614484072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  281 | Train Loss:  0.0014556051464751363 | Validation Loss:  0.001583210309036076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  282 | Train Loss:  0.0014549160841852427 | Validation Loss:  0.0015827517490833998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  283 | Train Loss:  0.0014542299322783947 | Validation Loss:  0.001582296215929091\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  284 | Train Loss:  0.0014535465743392706 | Validation Loss:  0.0015818437095731497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  285 | Train Loss:  0.0014528661267831922 | Validation Loss:  0.0015813943464308977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  286 | Train Loss:  0.0014521885896101594 | Validation Loss:  0.001580948126502335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  287 | Train Loss:  0.0014515140792354941 | Validation Loss:  0.00158050493337214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  288 | Train Loss:  0.001450842246413231 | Validation Loss:  0.001580064999870956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  289 | Train Loss:  0.0014501733239740133 | Validation Loss:  0.0015796282095834613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  290 | Train Loss:  0.0014495074283331633 | Validation Loss:  0.0015791946789249778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  291 | Train Loss:  0.0014488446759060025 | Validation Loss:  0.0015787644078955054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  292 | Train Loss:  0.001448184484615922 | Validation Loss:  0.0015783371636644006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  293 | Train Loss:  0.0014475275529548526 | Validation Loss:  0.0015779132954776287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  294 | Train Loss:  0.0014468736480921507 | Validation Loss:  0.0015774924540892243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  295 | Train Loss:  0.0014462225371971726 | Validation Loss:  0.0015770748723298311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  296 | Train Loss:  0.001445574569515884 | Validation Loss:  0.001576660550199449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  297 | Train Loss:  0.001444929395802319 | Validation Loss:  0.0015762492548674345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  298 | Train Loss:  0.0014442873653024435 | Validation Loss:  0.0015758415684103966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  299 | Train Loss:  0.0014436482451856136 | Validation Loss:  0.0015754367923364043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  300 | Train Loss:  0.001443012268282473 | Validation Loss:  0.001575035392306745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  301 | Train Loss:  0.0014423794345930219 | Validation Loss:  0.001574637251906097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  302 | Train Loss:  0.0014417493948712945 | Validation Loss:  0.00157424237113446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  303 | Train Loss:  0.0014411226147785783 | Validation Loss:  0.0015738505171611905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  304 | Train Loss:  0.0014404987450689077 | Validation Loss:  0.0015734619228169322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  305 | Train Loss:  0.0014398780185729265 | Validation Loss:  0.0015730768209323287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  306 | Train Loss:  0.0014392604352906346 | Validation Loss:  0.0015726948622614145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  307 | Train Loss:  0.0014386457623913884 | Validation Loss:  0.0015723160468041897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  308 | Train Loss:  0.0014380341162905097 | Validation Loss:  0.0015719407238066196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  309 | Train Loss:  0.0014374259626492858 | Validation Loss:  0.0015715686604380608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  310 | Train Loss:  0.001436820486560464 | Validation Loss:  0.0015711995074525476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  311 | Train Loss:  0.001436218386515975 | Validation Loss:  0.0015708340797573328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  312 | Train Loss:  0.001435619778931141 | Validation Loss:  0.0015704715624451637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  313 | Train Loss:  0.0014350236160680652 | Validation Loss:  0.0015701125375926495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  314 | Train Loss:  0.001434431062079966 | Validation Loss:  0.00156975700519979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  315 | Train Loss:  0.0014338414184749126 | Validation Loss:  0.001569404499605298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  316 | Train Loss:  0.0014332550344988704 | Validation Loss:  0.0015690551372244954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  317 | Train Loss:  0.0014326717937365174 | Validation Loss:  0.001568709616549313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  318 | Train Loss:  0.0014320919290184975 | Validation Loss:  0.0015683668898418546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  319 | Train Loss:  0.0014315149746835232 | Validation Loss:  0.0015680277720093727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  320 | Train Loss:  0.00143094127997756 | Validation Loss:  0.0015676920302212238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  321 | Train Loss:  0.0014303709613159299 | Validation Loss:  0.0015673594316467643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  322 | Train Loss:  0.0014298036694526672 | Validation Loss:  0.0015670305583626032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  323 | Train Loss:  0.0014292396372184157 | Validation Loss:  0.0015667045954614878\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  324 | Train Loss:  0.0014286788646131754 | Validation Loss:  0.001566382241435349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  325 | Train Loss:  0.001428121468052268 | Validation Loss:  0.0015660629142075777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  326 | Train Loss:  0.0014275670982897282 | Validation Loss:  0.001565747195854783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  327 | Train Loss:  0.0014270161045715213 | Validation Loss:  0.0015654346207156777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  328 | Train Loss:  0.0014264684868976474 | Validation Loss:  0.0015651255380362272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  329 | Train Loss:  0.001425923896022141 | Validation Loss:  0.0015648198314011097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  330 | Train Loss:  0.0014253825647756457 | Validation Loss:  0.0015645172679796815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  331 | Train Loss:  0.0014248448424041271 | Validation Loss:  0.00156421831343323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  332 | Train Loss:  0.0014243100304156542 | Validation Loss:  0.0015639225021004677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  333 | Train Loss:  0.001423778710886836 | Validation Loss:  0.0015636300668120384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  334 | Train Loss:  0.0014232504181563854 | Validation Loss:  0.0015633410075679421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  335 | Train Loss:  0.0014227258507162333 | Validation Loss:  0.0015630555571988225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  336 | Train Loss:  0.0014222043100744486 | Validation Loss:  0.0015627731336280704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  337 | Train Loss:  0.0014216863783076406 | Validation Loss:  0.0015624939696863294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  338 | Train Loss:  0.0014211714733392 | Validation Loss:  0.0015622185310348868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  339 | Train Loss:  0.0014206599444150925 | Validation Loss:  0.0015619461191818118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  340 | Train Loss:  0.001420151791535318 | Validation Loss:  0.0015616770833730698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  341 | Train Loss:  0.0014196470146998763 | Validation Loss:  0.0015614115400239825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  342 | Train Loss:  0.001419145381078124 | Validation Loss:  0.0015611492563039064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  343 | Train Loss:  0.0014186473563313484 | Validation Loss:  0.0015608903486281633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  344 | Train Loss:  0.001418152591213584 | Validation Loss:  0.0015606348169967532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  345 | Train Loss:  0.0014176612021401525 | Validation Loss:  0.0015603824285790324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  346 | Train Loss:  0.0014171730726957321 | Validation Loss:  0.0015601335326209664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  347 | Train Loss:  0.0014166883192956448 | Validation Loss:  0.0015598881291225553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  348 | Train Loss:  0.0014162072911858559 | Validation Loss:  0.0015596458688378334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  349 | Train Loss:  0.0014157292898744345 | Validation Loss:  0.0015594067517668009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  350 | Train Loss:  0.001415254664607346 | Validation Loss:  0.001559171243570745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  351 | Train Loss:  0.0014147835317999125 | Validation Loss:  0.0015589388785883784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  352 | Train Loss:  0.0014143157750368118 | Validation Loss:  0.0015587098896503448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  353 | Train Loss:  0.0014138512779027224 | Validation Loss:  0.0015584841603413224\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  354 | Train Loss:  0.0014133902732282877 | Validation Loss:  0.0015582619234919548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  355 | Train Loss:  0.0014129325281828642 | Validation Loss:  0.0015580427134409547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  356 | Train Loss:  0.0014124783920124173 | Validation Loss:  0.0015578268794342875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  357 | Train Loss:  0.001412027282640338 | Validation Loss:  0.0015576143050566316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  358 | Train Loss:  0.001411579898558557 | Validation Loss:  0.0015574051067233086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  359 | Train Loss:  0.001411135890521109 | Validation Loss:  0.0015571991680189967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  360 | Train Loss:  0.0014106952585279942 | Validation Loss:  0.001556996488943696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  361 | Train Loss:  0.0014102580025792122 | Validation Loss:  0.0015567970694974065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  362 | Train Loss:  0.0014098241226747632 | Validation Loss:  0.00155660102609545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  363 | Train Loss:  0.0014093936188146472 | Validation Loss:  0.0015564077766612172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  364 | Train Loss:  0.0014089664909988642 | Validation Loss:  0.0015562180196866393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  365 | Train Loss:  0.001408542855642736 | Validation Loss:  0.001556031289510429\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  366 | Train Loss:  0.0014081225963309407 | Validation Loss:  0.0015558480517938733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  367 | Train Loss:  0.0014077057130634785 | Validation Loss:  0.0015556678408756852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  368 | Train Loss:  0.0014072922058403492 | Validation Loss:  0.0015554908895865083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  369 | Train Loss:  0.0014068823074921966 | Validation Loss:  0.0015553170815110207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  370 | Train Loss:  0.0014064754359424114 | Validation Loss:  0.0015551463002339005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  371 | Train Loss:  0.001406072173267603 | Validation Loss:  0.0015549788950011134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  372 | Train Loss:  0.0014056721702218056 | Validation Loss:  0.0015548145165666938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  373 | Train Loss:  0.0014052757760509849 | Validation Loss:  0.0015546533977612853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  374 | Train Loss:  0.0014048826415091753 | Validation Loss:  0.0015544951893389225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  375 | Train Loss:  0.001404492650181055 | Validation Loss:  0.0015543402405455709\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  376 | Train Loss:  0.0014041063841432333 | Validation Loss:  0.0015541882021352649\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  377 | Train Loss:  0.0014037233777344227 | Validation Loss:  0.00155403942335397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  378 | Train Loss:  0.001403343747369945 | Validation Loss:  0.001553893554955721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  379 | Train Loss:  0.0014029676094651222 | Validation Loss:  0.0015537507133558393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  380 | Train Loss:  0.001402594498358667 | Validation Loss:  0.0015536112478002906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  381 | Train Loss:  0.0014022249961271882 | Validation Loss:  0.001553474459797144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  382 | Train Loss:  0.0014018587535247207 | Validation Loss:  0.0015533406985923648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  383 | Train Loss:  0.0014014958869665861 | Validation Loss:  0.001553210080601275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  384 | Train Loss:  0.0014011363964527845 | Validation Loss:  0.0015530823729932308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  385 | Train Loss:  0.0014007801655679941 | Validation Loss:  0.0015529574593529105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  386 | Train Loss:  0.0014004271943122149 | Validation Loss:  0.0015528354560956359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  387 | Train Loss:  0.0014000777155160904 | Validation Loss:  0.0015527164796367288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  388 | Train Loss:  0.0013997313799336553 | Validation Loss:  0.0015526005299761891\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  389 | Train Loss:  0.001399388536810875 | Validation Loss:  0.0015524872578680515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  390 | Train Loss:  0.0013990489533171058 | Validation Loss:  0.0015523770125582814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  391 | Train Loss:  0.001398712513037026 | Validation Loss:  0.0015522695612162352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  392 | Train Loss:  0.001398379448801279 | Validation Loss:  0.001552164671011269\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  393 | Train Loss:  0.0013980495277792215 | Validation Loss:  0.0015520629240199924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  394 | Train Loss:  0.0013977232156321406 | Validation Loss:  0.0015519637381657958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  395 | Train Loss:  0.0013973999302834272 | Validation Loss:  0.0015518675791099668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  396 | Train Loss:  0.001397079904563725 | Validation Loss:  0.0015517737483605742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  397 | Train Loss:  0.001396763022057712 | Validation Loss:  0.001551683060824871\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  398 | Train Loss:  0.0013964492827653885 | Validation Loss:  0.0015515948180109262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  399 | Train Loss:  0.0013961390359327197 | Validation Loss:  0.0015515092527493834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  400 | Train Loss:  0.0013958319323137403 | Validation Loss:  0.0015514264814555645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  401 | Train Loss:  0.0013955278554931283 | Validation Loss:  0.0015513462712988257\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  402 | Train Loss:  0.0013952271547168493 | Validation Loss:  0.0015512683894485235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  403 | Train Loss:  0.001394929364323616 | Validation Loss:  0.0015511933015659451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  404 | Train Loss:  0.001394634717144072 | Validation Loss:  0.0015511208912357688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  405 | Train Loss:  0.0013943436788395047 | Validation Loss:  0.001551050809212029\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  406 | Train Loss:  0.0013940554345026612 | Validation Loss:  0.0015509834047406912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  407 | Train Loss:  0.0013937702169641852 | Validation Loss:  0.00155091832857579\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  408 | Train Loss:  0.0013934882590547204 | Validation Loss:  0.0015508558135479689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  409 | Train Loss:  0.001393209327943623 | Validation Loss:  0.0015507957432419062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  410 | Train Loss:  0.001392933540046215 | Validation Loss:  0.0015507381176576018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  411 | Train Loss:  0.0013926608953624964 | Validation Loss:  0.0015506827039644122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  412 | Train Loss:  0.0013923909282311797 | Validation Loss:  0.001550629734992981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  413 | Train Loss:  0.0013921242207288742 | Validation Loss:  0.0015505790943279862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  414 | Train Loss:  0.001391860656440258 | Validation Loss:  0.001550530781969428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  415 | Train Loss:  0.0013915998861193657 | Validation Loss:  0.0015504847979173064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  416 | Train Loss:  0.0013913421425968409 | Validation Loss:  0.0015504407929256558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  417 | Train Loss:  0.0013910874258726835 | Validation Loss:  0.0015503992326557636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  418 | Train Loss:  0.0013908356195315719 | Validation Loss:  0.0015503598842769861\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  419 | Train Loss:  0.0013905868399888277 | Validation Loss:  0.0015503225149586797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  420 | Train Loss:  0.0013903408544138074 | Validation Loss:  0.0015502874739468098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  421 | Train Loss:  0.0013900977792218328 | Validation Loss:  0.001550254295580089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  422 | Train Loss:  0.0013898576144129038 | Validation Loss:  0.0015502233291044831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  423 | Train Loss:  0.0013896202435716987 | Validation Loss:  0.0015501943416893482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  424 | Train Loss:  0.001389385899528861 | Validation Loss:  0.0015501676825806499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  425 | Train Loss:  0.0013891542330384254 | Validation Loss:  0.001550142653286457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  426 | Train Loss:  0.0013889253605157137 | Validation Loss:  0.0015501196030527353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  427 | Train Loss:  0.0013886995147913694 | Validation Loss:  0.0015500985318794847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  428 | Train Loss:  0.0013884762302041054 | Validation Loss:  0.0015500792069360614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  429 | Train Loss:  0.0013882556231692433 | Validation Loss:  0.0015500620938837528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  430 | Train Loss:  0.0013880378101021051 | Validation Loss:  0.0015500468434765935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  431 | Train Loss:  0.0013878227910026908 | Validation Loss:  0.0015500332228839397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  432 | Train Loss:  0.0013876103330403566 | Validation Loss:  0.0015500212321057916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  433 | Train Loss:  0.0013874006690457463 | Validation Loss:  0.0015500111039727926\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  434 | Train Loss:  0.0013871934497728944 | Validation Loss:  0.0015500024892389774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  435 | Train Loss:  0.0013869891408830881 | Validation Loss:  0.0015499959699809551\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  436 | Train Loss:  0.001386787393130362 | Validation Loss:  0.001549990731291473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  437 | Train Loss:  0.0013865880900993943 | Validation Loss:  0.001549987238831818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  438 | Train Loss:  0.0013863916974514723 | Validation Loss:  0.001549985259771347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  439 | Train Loss:  0.0013861972838640213 | Validation Loss:  0.0015499847941100597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  440 | Train Loss:  0.001386005780659616 | Validation Loss:  0.0015499861910939217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  441 | Train Loss:  0.001385816722176969 | Validation Loss:  0.0015499887522310019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  442 | Train Loss:  0.0013856303412467241 | Validation Loss:  0.0015499928267672658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  443 | Train Loss:  0.001385446055792272 | Validation Loss:  0.0015499982982873917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  444 | Train Loss:  0.001385264447890222 | Validation Loss:  0.001550005399622023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  445 | Train Loss:  0.0013850850518792868 | Validation Loss:  0.0015500136651098728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  446 | Train Loss:  0.0013849082170054317 | Validation Loss:  0.0015500232111662626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  447 | Train Loss:  0.001384733710438013 | Validation Loss:  0.0015500342706218362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  448 | Train Loss:  0.0013845617650076747 | Validation Loss:  0.0015500462613999844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  449 | Train Loss:  0.0013843917986378074 | Validation Loss:  0.0015500598819926381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  450 | Train Loss:  0.0013842243934050202 | Validation Loss:  0.0015500744339078665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  451 | Train Loss:  0.0013840589672327042 | Validation Loss:  0.0015500900335609913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  452 | Train Loss:  0.00138389621861279 | Validation Loss:  0.001550107030197978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  453 | Train Loss:  0.0013837353326380253 | Validation Loss:  0.0015501250745728612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  454 | Train Loss:  0.0013835768913850188 | Validation Loss:  0.0015501443995162845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  455 | Train Loss:  0.0013834203127771616 | Validation Loss:  0.0015501643065363169\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  456 | Train Loss:  0.0013832664117217064 | Validation Loss:  0.0015501853777095675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  457 | Train Loss:  0.0013831142568960786 | Validation Loss:  0.0015502076130360365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  458 | Train Loss:  0.0013829644303768873 | Validation Loss:  0.0015502305468544364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  459 | Train Loss:  0.0013828165829181671 | Validation Loss:  0.0015502546448260546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  460 | Train Loss:  0.0013826708309352398 | Validation Loss:  0.0015502793248742819\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  461 | Train Loss:  0.0013825271744281054 | Validation Loss:  0.0015503049362450838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  462 | Train Loss:  0.0013823856133967638 | Validation Loss:  0.0015503315953537822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  463 | Train Loss:  0.0013822459150105715 | Validation Loss:  0.0015503588365390897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  464 | Train Loss:  0.0013821081956848502 | Validation Loss:  0.0015503872418776155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  465 | Train Loss:  0.0013819725718349218 | Validation Loss:  0.0015504161128774285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  466 | Train Loss:  0.001381838577799499 | Validation Loss:  0.001550445449538529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  467 | Train Loss:  0.001381706795655191 | Validation Loss:  0.001550475717522204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  468 | Train Loss:  0.0013815767597407103 | Validation Loss:  0.001550506567582488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  469 | Train Loss:  0.0013814487028867006 | Validation Loss:  0.0015505379997193813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  470 | Train Loss:  0.0013813225086778402 | Validation Loss:  0.0015505700139328837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  471 | Train Loss:  0.0013811978278681636 | Validation Loss:  0.0015506024938076735\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  472 | Train Loss:  0.0013810753589496017 | Validation Loss:  0.0015506356721743941\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  473 | Train Loss:  0.0013809542870149016 | Validation Loss:  0.0015506693162024021\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  474 | Train Loss:  0.0013808351941406727 | Validation Loss:  0.0015507033094763756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  475 | Train Loss:  0.001380717963911593 | Validation Loss:  0.0015507377684116364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  476 | Train Loss:  0.0013806021306663752 | Validation Loss:  0.0015507728094235063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  477 | Train Loss:  0.0013804882764816284 | Validation Loss:  0.0015508080832660198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  478 | Train Loss:  0.0013803757028654218 | Validation Loss:  0.001550843589939177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  479 | Train Loss:  0.0013802649918943644 | Validation Loss:  0.001550879329442978\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  480 | Train Loss:  0.0013801557943224907 | Validation Loss:  0.0015509157674387097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  481 | Train Loss:  0.0013800481101498008 | Validation Loss:  0.0015509523218497634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  482 | Train Loss:  0.00137994228862226 | Validation Loss:  0.0015509889926761389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  483 | Train Loss:  0.0013798377476632595 | Validation Loss:  0.001551025896333158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  484 | Train Loss:  0.0013797349529340863 | Validation Loss:  0.0015510631492361426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  485 | Train Loss:  0.001379633555188775 | Validation Loss:  0.001551100634969771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  486 | Train Loss:  0.0013795335544273257 | Validation Loss:  0.0015511378878727555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  487 | Train Loss:  0.0013794350670650601 | Validation Loss:  0.0015511754900217056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  488 | Train Loss:  0.0013793379766866565 | Validation Loss:  0.0015512133250012994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  489 | Train Loss:  0.0013792422832921147 | Validation Loss:  0.0015512510435655713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  490 | Train Loss:  0.0013791481032967567 | Validation Loss:  0.001551288878545165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  491 | Train Loss:  0.0013790552038699389 | Validation Loss:  0.0015513264806941152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  492 | Train Loss:  0.0013789637014269829 | Validation Loss:  0.0015513644320890307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  493 | Train Loss:  0.001378873479552567 | Validation Loss:  0.0015514020342379808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  494 | Train Loss:  0.0013787845382466912 | Validation Loss:  0.0015514398692175746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  495 | Train Loss:  0.0013786969939246774 | Validation Loss:  0.0015514774713665247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  496 | Train Loss:  0.0013786106137558818 | Validation Loss:  0.0015515150735154748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  497 | Train Loss:  0.0013785256305709481 | Validation Loss:  0.001551552559249103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  498 | Train Loss:  0.001378441578708589 | Validation Loss:  0.0015515899285674095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  499 | Train Loss:  0.0013783592730760574 | Validation Loss:  0.0015516269486397505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  500 | Train Loss:  0.0013782776659354568 | Validation Loss:  0.0015516638522967696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  501 | Train Loss:  0.0013781973393633962 | Validation Loss:  0.0015517004067078233\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  502 | Train Loss:  0.0013781182933598757 | Validation Loss:  0.001551736961118877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  503 | Train Loss:  0.0013780401786789298 | Validation Loss:  0.0015517736319452524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  504 | Train Loss:  0.0013779632281512022 | Validation Loss:  0.0015518094878643751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  505 | Train Loss:  0.0013778875581920147 | Validation Loss:  0.001551845227368176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  506 | Train Loss:  0.00137781270314008 | Validation Loss:  0.0015518807340413332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  507 | Train Loss:  0.0013777390122413635 | Validation Loss:  0.0015519160078838468\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  508 | Train Loss:  0.0013776663690805435 | Validation Loss:  0.0015519506996497512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  509 | Train Loss:  0.0013775948900729418 | Validation Loss:  0.0015519853914156556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  510 | Train Loss:  0.0013775242259725928 | Validation Loss:  0.001552019384689629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  511 | Train Loss:  0.0013774544931948185 | Validation Loss:  0.001552053028717637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  512 | Train Loss:  0.0013773858081549406 | Validation Loss:  0.0015520864399150014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  513 | Train Loss:  0.0013773181708529592 | Validation Loss:  0.0015521193854510784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  514 | Train Loss:  0.0013772513484582305 | Validation Loss:  0.00155215198174119\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  515 | Train Loss:  0.0013771854573860765 | Validation Loss:  0.0015521839959546924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  516 | Train Loss:  0.0013771203812211752 | Validation Loss:  0.0015522154280915856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  517 | Train Loss:  0.0013770563527941704 | Validation Loss:  0.0015522466273978353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  518 | Train Loss:  0.0013769931392744184 | Validation Loss:  0.0015522773610427976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  519 | Train Loss:  0.001376930857077241 | Validation Loss:  0.0015523075126111507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  520 | Train Loss:  0.0013768692733719945 | Validation Loss:  0.0015523373149335384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  521 | Train Loss:  0.0013768086209893227 | Validation Loss:  0.0015523660695180297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  522 | Train Loss:  0.00137674855068326 | Validation Loss:  0.0015523949405178428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  523 | Train Loss:  0.0013766894116997719 | Validation Loss:  0.0015524228801950812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  524 | Train Loss:  0.0013766312040388584 | Validation Loss:  0.0015524503542110324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  525 | Train Loss:  0.0013765734620392323 | Validation Loss:  0.0015524773625656962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  526 | Train Loss:  0.0013765166513621807 | Validation Loss:  0.0015525034395977855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  527 | Train Loss:  0.001376460655592382 | Validation Loss:  0.0015525291673839092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  528 | Train Loss:  0.0013764051254838705 | Validation Loss:  0.0015525543130934238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  529 | Train Loss:  0.0013763506431132555 | Validation Loss:  0.0015525789931416512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  530 | Train Loss:  0.0013762963935732841 | Validation Loss:  0.0015526027418673038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  531 | Train Loss:  0.0013762431917712092 | Validation Loss:  0.001552626141346991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  532 | Train Loss:  0.0013761904556304216 | Validation Loss:  0.0015526487259194255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  533 | Train Loss:  0.0013761386508122087 | Validation Loss:  0.0015526707284152508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  534 | Train Loss:  0.0013760871952399611 | Validation Loss:  0.001552692032419145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  535 | Train Loss:  0.0013760364381596446 | Validation Loss:  0.0015527126379311085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  536 | Train Loss:  0.0013759864959865808 | Validation Loss:  0.0015527325449511409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  537 | Train Loss:  0.0013759367866441607 | Validation Loss:  0.0015527518698945642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  538 | Train Loss:  0.0013758880086243153 | Validation Loss:  0.0015527706127613783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  539 | Train Loss:  0.001375839696265757 | Validation Loss:  0.0015527886571362615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  540 | Train Loss:  0.0013757917331531644 | Validation Loss:  0.00155280577018857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  541 | Train Loss:  0.0013757444685325027 | Validation Loss:  0.0015528223011642694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  542 | Train Loss:  0.00137569778598845 | Validation Loss:  0.001552838017232716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  543 | Train Loss:  0.0013756518019363284 | Validation Loss:  0.0015528531512245536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  544 | Train Loss:  0.0013756061671301723 | Validation Loss:  0.0015528675867244601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  545 | Train Loss:  0.0013755609979853034 | Validation Loss:  0.0015528809744864702\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  546 | Train Loss:  0.0013755164109170437 | Validation Loss:  0.0015528942458331585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  547 | Train Loss:  0.0013754722895100713 | Validation Loss:  0.0015529064694419503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  548 | Train Loss:  0.0013754285173490644 | Validation Loss:  0.0015529177617281675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  549 | Train Loss:  0.0013753854436799884 | Validation Loss:  0.0015529284719377756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  550 | Train Loss:  0.001375342719256878 | Validation Loss:  0.001552938367240131\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  551 | Train Loss:  0.0013753004604950547 | Validation Loss:  0.001552947680465877\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  552 | Train Loss:  0.001375258550979197 | Validation Loss:  0.0015529562951996922\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  553 | Train Loss:  0.0013752171071246266 | Validation Loss:  0.001552963862195611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  554 | Train Loss:  0.0013751761289313436 | Validation Loss:  0.0015529709635302424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  555 | Train Loss:  0.0013751356163993478 | Validation Loss:  0.001552977249957621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  556 | Train Loss:  0.0013750956859439611 | Validation Loss:  0.0015529824886471033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  557 | Train Loss:  0.0013750556390732527 | Validation Loss:  0.00155298737809062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  558 | Train Loss:  0.0013750165235251188 | Validation Loss:  0.0015529911033809185\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  559 | Train Loss:  0.0013749775243923068 | Validation Loss:  0.0015529943630099297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  560 | Train Loss:  0.0013749388745054603 | Validation Loss:  0.001552996807731688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  561 | Train Loss:  0.0013749004574492574 | Validation Loss:  0.0015529985539615154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  562 | Train Loss:  0.0013748628553003073 | Validation Loss:  0.0015529993688687682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  563 | Train Loss:  0.0013748252531513572 | Validation Loss:  0.00155299948528409\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  564 | Train Loss:  0.0013747881166636944 | Validation Loss:  0.001552998903207481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  565 | Train Loss:  0.001374751329421997 | Validation Loss:  0.0015529972733929753\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  566 | Train Loss:  0.0013747148914262652 | Validation Loss:  0.0015529952943325043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  567 | Train Loss:  0.001374678686261177 | Validation Loss:  0.0015529925003647804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  568 | Train Loss:  0.0013746427139267325 | Validation Loss:  0.0015529884258285165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  569 | Train Loss:  0.0013746070908382535 | Validation Loss:  0.0015529841184616089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  570 | Train Loss:  0.00137457181699574 | Validation Loss:  0.0015529787633568048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  571 | Train Loss:  0.0013745368923991919 | Validation Loss:  0.0015529727097600698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  572 | Train Loss:  0.0013745022006332874 | Validation Loss:  0.0015529661905020475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  573 | Train Loss:  0.0013744677416980267 | Validation Loss:  0.0015529588563367724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  574 | Train Loss:  0.0013744337484240532 | Validation Loss:  0.001552950358018279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  575 | Train Loss:  0.0013743999879807234 | Validation Loss:  0.0015529415104538202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  576 | Train Loss:  0.0013743662275373936 | Validation Loss:  0.0015529318479821086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  577 | Train Loss:  0.0013743328163400292 | Validation Loss:  0.0015529212541878223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  578 | Train Loss:  0.0013742997543886304 | Validation Loss:  0.0015529101947322488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  579 | Train Loss:  0.001374267041683197 | Validation Loss:  0.0015528982039541006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  580 | Train Loss:  0.0013742342125624418 | Validation Loss:  0.0015528852818533778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  581 | Train Loss:  0.0013742019655182958 | Validation Loss:  0.0015528718940913677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  582 | Train Loss:  0.0013741699513047934 | Validation Loss:  0.0015528578078374267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  583 | Train Loss:  0.0013741381699219346 | Validation Loss:  0.0015528429066762328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  584 | Train Loss:  0.0013741065049543977 | Validation Loss:  0.0015528270741924644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  585 | Train Loss:  0.0013740749564021826 | Validation Loss:  0.001552810543216765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  586 | Train Loss:  0.001374043757095933 | Validation Loss:  0.0015527937794104218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  587 | Train Loss:  0.0013740126742050052 | Validation Loss:  0.0015527758514508605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  588 | Train Loss:  0.001373981824144721 | Validation Loss:  0.00155275734141469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  589 | Train Loss:  0.0013739512069150805 | Validation Loss:  0.0015527381328865886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  590 | Train Loss:  0.0013739207061007619 | Validation Loss:  0.0015527182258665562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  591 | Train Loss:  0.0013738905545324087 | Validation Loss:  0.001552697503939271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  592 | Train Loss:  0.0013738605193793774 | Validation Loss:  0.0015526763163506985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  593 | Train Loss:  0.0013738306006416678 | Validation Loss:  0.0015526543138548732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  594 | Train Loss:  0.001373800914734602 | Validation Loss:  0.0015526313800364733\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  595 | Train Loss:  0.001373771345242858 | Validation Loss:  0.0015526078641414642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  596 | Train Loss:  0.0013737420085817575 | Validation Loss:  0.0015525839990004897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  597 | Train Loss:  0.0013737129047513008 | Validation Loss:  0.0015525590861216187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  598 | Train Loss:  0.001373683917336166 | Validation Loss:  0.0015525337075814605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  599 | Train Loss:  0.001373654929921031 | Validation Loss:  0.001552507746964693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  600 | Train Loss:  0.0013736264081671834 | Validation Loss:  0.0015524807386100292\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  601 | Train Loss:  0.001373597769998014 | Validation Loss:  0.0015524531481787562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  602 | Train Loss:  0.0013735693646594882 | Validation Loss:  0.001552425092086196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  603 | Train Loss:  0.0013735410757362843 | Validation Loss:  0.0015523964539170265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  604 | Train Loss:  0.0013735127868130803 | Validation Loss:  0.0015523668844252825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  605 | Train Loss:  0.0013734851963818073 | Validation Loss:  0.0015523368492722511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  606 | Train Loss:  0.0013734570238739252 | Validation Loss:  0.0015523061156272888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  607 | Train Loss:  0.0013734294334426522 | Validation Loss:  0.0015522747999057174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  608 | Train Loss:  0.0013734018430113792 | Validation Loss:  0.0015522430185228586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  609 | Train Loss:  0.00137337448541075 | Validation Loss:  0.0015522100729867816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  610 | Train Loss:  0.0013733470113947988 | Validation Loss:  0.0015521771274507046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  611 | Train Loss:  0.001373319886624813 | Validation Loss:  0.001552143250592053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  612 | Train Loss:  0.0013732928782701492 | Validation Loss:  0.001552108908072114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  613 | Train Loss:  0.0013732659863308072 | Validation Loss:  0.001552073867060244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  614 | Train Loss:  0.0013732389779761434 | Validation Loss:  0.0015520381275564432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  615 | Train Loss:  0.0013732122024521232 | Validation Loss:  0.0015520020388066769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  616 | Train Loss:  0.0013731857761740685 | Validation Loss:  0.0015519652515649796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  617 | Train Loss:  0.0013731593498960137 | Validation Loss:  0.0015519276494160295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  618 | Train Loss:  0.0013731328072026372 | Validation Loss:  0.0015518899308517575\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  619 | Train Loss:  0.0013731063809245825 | Validation Loss:  0.0015518510481342673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  620 | Train Loss:  0.0013730803038924932 | Validation Loss:  0.001551812281832099\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  621 | Train Loss:  0.001373054226860404 | Validation Loss:  0.0015517723513767123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  622 | Train Loss:  0.001373028033412993 | Validation Loss:  0.001551732188090682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  623 | Train Loss:  0.0013730021892115474 | Validation Loss:  0.0015516913263127208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  624 | Train Loss:  0.0013729764614254236 | Validation Loss:  0.001551650115288794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  625 | Train Loss:  0.0013729508500546217 | Validation Loss:  0.0015516079729422927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  626 | Train Loss:  0.001372925122268498 | Validation Loss:  0.0015515655977651477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  627 | Train Loss:  0.0013728996273130178 | Validation Loss:  0.0015515228733420372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  628 | Train Loss:  0.001372874015942216 | Validation Loss:  0.001551479334011674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  629 | Train Loss:  0.0013728488702327013 | Validation Loss:  0.0015514352126047015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  630 | Train Loss:  0.001372823491692543 | Validation Loss:  0.0015513907419517636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  631 | Train Loss:  0.0013727981131523848 | Validation Loss:  0.0015513456892222166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  632 | Train Loss:  0.0013727732002735138 | Validation Loss:  0.0015513001708313823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  633 | Train Loss:  0.0013727478217333555 | Validation Loss:  0.001551253953948617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  634 | Train Loss:  0.0013727230252698064 | Validation Loss:  0.0015512073878198862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  635 | Train Loss:  0.0013726981123909354 | Validation Loss:  0.0015511603560298681\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  636 | Train Loss:  0.0013726733159273863 | Validation Loss:  0.0015511129749938846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  637 | Train Loss:  0.0013726485194638371 | Validation Loss:  0.0015510650118812919\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  638 | Train Loss:  0.001372623723000288 | Validation Loss:  0.0015510162338614464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  639 | Train Loss:  0.0013725991593673825 | Validation Loss:  0.001550967455841601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  640 | Train Loss:  0.0013725744793191552 | Validation Loss:  0.0015509179793298244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  641 | Train Loss:  0.0013725499156862497 | Validation Loss:  0.0015508681535720825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  642 | Train Loss:  0.0013725257012993097 | Validation Loss:  0.0015508177457377315\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  643 | Train Loss:  0.0013725011376664042 | Validation Loss:  0.001550766872242093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  644 | Train Loss:  0.0013724768068641424 | Validation Loss:  0.0015507155330851674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  645 | Train Loss:  0.0013724525924772024 | Validation Loss:  0.0015506638446822762\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  646 | Train Loss:  0.001372428610920906 | Validation Loss:  0.0015506119234487414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  647 | Train Loss:  0.0013724042801186442 | Validation Loss:  0.0015505591873079538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  648 | Train Loss:  0.0013723802985623479 | Validation Loss:  0.0015505062183365226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  649 | Train Loss:  0.0013723563170060515 | Validation Loss:  0.0015504529001191258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  650 | Train Loss:  0.0013723322190344334 | Validation Loss:  0.0015503988834097981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  651 | Train Loss:  0.001372308237478137 | Validation Loss:  0.0015503446338698268\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  652 | Train Loss:  0.0013722846051678061 | Validation Loss:  0.00155029003508389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  653 | Train Loss:  0.001372260507196188 | Validation Loss:  0.0015502350870519876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  654 | Train Loss:  0.0013722367584705353 | Validation Loss:  0.0015501794405281544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  655 | Train Loss:  0.0013722132425755262 | Validation Loss:  0.0015501235611736774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  656 | Train Loss:  0.0013721893774345517 | Validation Loss:  0.0015500674489885569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  657 | Train Loss:  0.001372165628708899 | Validation Loss:  0.0015500107547268271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  658 | Train Loss:  0.0013721422292292118 | Validation Loss:  0.001549953711219132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  659 | Train Loss:  0.001372118596918881 | Validation Loss:  0.0015498963184654713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  660 | Train Loss:  0.0013720953138545156 | Validation Loss:  0.001549838576465845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  661 | Train Loss:  0.0013720719143748283 | Validation Loss:  0.0015497804852202535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  662 | Train Loss:  0.0013720482820644975 | Validation Loss:  0.0015497219283133745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  663 | Train Loss:  0.001372024999000132 | Validation Loss:  0.00154966302216053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  664 | Train Loss:  0.001372001483105123 | Validation Loss:  0.0015496039995923638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  665 | Train Loss:  0.0013719782000407577 | Validation Loss:  0.0015495441621169448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  666 | Train Loss:  0.001371955033391714 | Validation Loss:  0.001549484208226204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  667 | Train Loss:  0.0013719319831579924 | Validation Loss:  0.0015494239050894976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  668 | Train Loss:  0.0013719085836783051 | Validation Loss:  0.0015493634855374694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  669 | Train Loss:  0.0013718854170292616 | Validation Loss:  0.0015493023674935102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  670 | Train Loss:  0.0013718623667955399 | Validation Loss:  0.0015492411330342293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  671 | Train Loss:  0.0013718395493924618 | Validation Loss:  0.0015491796657443047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  672 | Train Loss:  0.00137181649915874 | Validation Loss:  0.0015491178492084146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  673 | Train Loss:  0.0013717934489250183 | Validation Loss:  0.001549055683426559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  674 | Train Loss:  0.0013717703986912966 | Validation Loss:  0.0015489929355680943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  675 | Train Loss:  0.0013717475812882185 | Validation Loss:  0.0015489303041249514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  676 | Train Loss:  0.0013717245310544968 | Validation Loss:  0.0015488670906051993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  677 | Train Loss:  0.0013717017136514187 | Validation Loss:  0.0015488036442548037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  678 | Train Loss:  0.0013716788962483406 | Validation Loss:  0.0015487397322431207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  679 | Train Loss:  0.0013716561952605844 | Validation Loss:  0.0015486759366467595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  680 | Train Loss:  0.0013716332614421844 | Validation Loss:  0.0015486113261431456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  681 | Train Loss:  0.00137161067686975 | Validation Loss:  0.0015485468320548534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  682 | Train Loss:  0.001371587859466672 | Validation Loss:  0.0015484819887205958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  683 | Train Loss:  0.0013715651584789157 | Validation Loss:  0.001548416679725051\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  684 | Train Loss:  0.0013715424574911594 | Validation Loss:  0.001548351370729506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  685 | Train Loss:  0.0013715199893340468 | Validation Loss:  0.0015482858289033175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  686 | Train Loss:  0.0013714972883462906 | Validation Loss:  0.0015482197050005198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  687 | Train Loss:  0.0013714747037738562 | Validation Loss:  0.0015481534646824002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  688 | Train Loss:  0.0013714521192014217 | Validation Loss:  0.0015480868751183152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  689 | Train Loss:  0.0013714295346289873 | Validation Loss:  0.0015480202855542302\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  690 | Train Loss:  0.0013714071828871965 | Validation Loss:  0.001547953113913536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  691 | Train Loss:  0.001371384714730084 | Validation Loss:  0.001547885942272842\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  692 | Train Loss:  0.0013713621301576495 | Validation Loss:  0.0015478184213861823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  693 | Train Loss:  0.0013713397784158587 | Validation Loss:  0.0015477505512535572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  694 | Train Loss:  0.001371317426674068 | Validation Loss:  0.001547682681120932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  695 | Train Loss:  0.0013712950749322772 | Validation Loss:  0.0015476144617423415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  696 | Train Loss:  0.0013712727231904864 | Validation Loss:  0.0015475457767024636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  697 | Train Loss:  0.0013712503714486957 | Validation Loss:  0.0015474770916625857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  698 | Train Loss:  0.001371227903291583 | Validation Loss:  0.0015474084066227078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  699 | Train Loss:  0.0013712056679651141 | Validation Loss:  0.0015473393723368645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  700 | Train Loss:  0.001371183549053967 | Validation Loss:  0.0015472699888050556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  701 | Train Loss:  0.001371161313727498 | Validation Loss:  0.0015472001396119595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  702 | Train Loss:  0.0013711390784010291 | Validation Loss:  0.0015471304068341851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  703 | Train Loss:  0.0013711168430745602 | Validation Loss:  0.001547060557641089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  704 | Train Loss:  0.0013710946077480912 | Validation Loss:  0.0015469902427867055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  705 | Train Loss:  0.0013710723724216223 | Validation Loss:  0.0015469200443476439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  706 | Train Loss:  0.0013710504863411188 | Validation Loss:  0.001546849380247295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  707 | Train Loss:  0.0013710282510146499 | Validation Loss:  0.001546778716146946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  708 | Train Loss:  0.0013710061321035028 | Validation Loss:  0.0015467079356312752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  709 | Train Loss:  0.0013709840131923556 | Validation Loss:  0.001546636805869639\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  710 | Train Loss:  0.0013709618942812085 | Validation Loss:  0.0015465649776160717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  711 | Train Loss:  0.001370940008200705 | Validation Loss:  0.0015464937314391136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  712 | Train Loss:  0.001370917772874236 | Validation Loss:  0.0015464220196008682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  713 | Train Loss:  0.0013708957703784108 | Validation Loss:  0.0015463499585166574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  714 | Train Loss:  0.0013708738842979074 | Validation Loss:  0.0015462778974324465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  715 | Train Loss:  0.0013708517653867602 | Validation Loss:  0.0015462058363482356\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  716 | Train Loss:  0.0013708299957215786 | Validation Loss:  0.0015461333096027374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  717 | Train Loss:  0.0013708079932257533 | Validation Loss:  0.0015460606664419174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  718 | Train Loss:  0.0013707861071452498 | Validation Loss:  0.0015459880232810974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  719 | Train Loss:  0.0013707641046494246 | Validation Loss:  0.001545915030874312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  720 | Train Loss:  0.0013707421021535993 | Validation Loss:  0.0015458420384675264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  721 | Train Loss:  0.0013707203324884176 | Validation Loss:  0.0015457686968147755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  722 | Train Loss:  0.0013706984464079142 | Validation Loss:  0.0015456951223313808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  723 | Train Loss:  0.0013706766767427325 | Validation Loss:  0.001545621664263308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  724 | Train Loss:  0.0013706546742469072 | Validation Loss:  0.0015455480897799134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  725 | Train Loss:  0.0013706329045817256 | Validation Loss:  0.001545474398881197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  726 | Train Loss:  0.001370611134916544 | Validation Loss:  0.001545400358736515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  727 | Train Loss:  0.0013705891324207187 | Validation Loss:  0.0015453259693458676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  728 | Train Loss:  0.001370567362755537 | Validation Loss:  0.0015452519292011857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  729 | Train Loss:  0.0013705457095056772 | Validation Loss:  0.0015451775398105383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  730 | Train Loss:  0.0013705241726711392 | Validation Loss:  0.0015451029175892472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  731 | Train Loss:  0.001370502170175314 | Validation Loss:  0.0015450282953679562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  732 | Train Loss:  0.0013704805169254541 | Validation Loss:  0.0015449534403160214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  733 | Train Loss:  0.0013704588636755943 | Validation Loss:  0.0015448787016794086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  734 | Train Loss:  0.0013704370940104127 | Validation Loss:  0.0015448034973815084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  735 | Train Loss:  0.0013704154407605529 | Validation Loss:  0.00154472840949893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  736 | Train Loss:  0.0013703935546800494 | Validation Loss:  0.0015446529723703861\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  737 | Train Loss:  0.0013703720178455114 | Validation Loss:  0.001544577768072486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  738 | Train Loss:  0.0013703503645956516 | Validation Loss:  0.0015445022145286202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  739 | Train Loss:  0.0013703287113457918 | Validation Loss:  0.001544426428154111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  740 | Train Loss:  0.001370307058095932 | Validation Loss:  0.0015443506417796016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  741 | Train Loss:  0.0013702856376767159 | Validation Loss:  0.001544275088235736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  742 | Train Loss:  0.0013702638680115342 | Validation Loss:  0.001544198952615261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  743 | Train Loss:  0.0013702422147616744 | Validation Loss:  0.0015441227005794644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  744 | Train Loss:  0.0013702206779271364 | Validation Loss:  0.0015440466813743114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  745 | Train Loss:  0.0013701991410925984 | Validation Loss:  0.0015439704293385148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  746 | Train Loss:  0.0013701774878427386 | Validation Loss:  0.0015438939444720745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  747 | Train Loss:  0.0013701559510082006 | Validation Loss:  0.0015438173431903124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  748 | Train Loss:  0.0013701342977583408 | Validation Loss:  0.0015437407419085503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  749 | Train Loss:  0.0013701127609238029 | Validation Loss:  0.00154366425704211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  750 | Train Loss:  0.0013700912240892649 | Validation Loss:  0.001543587539345026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  751 | Train Loss:  0.0013700696872547269 | Validation Loss:  0.0015435105888172984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  752 | Train Loss:  0.0013700482668355107 | Validation Loss:  0.0015434336382895708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  753 | Train Loss:  0.0013700267300009727 | Validation Loss:  0.0015433566877618432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  754 | Train Loss:  0.0013700053095817566 | Validation Loss:  0.0015432793879881501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  755 | Train Loss:  0.0013699838891625404 | Validation Loss:  0.0015432024374604225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  756 | Train Loss:  0.0013699624687433243 | Validation Loss:  0.0015431251376867294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  757 | Train Loss:  0.0013699409319087863 | Validation Loss:  0.0015430478379130363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  758 | Train Loss:  0.0013699193950742483 | Validation Loss:  0.0015429701888933778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  759 | Train Loss:  0.001369898091070354 | Validation Loss:  0.0015428925398737192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  760 | Train Loss:  0.001369876554235816 | Validation Loss:  0.0015428152401000261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  761 | Train Loss:  0.0013698552502319217 | Validation Loss:  0.0015427375910803676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  762 | Train Loss:  0.0013698337133973837 | Validation Loss:  0.0015426597092300653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  763 | Train Loss:  0.0013698122929781675 | Validation Loss:  0.0015425818273797631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  764 | Train Loss:  0.0013697909889742732 | Validation Loss:  0.001542503945529461\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  765 | Train Loss:  0.0013697694521397352 | Validation Loss:  0.0015424261800944805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  766 | Train Loss:  0.001369748148135841 | Validation Loss:  0.0015423481818288565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  767 | Train Loss:  0.0013697267277166247 | Validation Loss:  0.001542269834317267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  768 | Train Loss:  0.0013697053072974086 | Validation Loss:  0.001542191836051643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  769 | Train Loss:  0.0013696840032935143 | Validation Loss:  0.001542113721370697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  770 | Train Loss:  0.001369662582874298 | Validation Loss:  0.0015420354902744293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  771 | Train Loss:  0.0013696412788704038 | Validation Loss:  0.001541957026347518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  772 | Train Loss:  0.0013696199748665094 | Validation Loss:  0.001541878911666572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  773 | Train Loss:  0.0013695985544472933 | Validation Loss:  0.001541800331324339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  774 | Train Loss:  0.0013695773668587208 | Validation Loss:  0.0015417221002280712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  775 | Train Loss:  0.0013695560628548265 | Validation Loss:  0.0015416434034705162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  776 | Train Loss:  0.0013695347588509321 | Validation Loss:  0.0015415649395436049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  777 | Train Loss:  0.001369513338431716 | Validation Loss:  0.0015414862427860498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  778 | Train Loss:  0.0013694920344278216 | Validation Loss:  0.0015414076624438167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  779 | Train Loss:  0.0013694708468392491 | Validation Loss:  0.0015413288492709398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  780 | Train Loss:  0.0013694495428353548 | Validation Loss:  0.001541250036098063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  781 | Train Loss:  0.0013694282388314605 | Validation Loss:  0.001541171339340508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  782 | Train Loss:  0.0013694069348275661 | Validation Loss:  0.001541092642582953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  783 | Train Loss:  0.0013693857472389936 | Validation Loss:  0.0015410137129947543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  784 | Train Loss:  0.0013693643268197775 | Validation Loss:  0.0015409346669912338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  785 | Train Loss:  0.001369343139231205 | Validation Loss:  0.001540855853818357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  786 | Train Loss:  0.0013693219516426325 | Validation Loss:  0.0015407768078148365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  787 | Train Loss:  0.0013693006476387382 | Validation Loss:  0.0015406976453959942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  788 | Train Loss:  0.0013692793436348438 | Validation Loss:  0.0015406185993924737\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  789 | Train Loss:  0.0013692582724615932 | Validation Loss:  0.0015405395533889532\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  790 | Train Loss:  0.0013692370848730206 | Validation Loss:  0.0015404606238007545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  791 | Train Loss:  0.0013692156644538045 | Validation Loss:  0.0015403813449665904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  792 | Train Loss:  0.001369194476865232 | Validation Loss:  0.001540302182547748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  793 | Train Loss:  0.0013691732892766595 | Validation Loss:  0.0015402230201289058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  794 | Train Loss:  0.0013691519852727652 | Validation Loss:  0.0015401438577100635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  795 | Train Loss:  0.0013691307976841927 | Validation Loss:  0.0015400644624605775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  796 | Train Loss:  0.0013691096100956202 | Validation Loss:  0.0015399850672110915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  797 | Train Loss:  0.0013690884225070477 | Validation Loss:  0.0015399059047922492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  798 | Train Loss:  0.0013690672349184752 | Validation Loss:  0.0015398263931274414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  799 | Train Loss:  0.0013690460473299026 | Validation Loss:  0.001539747230708599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  800 | Train Loss:  0.001369024976156652 | Validation Loss:  0.0015396677190437913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  801 | Train Loss:  0.0013690037885680795 | Validation Loss:  0.0015395884402096272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  802 | Train Loss:  0.0013689824845641851 | Validation Loss:  0.001539509161375463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  803 | Train Loss:  0.0013689612969756126 | Validation Loss:  0.0015394296497106552\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  804 | Train Loss:  0.001368940225802362 | Validation Loss:  0.0015393500216305256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  805 | Train Loss:  0.0013689190382137895 | Validation Loss:  0.0015392707427963614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  806 | Train Loss:  0.001368897850625217 | Validation Loss:  0.0015391912311315536\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  807 | Train Loss:  0.0013688767794519663 | Validation Loss:  0.0015391118358820677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  808 | Train Loss:  0.0013688557082787156 | Validation Loss:  0.0015390320913866162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  809 | Train Loss:  0.001368834520690143 | Validation Loss:  0.001538952812552452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  810 | Train Loss:  0.0013688135659322143 | Validation Loss:  0.0015388731844723225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  811 | Train Loss:  0.00136879226192832 | Validation Loss:  0.0015387936728075147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  812 | Train Loss:  0.0013687711907550693 | Validation Loss:  0.001538714044727385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  813 | Train Loss:  0.0013687501195818186 | Validation Loss:  0.001538634649477899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  814 | Train Loss:  0.0013687288155779243 | Validation Loss:  0.0015385551378130913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  815 | Train Loss:  0.0013687077444046736 | Validation Loss:  0.0015384756261482835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  816 | Train Loss:  0.001368686556816101 | Validation Loss:  0.0015383957652375102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  817 | Train Loss:  0.0013686656020581722 | Validation Loss:  0.0015383163699880242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  818 | Train Loss:  0.0013686444144695997 | Validation Loss:  0.0015382368583232164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  819 | Train Loss:  0.0013686232268810272 | Validation Loss:  0.001538157113827765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  820 | Train Loss:  0.0013686020392924547 | Validation Loss:  0.0015380776021629572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  821 | Train Loss:  0.001368580968119204 | Validation Loss:  0.0015379980904981494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  822 | Train Loss:  0.0013685597805306315 | Validation Loss:  0.0015379185788333416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  823 | Train Loss:  0.0013685389421880245 | Validation Loss:  0.0015378388343378901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  824 | Train Loss:  0.0013685176381841302 | Validation Loss:  0.0015377590898424387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  825 | Train Loss:  0.0013684965670108795 | Validation Loss:  0.0015376799274235964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  826 | Train Loss:  0.0013684754958376288 | Validation Loss:  0.001537600182928145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  827 | Train Loss:  0.0013684544246643782 | Validation Loss:  0.0015375203220173717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  828 | Train Loss:  0.0013684333534911275 | Validation Loss:  0.0015374411595985293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  829 | Train Loss:  0.0013684122823178768 | Validation Loss:  0.0015373616479337215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  830 | Train Loss:  0.0013683910947293043 | Validation Loss:  0.00153728190343827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  831 | Train Loss:  0.0013683700235560536 | Validation Loss:  0.0015372022753581405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  832 | Train Loss:  0.001368348952382803 | Validation Loss:  0.0015371228801086545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  833 | Train Loss:  0.0013683277647942305 | Validation Loss:  0.0015370433684438467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  834 | Train Loss:  0.0013683066936209798 | Validation Loss:  0.001536963856779039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  835 | Train Loss:  0.0013682855060324073 | Validation Loss:  0.0015368843451142311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  836 | Train Loss:  0.0013682646676898003 | Validation Loss:  0.0015368049498647451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  837 | Train Loss:  0.0013682435965165496 | Validation Loss:  0.0015367254381999373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  838 | Train Loss:  0.001368222408927977 | Validation Loss:  0.0015366459265351295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  839 | Train Loss:  0.0013682014541700482 | Validation Loss:  0.001536566182039678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  840 | Train Loss:  0.0013681802665814757 | Validation Loss:  0.0015364870196208358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  841 | Train Loss:  0.001368159195408225 | Validation Loss:  0.001536407507956028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  842 | Train Loss:  0.0013681381242349744 | Validation Loss:  0.001536328112706542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  843 | Train Loss:  0.0013681170530617237 | Validation Loss:  0.0015362486010417342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  844 | Train Loss:  0.001368095981888473 | Validation Loss:  0.001536169438622892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  845 | Train Loss:  0.0013680749107152224 | Validation Loss:  0.001536090043373406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  846 | Train Loss:  0.001368053606711328 | Validation Loss:  0.0015360105317085981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  847 | Train Loss:  0.0013680326519533992 | Validation Loss:  0.0015359310200437903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  848 | Train Loss:  0.0013680115807801485 | Validation Loss:  0.0015358519740402699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  849 | Train Loss:  0.0013679905096068978 | Validation Loss:  0.0015357725787907839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  850 | Train Loss:  0.0013679694384336472 | Validation Loss:  0.001535693183541298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  851 | Train Loss:  0.0013679484836757183 | Validation Loss:  0.0015356140211224556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  852 | Train Loss:  0.0013679272960871458 | Validation Loss:  0.0015355348587036133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  853 | Train Loss:  0.001367906341329217 | Validation Loss:  0.0015354554634541273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  854 | Train Loss:  0.0013678851537406445 | Validation Loss:  0.0015353761846199632\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  855 | Train Loss:  0.0013678640825673938 | Validation Loss:  0.001535296905785799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  856 | Train Loss:  0.001367843011394143 | Validation Loss:  0.0015352177433669567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  857 | Train Loss:  0.0013678218238055706 | Validation Loss:  0.0015351386973634362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  858 | Train Loss:  0.0013678008690476418 | Validation Loss:  0.001535059418529272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  859 | Train Loss:  0.0013677795650437474 | Validation Loss:  0.0015349803725257516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  860 | Train Loss:  0.0013677587267011404 | Validation Loss:  0.0015349012101069093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  861 | Train Loss:  0.001367737539112568 | Validation Loss:  0.0015348221641033888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  862 | Train Loss:  0.0013677164679393172 | Validation Loss:  0.001534743350930512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  863 | Train Loss:  0.0013676953967660666 | Validation Loss:  0.0015346643049269915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  864 | Train Loss:  0.001367674209177494 | Validation Loss:  0.0015345851425081491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  865 | Train Loss:  0.0013676531380042434 | Validation Loss:  0.0015345063293352723\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  866 | Train Loss:  0.0013676319504156709 | Validation Loss:  0.0015344272833317518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  867 | Train Loss:  0.0013676108792424202 | Validation Loss:  0.0015343482373282313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  868 | Train Loss:  0.0013675898080691695 | Validation Loss:  0.0015342694241553545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  869 | Train Loss:  0.0013675688533112407 | Validation Loss:  0.0015341907273977995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  870 | Train Loss:  0.0013675475493073463 | Validation Loss:  0.0015341119142249227\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  871 | Train Loss:  0.0013675263617187738 | Validation Loss:  0.0015340332174673676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  872 | Train Loss:  0.001367505406960845 | Validation Loss:  0.0015339541714638472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  873 | Train Loss:  0.0013674841029569507 | Validation Loss:  0.001533875591121614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  874 | Train Loss:  0.0013674631481990218 | Validation Loss:  0.001533796894364059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  875 | Train Loss:  0.0013674420770257711 | Validation Loss:  0.001533718197606504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  876 | Train Loss:  0.0013674210058525205 | Validation Loss:  0.0015336393844336271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  877 | Train Loss:  0.001367399818263948 | Validation Loss:  0.0015335606876760721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  878 | Train Loss:  0.0013673786306753755 | Validation Loss:  0.0015334822237491608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  879 | Train Loss:  0.0013673573266714811 | Validation Loss:  0.0015334035269916058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  880 | Train Loss:  0.0013673364883288741 | Validation Loss:  0.0015333250630646944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  881 | Train Loss:  0.0013673151843249798 | Validation Loss:  0.0015332467155531049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  882 | Train Loss:  0.0013672939967364073 | Validation Loss:  0.0015331680187955499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  883 | Train Loss:  0.0013672728091478348 | Validation Loss:  0.0015330896712839603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  884 | Train Loss:  0.0013672516215592623 | Validation Loss:  0.0015330113237723708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  885 | Train Loss:  0.0013672304339706898 | Validation Loss:  0.0015329328598454595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  886 | Train Loss:  0.001367209479212761 | Validation Loss:  0.00153285451233387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  887 | Train Loss:  0.0013671880587935448 | Validation Loss:  0.0015327761648222804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  888 | Train Loss:  0.0013671668712049723 | Validation Loss:  0.0015326980501413345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  889 | Train Loss:  0.0013671458000317216 | Validation Loss:  0.0015326199354603887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  890 | Train Loss:  0.001367124612443149 | Validation Loss:  0.0015325415879487991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  891 | Train Loss:  0.0013671034248545766 | Validation Loss:  0.0015324634732678533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  892 | Train Loss:  0.001367082353681326 | Validation Loss:  0.0015323852421715856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  893 | Train Loss:  0.0013670611660927534 | Validation Loss:  0.0015323072439059615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  894 | Train Loss:  0.0013670397456735373 | Validation Loss:  0.001532228896394372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  895 | Train Loss:  0.0013670185580849648 | Validation Loss:  0.0015321510145440698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  896 | Train Loss:  0.001366997486911714 | Validation Loss:  0.0015320731326937675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  897 | Train Loss:  0.0013669762993231416 | Validation Loss:  0.0015319951344281435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  898 | Train Loss:  0.001366955111734569 | Validation Loss:  0.0015319171361625195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  899 | Train Loss:  0.001366933691315353 | Validation Loss:  0.001531839370727539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  900 | Train Loss:  0.0013669123873114586 | Validation Loss:  0.0015317616052925587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  901 | Train Loss:  0.001366891316138208 | Validation Loss:  0.0015316836070269346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  902 | Train Loss:  0.0013668700121343136 | Validation Loss:  0.001531605958007276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  903 | Train Loss:  0.001366848824545741 | Validation Loss:  0.0015315283089876175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  904 | Train Loss:  0.001366827404126525 | Validation Loss:  0.0015314504271373153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  905 | Train Loss:  0.0013668063329532743 | Validation Loss:  0.0015313726617023349\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  906 | Train Loss:  0.001366784912534058 | Validation Loss:  0.0015312950126826763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  907 | Train Loss:  0.0013667636085301638 | Validation Loss:  0.0015312175964936614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  908 | Train Loss:  0.0013667423045262694 | Validation Loss:  0.0015311400638893247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  909 | Train Loss:  0.001366721116937697 | Validation Loss:  0.001531062414869666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  910 | Train Loss:  0.0013666998129338026 | Validation Loss:  0.0015309849986806512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  911 | Train Loss:  0.0013666785089299083 | Validation Loss:  0.001530907698906958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  912 | Train Loss:  0.0013666569720953703 | Validation Loss:  0.0015308300498872995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  913 | Train Loss:  0.001366635668091476 | Validation Loss:  0.0015307527501136065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  914 | Train Loss:  0.0013666143640875816 | Validation Loss:  0.0015306755667552352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  915 | Train Loss:  0.0013665930600836873 | Validation Loss:  0.0015305984998121858\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  916 | Train Loss:  0.0013665716396644711 | Validation Loss:  0.0015305208507925272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  917 | Train Loss:  0.0013665503356605768 | Validation Loss:  0.0015304439002647996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  918 | Train Loss:  0.0013665289152413607 | Validation Loss:  0.0015303668333217502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  919 | Train Loss:  0.0013665076112374663 | Validation Loss:  0.0015302897663787007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  920 | Train Loss:  0.0013664860744029284 | Validation Loss:  0.0015302124666050076\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  921 | Train Loss:  0.0013664646539837122 | Validation Loss:  0.0015301353996619582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  922 | Train Loss:  0.0013664433499798179 | Validation Loss:  0.0015300585655495524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  923 | Train Loss:  0.0013664218131452799 | Validation Loss:  0.0015299816150218248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  924 | Train Loss:  0.0013664006255567074 | Validation Loss:  0.0015299046644940972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  925 | Train Loss:  0.0013663789723068476 | Validation Loss:  0.0015298280632123351\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  926 | Train Loss:  0.0013663576683029532 | Validation Loss:  0.0015297511126846075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  927 | Train Loss:  0.0013663360150530934 | Validation Loss:  0.00152967416215688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  928 | Train Loss:  0.001366314711049199 | Validation Loss:  0.0015295975608751178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  929 | Train Loss:  0.0013662931742146611 | Validation Loss:  0.0015295209595933557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  930 | Train Loss:  0.0013662716373801231 | Validation Loss:  0.0015294442418962717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  931 | Train Loss:  0.0013662501005455852 | Validation Loss:  0.001529367407783866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  932 | Train Loss:  0.0013662285637110472 | Validation Loss:  0.0015292911557480693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  933 | Train Loss:  0.0013662070268765092 | Validation Loss:  0.001529214670881629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  934 | Train Loss:  0.0013661854900419712 | Validation Loss:  0.0015291380695998669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  935 | Train Loss:  0.0013661639532074332 | Validation Loss:  0.0015290617011487484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  936 | Train Loss:  0.0013661422999575734 | Validation Loss:  0.00152898533269763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  937 | Train Loss:  0.0013661208795383573 | Validation Loss:  0.0015289089642465115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  938 | Train Loss:  0.0013660992262884974 | Validation Loss:  0.0015288327122107148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  939 | Train Loss:  0.0013660775730386376 | Validation Loss:  0.00152875657659024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  940 | Train Loss:  0.0013660560362040997 | Validation Loss:  0.0015286802081391215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  941 | Train Loss:  0.0013660343829542398 | Validation Loss:  0.0015286040725186467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  942 | Train Loss:  0.00136601272970438 | Validation Loss:  0.001528527936898172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  943 | Train Loss:  0.001365991192869842 | Validation Loss:  0.001528451917693019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  944 | Train Loss:  0.0013659694232046604 | Validation Loss:  0.001528375782072544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  945 | Train Loss:  0.0013659476535394788 | Validation Loss:  0.001528299879282713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  946 | Train Loss:  0.0013659257674589753 | Validation Loss:  0.0015282239764928818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  947 | Train Loss:  0.0013659043470397592 | Validation Loss:  0.0015281479572877288\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  948 | Train Loss:  0.0013658825773745775 | Validation Loss:  0.001528072403743863\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  949 | Train Loss:  0.0013658608077093959 | Validation Loss:  0.001527996500954032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  950 | Train Loss:  0.001365839154459536 | Validation Loss:  0.0015279207145795226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  951 | Train Loss:  0.0013658173847943544 | Validation Loss:  0.001527845044620335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  952 | Train Loss:  0.001365795498713851 | Validation Loss:  0.0015277692582458258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  953 | Train Loss:  0.0013657736126333475 | Validation Loss:  0.001527693821117282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  954 | Train Loss:  0.0013657518429681659 | Validation Loss:  0.0015276182675734162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  955 | Train Loss:  0.0013657300733029842 | Validation Loss:  0.001527542481198907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  956 | Train Loss:  0.0013657083036378026 | Validation Loss:  0.0015274671604856849\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  957 | Train Loss:  0.0013656864175572991 | Validation Loss:  0.001527391723357141\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  958 | Train Loss:  0.0013656647643074393 | Validation Loss:  0.0015273162862285972\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  959 | Train Loss:  0.001365642761811614 | Validation Loss:  0.0015272408491000533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  960 | Train Loss:  0.0013656207593157887 | Validation Loss:  0.0015271655283868313\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  961 | Train Loss:  0.0013655987568199635 | Validation Loss:  0.001527090440504253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  962 | Train Loss:  0.00136557687073946 | Validation Loss:  0.0015270151197910309\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  963 | Train Loss:  0.0013655548682436347 | Validation Loss:  0.0015269399154931307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  964 | Train Loss:  0.0013655328657478094 | Validation Loss:  0.0015268648276105523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  965 | Train Loss:  0.0013655108632519841 | Validation Loss:  0.0015267898561432958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  966 | Train Loss:  0.0013654889771714807 | Validation Loss:  0.0015267148846760392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  967 | Train Loss:  0.0013654669746756554 | Validation Loss:  0.0015266397967934608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  968 | Train Loss:  0.0013654448557645082 | Validation Loss:  0.0015265648253262043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  969 | Train Loss:  0.0013654227368533611 | Validation Loss:  0.0015264900866895914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  970 | Train Loss:  0.001365400617942214 | Validation Loss:  0.0015264152316376567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  971 | Train Loss:  0.001365378499031067 | Validation Loss:  0.0015263406094163656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  972 | Train Loss:  0.0013653566129505634 | Validation Loss:  0.001526265754364431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  973 | Train Loss:  0.0013653342612087727 | Validation Loss:  0.001526191015727818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  974 | Train Loss:  0.0013653121422976255 | Validation Loss:  0.0015261162770912051\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  975 | Train Loss:  0.0013652899069711566 | Validation Loss:  0.0015260417712852359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  976 | Train Loss:  0.0013652677880600095 | Validation Loss:  0.0015259671490639448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  977 | Train Loss:  0.0013652455527335405 | Validation Loss:  0.0015258926432579756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  978 | Train Loss:  0.0013652233174070716 | Validation Loss:  0.0015258181374520063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  979 | Train Loss:  0.0013652010820806026 | Validation Loss:  0.001525743748061359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  980 | Train Loss:  0.0013651787303388119 | Validation Loss:  0.0015256693586707115\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  981 | Train Loss:  0.0013651566114276648 | Validation Loss:  0.001525595085695386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  982 | Train Loss:  0.0013651341432705522 | Validation Loss:  0.0015255208127200603\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  983 | Train Loss:  0.0013651117915287614 | Validation Loss:  0.0015254465397447348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  984 | Train Loss:  0.0013650895562022924 | Validation Loss:  0.001525372383184731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  985 | Train Loss:  0.0013650670880451798 | Validation Loss:  0.001525298343040049\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  986 | Train Loss:  0.0013650446198880672 | Validation Loss:  0.0015252243028953671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  987 | Train Loss:  0.0013650221517309546 | Validation Loss:  0.0015251502627506852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  988 | Train Loss:  0.0013649997999891639 | Validation Loss:  0.0015250759897753596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  989 | Train Loss:  0.0013649773318320513 | Validation Loss:  0.0015250021824613214\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  990 | Train Loss:  0.0013649547472596169 | Validation Loss:  0.001524928375147283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  991 | Train Loss:  0.0013649321626871824 | Validation Loss:  0.001524854451417923\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  992 | Train Loss:  0.0013649096945300698 | Validation Loss:  0.0015247806441038847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  993 | Train Loss:  0.0013648871099576354 | Validation Loss:  0.0015247067203745246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  994 | Train Loss:  0.0013648646418005228 | Validation Loss:  0.0015246330294758081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  995 | Train Loss:  0.0013648418243974447 | Validation Loss:  0.0015245593385770917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  996 | Train Loss:  0.0013648192398250103 | Validation Loss:  0.001524485880509019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  997 | Train Loss:  0.001364796538837254 | Validation Loss:  0.0015244123060256243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  998 | Train Loss:  0.0013647739542648196 | Validation Loss:  0.0015243386151269078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Epoch:  999 | Train Loss:  0.0013647510204464197 | Validation Loss:  0.0015242652734741569\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'Adam'} Test Loss:  0.001165293506346643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  0 | Train Loss:  0.1104988232254982 | Validation Loss:  0.10417775809764862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  1 | Train Loss:  0.11009068042039871 | Validation Loss:  0.10377886891365051\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  2 | Train Loss:  0.1096843034029007 | Validation Loss:  0.10338172316551208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  3 | Train Loss:  0.10927966982126236 | Validation Loss:  0.10298631340265274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  4 | Train Loss:  0.1088767945766449 | Validation Loss:  0.10259262472391129\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  5 | Train Loss:  0.10847566276788712 | Validation Loss:  0.10220064222812653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  6 | Train Loss:  0.10807625949382782 | Validation Loss:  0.10181038081645966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  7 | Train Loss:  0.10767856240272522 | Validation Loss:  0.10142180323600769\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  8 | Train Loss:  0.10728258639574051 | Validation Loss:  0.10103493183851242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  9 | Train Loss:  0.10688833147287369 | Validation Loss:  0.10064974427223206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  10 | Train Loss:  0.10649577528238297 | Validation Loss:  0.1002662405371666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  11 | Train Loss:  0.10610491037368774 | Validation Loss:  0.09988438338041306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  12 | Train Loss:  0.10571572929620743 | Validation Loss:  0.09950420260429382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  13 | Train Loss:  0.10532822459936142 | Validation Loss:  0.09912567585706711\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  14 | Train Loss:  0.10494240373373032 | Validation Loss:  0.09874880313873291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  15 | Train Loss:  0.10455823689699173 | Validation Loss:  0.09837356954813004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  16 | Train Loss:  0.10417573153972626 | Validation Loss:  0.09799998253583908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  17 | Train Loss:  0.1037948876619339 | Validation Loss:  0.09762800484895706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  18 | Train Loss:  0.10341566056013107 | Validation Loss:  0.09725766628980637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  19 | Train Loss:  0.10303808748722076 | Validation Loss:  0.0968889370560646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  20 | Train Loss:  0.10266215354204178 | Validation Loss:  0.09652180224657059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  21 | Train Loss:  0.10228782147169113 | Validation Loss:  0.09615626931190491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  22 | Train Loss:  0.10191511362791061 | Validation Loss:  0.09579234570264816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  23 | Train Loss:  0.10154401510953903 | Validation Loss:  0.09542998671531677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  24 | Train Loss:  0.10117451846599579 | Validation Loss:  0.09506921470165253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  25 | Train Loss:  0.10080660134553909 | Validation Loss:  0.09471000730991364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  26 | Train Loss:  0.10044029355049133 | Validation Loss:  0.0943523719906807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  27 | Train Loss:  0.10007553547620773 | Validation Loss:  0.09399629384279251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  28 | Train Loss:  0.09971237927675247 | Validation Loss:  0.09364177286624908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  29 | Train Loss:  0.09935078024864197 | Validation Loss:  0.09328878670930862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  30 | Train Loss:  0.09899073839187622 | Validation Loss:  0.09293735027313232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  31 | Train Loss:  0.09863226115703583 | Validation Loss:  0.0925874412059784\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  32 | Train Loss:  0.098275326192379 | Validation Loss:  0.09223905205726624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  33 | Train Loss:  0.09791992604732513 | Validation Loss:  0.09189219027757645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  34 | Train Loss:  0.09756606072187424 | Validation Loss:  0.09154683351516724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  35 | Train Loss:  0.0972137302160263 | Validation Loss:  0.09120296686887741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  36 | Train Loss:  0.09686291217803955 | Validation Loss:  0.09086062014102936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  37 | Train Loss:  0.09651359915733337 | Validation Loss:  0.09051975607872009\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  38 | Train Loss:  0.09616579860448837 | Validation Loss:  0.09018037468194962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  39 | Train Loss:  0.09581951051950455 | Validation Loss:  0.08984248340129852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  40 | Train Loss:  0.09547470510005951 | Validation Loss:  0.08950605243444443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  41 | Train Loss:  0.09513138234615326 | Validation Loss:  0.08917109668254852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  42 | Train Loss:  0.09478955715894699 | Validation Loss:  0.08883760124444962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  43 | Train Loss:  0.09444919228553772 | Validation Loss:  0.08850555121898651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  44 | Train Loss:  0.09411030262708664 | Validation Loss:  0.08817495405673981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  45 | Train Loss:  0.09377287328243256 | Validation Loss:  0.0878458023071289\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  46 | Train Loss:  0.09343690425157547 | Validation Loss:  0.08751807361841202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  47 | Train Loss:  0.09310238063335419 | Validation Loss:  0.08719179034233093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  48 | Train Loss:  0.09276929497718811 | Validation Loss:  0.08686691522598267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  49 | Train Loss:  0.09243765473365784 | Validation Loss:  0.08654345571994781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  50 | Train Loss:  0.09210743010044098 | Validation Loss:  0.08622141182422638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  51 | Train Loss:  0.09177865833044052 | Validation Loss:  0.08590076863765717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  52 | Train Loss:  0.09145128726959229 | Validation Loss:  0.08558151870965958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  53 | Train Loss:  0.09112532436847687 | Validation Loss:  0.08526366949081421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  54 | Train Loss:  0.09080076962709427 | Validation Loss:  0.08494720607995987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  55 | Train Loss:  0.09047762304544449 | Validation Loss:  0.08463212102651596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  56 | Train Loss:  0.09015586227178574 | Validation Loss:  0.08431841433048248\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  57 | Train Loss:  0.08983548730611801 | Validation Loss:  0.08400605618953705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  58 | Train Loss:  0.08951650559902191 | Validation Loss:  0.08369507640600204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  59 | Train Loss:  0.08919889479875565 | Validation Loss:  0.08338543772697449\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  60 | Train Loss:  0.08888265490531921 | Validation Loss:  0.08307716995477676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  61 | Train Loss:  0.08856777101755142 | Validation Loss:  0.08277022838592529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  62 | Train Loss:  0.08825425803661346 | Validation Loss:  0.08246464282274246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  63 | Train Loss:  0.08794208616018295 | Validation Loss:  0.08216037601232529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  64 | Train Loss:  0.08763127028942108 | Validation Loss:  0.08185742795467377\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  65 | Train Loss:  0.08732178807258606 | Validation Loss:  0.0815558135509491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  66 | Train Loss:  0.08701363950967789 | Validation Loss:  0.08125551789999008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  67 | Train Loss:  0.08670682460069656 | Validation Loss:  0.08095651865005493\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  68 | Train Loss:  0.0864013209939003 | Validation Loss:  0.08065883070230484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  69 | Train Loss:  0.08609715104103088 | Validation Loss:  0.08036243915557861\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  70 | Train Loss:  0.08579429239034653 | Validation Loss:  0.08006734400987625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  71 | Train Loss:  0.08549273014068604 | Validation Loss:  0.07977352291345596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  72 | Train Loss:  0.0851924791932106 | Validation Loss:  0.07948100566864014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  73 | Train Loss:  0.08489351719617844 | Validation Loss:  0.07918975502252579\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  74 | Train Loss:  0.08459584414958954 | Validation Loss:  0.07889975607395172\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  75 | Train Loss:  0.0842994675040245 | Validation Loss:  0.07861103117465973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  76 | Train Loss:  0.08400434255599976 | Validation Loss:  0.07832358777523041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  77 | Train Loss:  0.08371051400899887 | Validation Loss:  0.07803737372159958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  78 | Train Loss:  0.08341795206069946 | Validation Loss:  0.07775240391492844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  79 | Train Loss:  0.08312663435935974 | Validation Loss:  0.07746869325637817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  80 | Train Loss:  0.08283659815788269 | Validation Loss:  0.077186219394207\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  81 | Train Loss:  0.08254779875278473 | Validation Loss:  0.07690496742725372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  82 | Train Loss:  0.08226024359464645 | Validation Loss:  0.07662494480609894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  83 | Train Loss:  0.08197392523288727 | Validation Loss:  0.07634614408016205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  84 | Train Loss:  0.08168885111808777 | Validation Loss:  0.07606857270002365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  85 | Train Loss:  0.08140499889850616 | Validation Loss:  0.07579218596220016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  86 | Train Loss:  0.08112237602472305 | Validation Loss:  0.07551702111959457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  87 | Train Loss:  0.08084096759557724 | Validation Loss:  0.07524305582046509\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  88 | Train Loss:  0.08056077361106873 | Validation Loss:  0.07497028261423111\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  89 | Train Loss:  0.08028178662061691 | Validation Loss:  0.07469870895147324\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  90 | Train Loss:  0.0800040066242218 | Validation Loss:  0.07442831248044968\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  91 | Train Loss:  0.07972743362188339 | Validation Loss:  0.07415908575057983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  92 | Train Loss:  0.0794520378112793 | Validation Loss:  0.0738910511136055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  93 | Train Loss:  0.0791778415441513 | Validation Loss:  0.07362417131662369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  94 | Train Loss:  0.07890481501817703 | Validation Loss:  0.0733584612607956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  95 | Train Loss:  0.07863298058509827 | Validation Loss:  0.07309390604496002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  96 | Train Loss:  0.07836230844259262 | Validation Loss:  0.07283052057027817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  97 | Train Loss:  0.07809281349182129 | Validation Loss:  0.07256826758384705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  98 | Train Loss:  0.07782447338104248 | Validation Loss:  0.07230715453624725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  99 | Train Loss:  0.0775572881102562 | Validation Loss:  0.07204718887805939\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  100 | Train Loss:  0.07729125767946243 | Validation Loss:  0.07178836315870285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  101 | Train Loss:  0.0770263671875 | Validation Loss:  0.07153066247701645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  102 | Train Loss:  0.07676263153553009 | Validation Loss:  0.07127407938241959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  103 | Train Loss:  0.07650002837181091 | Validation Loss:  0.07101862877607346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  104 | Train Loss:  0.07623855769634247 | Validation Loss:  0.07076428830623627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  105 | Train Loss:  0.07597821950912476 | Validation Loss:  0.07051105052232742\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  106 | Train Loss:  0.07571899145841599 | Validation Loss:  0.07025892287492752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  107 | Train Loss:  0.07546088844537735 | Validation Loss:  0.07000789791345596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  108 | Train Loss:  0.07520390301942825 | Validation Loss:  0.06975796073675156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  109 | Train Loss:  0.0749480202794075 | Validation Loss:  0.0695091187953949\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  110 | Train Loss:  0.07469324767589569 | Validation Loss:  0.06926136463880539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  111 | Train Loss:  0.07443956285715103 | Validation Loss:  0.06901469081640244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  112 | Train Loss:  0.07418698817491531 | Validation Loss:  0.06876908987760544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  113 | Train Loss:  0.07393547892570496 | Validation Loss:  0.0685245618224144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  114 | Train Loss:  0.07368507236242294 | Validation Loss:  0.06828109920024872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  115 | Train Loss:  0.07343573868274689 | Validation Loss:  0.0680387020111084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  116 | Train Loss:  0.07318747788667679 | Validation Loss:  0.06779737025499344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  117 | Train Loss:  0.07294031232595444 | Validation Loss:  0.06755708158016205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  118 | Train Loss:  0.07269418239593506 | Validation Loss:  0.06731785088777542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  119 | Train Loss:  0.07244911789894104 | Validation Loss:  0.06707965582609177\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  120 | Train Loss:  0.07220511883497238 | Validation Loss:  0.06684250384569168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  121 | Train Loss:  0.07196217030286789 | Validation Loss:  0.06660638749599457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  122 | Train Loss:  0.07172026485204697 | Validation Loss:  0.06637129932641983\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  123 | Train Loss:  0.07147940993309021 | Validation Loss:  0.06613723933696747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  124 | Train Loss:  0.07123959064483643 | Validation Loss:  0.06590419262647629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  125 | Train Loss:  0.07100080698728561 | Validation Loss:  0.06567218154668808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  126 | Train Loss:  0.07076305150985718 | Validation Loss:  0.06544116884469986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  127 | Train Loss:  0.07052632421255112 | Validation Loss:  0.06521116942167282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  128 | Train Loss:  0.07029062509536743 | Validation Loss:  0.06498217582702637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  129 | Train Loss:  0.07005592435598373 | Validation Loss:  0.0647541806101799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  130 | Train Loss:  0.06982222944498062 | Validation Loss:  0.06452716886997223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  131 | Train Loss:  0.06958957016468048 | Validation Loss:  0.06430116295814514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  132 | Train Loss:  0.06935789436101913 | Validation Loss:  0.06407614052295685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  133 | Train Loss:  0.06912722438573837 | Validation Loss:  0.06385210901498795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  134 | Train Loss:  0.068897545337677 | Validation Loss:  0.06362903863191605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  135 | Train Loss:  0.06866885721683502 | Validation Loss:  0.06340695172548294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  136 | Train Loss:  0.06844116747379303 | Validation Loss:  0.06318584084510803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  137 | Train Loss:  0.06821443885564804 | Validation Loss:  0.06296567618846893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  138 | Train Loss:  0.06798870861530304 | Validation Loss:  0.06274648755788803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  139 | Train Loss:  0.06776394695043564 | Validation Loss:  0.06252825260162354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  140 | Train Loss:  0.06754013895988464 | Validation Loss:  0.06231097877025604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  141 | Train Loss:  0.06731730699539185 | Validation Loss:  0.062094636261463165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  142 | Train Loss:  0.06709542870521545 | Validation Loss:  0.061879247426986694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  143 | Train Loss:  0.06687452644109726 | Validation Loss:  0.061664801090955734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  144 | Train Loss:  0.06665455549955368 | Validation Loss:  0.061451297253370285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  145 | Train Loss:  0.06643553823232651 | Validation Loss:  0.06123870983719826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  146 | Train Loss:  0.06621747463941574 | Validation Loss:  0.06102706119418144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  147 | Train Loss:  0.06600033491849899 | Validation Loss:  0.06081633269786835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  148 | Train Loss:  0.06578414887189865 | Validation Loss:  0.06060653552412987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  149 | Train Loss:  0.06556889414787292 | Validation Loss:  0.06039764732122421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  150 | Train Loss:  0.06535454839468002 | Validation Loss:  0.060189660638570786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  151 | Train Loss:  0.06514114886522293 | Validation Loss:  0.05998259782791138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  152 | Train Loss:  0.06492865085601807 | Validation Loss:  0.0597764253616333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  153 | Train Loss:  0.06471708416938782 | Validation Loss:  0.05957116559147835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  154 | Train Loss:  0.0645064264535904 | Validation Loss:  0.05936680734157562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  155 | Train Loss:  0.0642966702580452 | Validation Loss:  0.05916333198547363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  156 | Train Loss:  0.06408783048391342 | Validation Loss:  0.058960746973752975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  157 | Train Loss:  0.06387987732887268 | Validation Loss:  0.058759044855833054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  158 | Train Loss:  0.06367283314466476 | Validation Loss:  0.058558229357004166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  159 | Train Loss:  0.06346666812896729 | Validation Loss:  0.058358289301395416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  160 | Train Loss:  0.06326139718294144 | Validation Loss:  0.05815922096371651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  161 | Train Loss:  0.06305702030658722 | Validation Loss:  0.05796102061867714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  162 | Train Loss:  0.06285351514816284 | Validation Loss:  0.05776369199156761\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  163 | Train Loss:  0.0626508966088295 | Validation Loss:  0.05756722390651703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  164 | Train Loss:  0.0624491386115551 | Validation Loss:  0.057371605187654495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  165 | Train Loss:  0.06224825978279114 | Validation Loss:  0.0571768581867218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  166 | Train Loss:  0.06204824522137642 | Validation Loss:  0.05698295310139656\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  167 | Train Loss:  0.061849091202020645 | Validation Loss:  0.05678989738225937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  168 | Train Loss:  0.061650797724723816 | Validation Loss:  0.05659767985343933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  169 | Train Loss:  0.061453357338905334 | Validation Loss:  0.05640631169080734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  170 | Train Loss:  0.0612567737698555 | Validation Loss:  0.05621577054262161\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  171 | Train Loss:  0.06106103956699371 | Validation Loss:  0.05602606385946274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  172 | Train Loss:  0.060866136103868484 | Validation Loss:  0.05583719164133072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  173 | Train Loss:  0.060672082006931305 | Validation Loss:  0.05564913898706436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  174 | Train Loss:  0.06047885864973068 | Validation Loss:  0.05546191707253456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  175 | Train Loss:  0.060286473482847214 | Validation Loss:  0.05527550354599953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  176 | Train Loss:  0.0600949190557003 | Validation Loss:  0.055089905858039856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  177 | Train Loss:  0.05990418791770935 | Validation Loss:  0.05490512400865555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  178 | Train Loss:  0.05971427634358406 | Validation Loss:  0.054721150547266006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  179 | Train Loss:  0.05952519178390503 | Validation Loss:  0.054537974298000336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  180 | Train Loss:  0.059336915612220764 | Validation Loss:  0.05435560643672943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  181 | Train Loss:  0.059149451553821564 | Validation Loss:  0.0541740283370018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  182 | Train Loss:  0.05896279588341713 | Validation Loss:  0.05399325117468834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  183 | Train Loss:  0.05877695232629776 | Validation Loss:  0.05381326377391815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  184 | Train Loss:  0.058591898530721664 | Validation Loss:  0.05363406240940094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  185 | Train Loss:  0.05840764939785004 | Validation Loss:  0.053455643355846405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  186 | Train Loss:  0.05822419747710228 | Validation Loss:  0.05327799543738365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  187 | Train Loss:  0.0580415241420269 | Validation Loss:  0.05310113728046417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  188 | Train Loss:  0.057859644293785095 | Validation Loss:  0.05292504280805588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  189 | Train Loss:  0.05767855793237686 | Validation Loss:  0.05274972692131996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  190 | Train Loss:  0.05749824270606041 | Validation Loss:  0.05257517471909523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  191 | Train Loss:  0.057318709790706635 | Validation Loss:  0.05240138620138168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  192 | Train Loss:  0.05713995546102524 | Validation Loss:  0.05222835764288902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  193 | Train Loss:  0.05696195736527443 | Validation Loss:  0.05205608531832695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  194 | Train Loss:  0.0567847341299057 | Validation Loss:  0.05188456550240517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  195 | Train Loss:  0.05660827085375786 | Validation Loss:  0.051713794469833374\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  196 | Train Loss:  0.0564325787127018 | Validation Loss:  0.05154377594590187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  197 | Train Loss:  0.05625763535499573 | Validation Loss:  0.051374491304159164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  198 | Train Loss:  0.056083451956510544 | Validation Loss:  0.05120595544576645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  199 | Train Loss:  0.05591001734137535 | Validation Loss:  0.05103815346956253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  200 | Train Loss:  0.05573732405900955 | Validation Loss:  0.05087108910083771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  201 | Train Loss:  0.05556538701057434 | Validation Loss:  0.050704747438430786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  202 | Train Loss:  0.05539418011903763 | Validation Loss:  0.05053913593292236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  203 | Train Loss:  0.05522371828556061 | Validation Loss:  0.05037425458431244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  204 | Train Loss:  0.05505399405956268 | Validation Loss:  0.050210095942020416\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  205 | Train Loss:  0.05488499253988266 | Validation Loss:  0.0500466525554657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  206 | Train Loss:  0.054716724902391434 | Validation Loss:  0.049883920699357986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  207 | Train Loss:  0.05454917997121811 | Validation Loss:  0.04972190037369728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  208 | Train Loss:  0.054382361471652985 | Validation Loss:  0.04956059157848358\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  209 | Train Loss:  0.054216258227825165 | Validation Loss:  0.04939999058842659\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  210 | Train Loss:  0.05405087396502495 | Validation Loss:  0.04924008995294571\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  211 | Train Loss:  0.05388620123267174 | Validation Loss:  0.04908088967204094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  212 | Train Loss:  0.053722232580184937 | Validation Loss:  0.04892238229513168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  213 | Train Loss:  0.05355897918343544 | Validation Loss:  0.04876456782221794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  214 | Train Loss:  0.053396426141262054 | Validation Loss:  0.048607442528009415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  215 | Train Loss:  0.05323457717895508 | Validation Loss:  0.0484510138630867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  216 | Train Loss:  0.053073421120643616 | Validation Loss:  0.048295266926288605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  217 | Train Loss:  0.052912961691617966 | Validation Loss:  0.048140205442905426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  218 | Train Loss:  0.05275319516658783 | Validation Loss:  0.04798581451177597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  219 | Train Loss:  0.05259411782026291 | Validation Loss:  0.04783210530877113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  220 | Train Loss:  0.052435725927352905 | Validation Loss:  0.04767905920743942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  221 | Train Loss:  0.05227801948785782 | Validation Loss:  0.04752668738365173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  222 | Train Loss:  0.05212099105119705 | Validation Loss:  0.047374989837408066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  223 | Train Loss:  0.05196463316679001 | Validation Loss:  0.04722394421696663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  224 | Train Loss:  0.051808957010507584 | Validation Loss:  0.047073569148778915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  225 | Train Loss:  0.05165394768118858 | Validation Loss:  0.04692385345697403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  226 | Train Loss:  0.051499612629413605 | Validation Loss:  0.046774789690971375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  227 | Train Loss:  0.051345936954021454 | Validation Loss:  0.04662637785077095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  228 | Train Loss:  0.05119292065501213 | Validation Loss:  0.04647861048579216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  229 | Train Loss:  0.05104057118296623 | Validation Loss:  0.0463314987719059\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  230 | Train Loss:  0.050888873636722565 | Validation Loss:  0.046185027807950974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  231 | Train Loss:  0.050737831741571426 | Validation Loss:  0.04603920131921768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  232 | Train Loss:  0.050587449222803116 | Validation Loss:  0.04589400812983513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  233 | Train Loss:  0.05043770745396614 | Validation Loss:  0.04574945196509361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  234 | Train Loss:  0.0502886064350605 | Validation Loss:  0.045605529099702835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  235 | Train Loss:  0.050140149891376495 | Validation Loss:  0.0454622358083725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  236 | Train Loss:  0.04999234154820442 | Validation Loss:  0.0453195720911026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  237 | Train Loss:  0.04984516650438309 | Validation Loss:  0.045177530497312546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  238 | Train Loss:  0.04969862475991249 | Validation Loss:  0.04503611475229263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  239 | Train Loss:  0.049552712589502335 | Validation Loss:  0.044895317405462265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  240 | Train Loss:  0.04940743371844292 | Validation Loss:  0.04475513473153114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  241 | Train Loss:  0.049262773245573044 | Validation Loss:  0.04461556673049927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  242 | Train Loss:  0.04911874979734421 | Validation Loss:  0.04447660967707634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  243 | Train Loss:  0.04897534102201462 | Validation Loss:  0.04433826357126236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  244 | Train Loss:  0.048832543194293976 | Validation Loss:  0.04420051723718643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  245 | Train Loss:  0.048690371215343475 | Validation Loss:  0.04406338185071945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  246 | Train Loss:  0.048548806458711624 | Validation Loss:  0.043926842510700226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  247 | Train Loss:  0.04840785264968872 | Validation Loss:  0.04379090294241905\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  248 | Train Loss:  0.04826750606298447 | Validation Loss:  0.043655555695295334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  249 | Train Loss:  0.04812776669859886 | Validation Loss:  0.04352080821990967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  250 | Train Loss:  0.04798862710595131 | Validation Loss:  0.04338664188981056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  251 | Train Loss:  0.047850094735622406 | Validation Loss:  0.04325307160615921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  252 | Train Loss:  0.04771215096116066 | Validation Loss:  0.04312007874250412\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  253 | Train Loss:  0.047574806958436966 | Validation Loss:  0.042987674474716187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  254 | Train Loss:  0.04743804410099983 | Validation Loss:  0.042855847626924515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  255 | Train Loss:  0.04730188846588135 | Validation Loss:  0.042724598199129105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  256 | Train Loss:  0.047166306525468826 | Validation Loss:  0.042593929916620255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  257 | Train Loss:  0.04703130945563316 | Validation Loss:  0.04246382415294647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  258 | Train Loss:  0.046896904706954956 | Validation Loss:  0.04233429580926895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  259 | Train Loss:  0.04676307365298271 | Validation Loss:  0.042205337435007095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  260 | Train Loss:  0.04662981256842613 | Validation Loss:  0.04207694157958031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  261 | Train Loss:  0.04649713635444641 | Validation Loss:  0.04194910079240799\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  262 | Train Loss:  0.046365026384592056 | Validation Loss:  0.04182182624936104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  263 | Train Loss:  0.046233486384153366 | Validation Loss:  0.04169511795043945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  264 | Train Loss:  0.04610251262784004 | Validation Loss:  0.041568953543901443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  265 | Train Loss:  0.04597210884094238 | Validation Loss:  0.041443340480327606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  266 | Train Loss:  0.045842260122299194 | Validation Loss:  0.04131828993558884\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  267 | Train Loss:  0.045712973922491074 | Validation Loss:  0.041193775832653046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  268 | Train Loss:  0.04558425024151802 | Validation Loss:  0.04106981307268143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  269 | Train Loss:  0.04545607045292854 | Validation Loss:  0.040946390479803085\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  270 | Train Loss:  0.045328449457883835 | Validation Loss:  0.04082351550459862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  271 | Train Loss:  0.0452013798058033 | Validation Loss:  0.04070117324590683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  272 | Train Loss:  0.04507485777139664 | Validation Loss:  0.04057937115430832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  273 | Train Loss:  0.04494888335466385 | Validation Loss:  0.04045809805393219\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  274 | Train Loss:  0.04482344910502434 | Validation Loss:  0.04033736512064934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  275 | Train Loss:  0.044698555022478104 | Validation Loss:  0.04021715372800827\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  276 | Train Loss:  0.04457419365644455 | Validation Loss:  0.040097471326589584\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  277 | Train Loss:  0.04445037990808487 | Validation Loss:  0.03997831419110298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  278 | Train Loss:  0.04432709142565727 | Validation Loss:  0.03985968232154846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  279 | Train Loss:  0.044204339385032654 | Validation Loss:  0.03974156826734543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  280 | Train Loss:  0.04408211261034012 | Validation Loss:  0.03962397202849388\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  281 | Train Loss:  0.043960414826869965 | Validation Loss:  0.03950688987970352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  282 | Train Loss:  0.04383924603462219 | Validation Loss:  0.03939032554626465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  283 | Train Loss:  0.04371858760714531 | Validation Loss:  0.039274267852306366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  284 | Train Loss:  0.043598465621471405 | Validation Loss:  0.03915872424840927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  285 | Train Loss:  0.04347884654998779 | Validation Loss:  0.03904368355870247\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  286 | Train Loss:  0.043359752744436264 | Validation Loss:  0.03892914950847626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  287 | Train Loss:  0.043241169303655624 | Validation Loss:  0.03881511464715004\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  288 | Train Loss:  0.04312308877706528 | Validation Loss:  0.03870158642530441\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  289 | Train Loss:  0.04300552234053612 | Validation Loss:  0.03858854994177818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  290 | Train Loss:  0.042888473719358444 | Validation Loss:  0.03847601264715195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  291 | Train Loss:  0.04277191311120987 | Validation Loss:  0.03836396336555481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  292 | Train Loss:  0.04265586659312248 | Validation Loss:  0.038252416998147964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  293 | Train Loss:  0.04254031553864479 | Validation Loss:  0.03814135491847992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  294 | Train Loss:  0.04242526739835739 | Validation Loss:  0.038030773401260376\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  295 | Train Loss:  0.04231071099638939 | Validation Loss:  0.03792068362236023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  296 | Train Loss:  0.042196646332740784 | Validation Loss:  0.037811074405908585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  297 | Train Loss:  0.042083073407411575 | Validation Loss:  0.03770194947719574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  298 | Train Loss:  0.041969992220401764 | Validation Loss:  0.0375933013856411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  299 | Train Loss:  0.04185740277171135 | Validation Loss:  0.03748513013124466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  300 | Train Loss:  0.04174529016017914 | Validation Loss:  0.037377435714006424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  301 | Train Loss:  0.041633669286966324 | Validation Loss:  0.037270206958055496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  302 | Train Loss:  0.04152252525091171 | Validation Loss:  0.03716345876455307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  303 | Train Loss:  0.041411854326725006 | Validation Loss:  0.037057168781757355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  304 | Train Loss:  0.0413016676902771 | Validation Loss:  0.03695135563611984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  305 | Train Loss:  0.0411919541656971 | Validation Loss:  0.03684599697589874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  306 | Train Loss:  0.0410827174782753 | Validation Loss:  0.03674110770225525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  307 | Train Loss:  0.04097395017743111 | Validation Loss:  0.036636680364608765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  308 | Train Loss:  0.04086564481258392 | Validation Loss:  0.03653270751237869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  309 | Train Loss:  0.04075781628489494 | Validation Loss:  0.03642918914556503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  310 | Train Loss:  0.040650442242622375 | Validation Loss:  0.03632613271474838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  311 | Train Loss:  0.04054354503750801 | Validation Loss:  0.03622352331876755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  312 | Train Loss:  0.04043709859251976 | Validation Loss:  0.03612136468291283\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  313 | Train Loss:  0.04033111035823822 | Validation Loss:  0.03601965680718422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  314 | Train Loss:  0.04022558033466339 | Validation Loss:  0.03591839596629143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  315 | Train Loss:  0.04012050852179527 | Validation Loss:  0.03581757843494415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  316 | Train Loss:  0.04001588374376297 | Validation Loss:  0.035717204213142395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  317 | Train Loss:  0.03991171717643738 | Validation Loss:  0.035617269575595856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  318 | Train Loss:  0.0398079939186573 | Validation Loss:  0.035517774522304535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  319 | Train Loss:  0.03970471769571304 | Validation Loss:  0.035418715327978134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  320 | Train Loss:  0.0396018885076046 | Validation Loss:  0.03532009199261665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  321 | Train Loss:  0.03949950635433197 | Validation Loss:  0.035221900790929794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  322 | Train Loss:  0.03939756006002426 | Validation Loss:  0.035124145448207855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  323 | Train Loss:  0.03929605334997177 | Validation Loss:  0.03502681106328964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  324 | Train Loss:  0.0391949899494648 | Validation Loss:  0.03492990881204605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  325 | Train Loss:  0.03909435123205185 | Validation Loss:  0.03483343869447708\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  326 | Train Loss:  0.038994159549474716 | Validation Loss:  0.03473738580942154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  327 | Train Loss:  0.03889438882470131 | Validation Loss:  0.034641753882169724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  328 | Train Loss:  0.03879505395889282 | Validation Loss:  0.03454654663801193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  329 | Train Loss:  0.03869615122675896 | Validation Loss:  0.03445175290107727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  330 | Train Loss:  0.03859766945242882 | Validation Loss:  0.034357380121946335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  331 | Train Loss:  0.038499608635902405 | Validation Loss:  0.03426341712474823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  332 | Train Loss:  0.038401972502470016 | Validation Loss:  0.034169867634773254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  333 | Train Loss:  0.03830476105213165 | Validation Loss:  0.03407673537731171\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  334 | Train Loss:  0.03820796683430672 | Validation Loss:  0.03398401290178299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  335 | Train Loss:  0.03811158984899521 | Validation Loss:  0.03389168903231621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  336 | Train Loss:  0.03801562637090683 | Validation Loss:  0.03379977494478226\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  337 | Train Loss:  0.03792008012533188 | Validation Loss:  0.03370826691389084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  338 | Train Loss:  0.03782493993639946 | Validation Loss:  0.033617161214351654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  339 | Train Loss:  0.03773021325469017 | Validation Loss:  0.033526454120874405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  340 | Train Loss:  0.03763590008020401 | Validation Loss:  0.03343614563345909\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  341 | Train Loss:  0.037541989237070084 | Validation Loss:  0.033346232026815414\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  342 | Train Loss:  0.03744848072528839 | Validation Loss:  0.03325672075152397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  343 | Train Loss:  0.03735537827014923 | Validation Loss:  0.03316759690642357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  344 | Train Loss:  0.037262674421072006 | Validation Loss:  0.0330788679420948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  345 | Train Loss:  0.03717036917805672 | Validation Loss:  0.03299052268266678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  346 | Train Loss:  0.03707846626639366 | Validation Loss:  0.03290257230401039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  347 | Train Loss:  0.03698695823550224 | Validation Loss:  0.032815005630254745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  348 | Train Loss:  0.03689584508538246 | Validation Loss:  0.03272783383727074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  349 | Train Loss:  0.03680511936545372 | Validation Loss:  0.032641034573316574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  350 | Train Loss:  0.036714788526296616 | Validation Loss:  0.03255462273955345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  351 | Train Loss:  0.03662485256791115 | Validation Loss:  0.032468587160110474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  352 | Train Loss:  0.036535296589136124 | Validation Loss:  0.03238293156027794\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  353 | Train Loss:  0.03644613176584244 | Validation Loss:  0.03229765221476555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  354 | Train Loss:  0.036357346922159195 | Validation Loss:  0.032212745398283005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  355 | Train Loss:  0.03626894950866699 | Validation Loss:  0.032128214836120605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  356 | Train Loss:  0.03618093207478523 | Validation Loss:  0.03204406052827835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  357 | Train Loss:  0.03609328716993332 | Validation Loss:  0.03196026757359505\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  358 | Train Loss:  0.03600602596998215 | Validation Loss:  0.03187685087323189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  359 | Train Loss:  0.03591913729906082 | Validation Loss:  0.03179379925131798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  360 | Train Loss:  0.03583262860774994 | Validation Loss:  0.03171111270785332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  361 | Train Loss:  0.0357464924454689 | Validation Loss:  0.03162878751754761\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  362 | Train Loss:  0.035660721361637115 | Validation Loss:  0.03154681995511055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  363 | Train Loss:  0.03557532653212547 | Validation Loss:  0.03146522864699364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  364 | Train Loss:  0.03549029305577278 | Validation Loss:  0.03138398006558418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  365 | Train Loss:  0.035405635833740234 | Validation Loss:  0.031303100287914276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  366 | Train Loss:  0.03532133623957634 | Validation Loss:  0.031222568824887276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  367 | Train Loss:  0.035237401723861694 | Validation Loss:  0.031142396852374077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  368 | Train Loss:  0.035153828561306 | Validation Loss:  0.031062576919794083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  369 | Train Loss:  0.035070616751909256 | Validation Loss:  0.030983109027147293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  370 | Train Loss:  0.034987758845090866 | Validation Loss:  0.03090398944914341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  371 | Train Loss:  0.034905266016721725 | Validation Loss:  0.030825220048427582\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  372 | Train Loss:  0.03482312336564064 | Validation Loss:  0.03074679523706436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  373 | Train Loss:  0.03474133834242821 | Validation Loss:  0.0306687168776989\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  374 | Train Loss:  0.03465990722179413 | Validation Loss:  0.030590981245040894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  375 | Train Loss:  0.03457881510257721 | Validation Loss:  0.030513588339090347\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  376 | Train Loss:  0.03449808061122894 | Validation Loss:  0.03043653443455696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  377 | Train Loss:  0.03441770002245903 | Validation Loss:  0.030359826982021332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  378 | Train Loss:  0.03433765843510628 | Validation Loss:  0.030283447355031967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  379 | Train Loss:  0.03425795957446098 | Validation Loss:  0.030207408592104912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  380 | Train Loss:  0.03417861461639404 | Validation Loss:  0.030131705105304718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  381 | Train Loss:  0.034099604934453964 | Validation Loss:  0.030056335031986237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  382 | Train Loss:  0.034020934253931046 | Validation Loss:  0.029981296509504318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  383 | Train Loss:  0.033942606300115585 | Validation Loss:  0.029906591400504112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  384 | Train Loss:  0.03386461362242699 | Validation Loss:  0.02983221411705017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  385 | Train Loss:  0.03378695622086525 | Validation Loss:  0.029758162796497345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  386 | Train Loss:  0.03370963782072067 | Validation Loss:  0.029684437438845634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  387 | Train Loss:  0.03363265097141266 | Validation Loss:  0.02961103431880474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  388 | Train Loss:  0.03355599194765091 | Validation Loss:  0.029537959024310112\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  389 | Train Loss:  0.03347966820001602 | Validation Loss:  0.0294652059674263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  390 | Train Loss:  0.0334036722779274 | Validation Loss:  0.029392769560217857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  391 | Train Loss:  0.03332800045609474 | Validation Loss:  0.02932065725326538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  392 | Train Loss:  0.03325266018509865 | Validation Loss:  0.029248856008052826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  393 | Train Loss:  0.03317764401435852 | Validation Loss:  0.029177380725741386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  394 | Train Loss:  0.03310294821858406 | Validation Loss:  0.02910621091723442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  395 | Train Loss:  0.03302857652306557 | Validation Loss:  0.029035359621047974\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  396 | Train Loss:  0.03295452147722244 | Validation Loss:  0.028964821249246597\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  397 | Train Loss:  0.032880790531635284 | Validation Loss:  0.028894592076539993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  398 | Train Loss:  0.032807379961013794 | Validation Loss:  0.02882467210292816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  399 | Train Loss:  0.032734278589487076 | Validation Loss:  0.028755061328411102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  400 | Train Loss:  0.03266149386763573 | Validation Loss:  0.028685759752988815\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  401 | Train Loss:  0.03258902579545975 | Validation Loss:  0.028616759926080704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  402 | Train Loss:  0.03251686692237854 | Validation Loss:  0.028548061847686768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  403 | Train Loss:  0.032445017248392105 | Validation Loss:  0.028479672968387604\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  404 | Train Loss:  0.03237348049879074 | Validation Loss:  0.028411582112312317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  405 | Train Loss:  0.03230225667357445 | Validation Loss:  0.028343794867396355\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  406 | Train Loss:  0.03223133459687233 | Validation Loss:  0.02827630005776882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  407 | Train Loss:  0.03216071426868439 | Validation Loss:  0.028209106996655464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  408 | Train Loss:  0.032090406864881516 | Validation Loss:  0.028142211958765984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  409 | Train Loss:  0.03202039748430252 | Validation Loss:  0.02807561308145523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  410 | Train Loss:  0.0319506861269474 | Validation Loss:  0.02800929918885231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  411 | Train Loss:  0.031881287693977356 | Validation Loss:  0.027943283319473267\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  412 | Train Loss:  0.03181217983365059 | Validation Loss:  0.02787756361067295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  413 | Train Loss:  0.0317433662712574 | Validation Loss:  0.027812127023935318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  414 | Train Loss:  0.031674858182668686 | Validation Loss:  0.027746984735131264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  415 | Train Loss:  0.03160664066672325 | Validation Loss:  0.027682125568389893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  416 | Train Loss:  0.0315387137234211 | Validation Loss:  0.027617551386356354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  417 | Train Loss:  0.03147108852863312 | Validation Loss:  0.027553262189030647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  418 | Train Loss:  0.03140375018119812 | Validation Loss:  0.02748926170170307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  419 | Train Loss:  0.031336698681116104 | Validation Loss:  0.02742554061114788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  420 | Train Loss:  0.03126993775367737 | Validation Loss:  0.027362097054719925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  421 | Train Loss:  0.03120346926152706 | Validation Loss:  0.02729894034564495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  422 | Train Loss:  0.03113728202879429 | Validation Loss:  0.027236059308052063\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  423 | Train Loss:  0.031071383506059647 | Validation Loss:  0.02717345394194126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  424 | Train Loss:  0.03100576438009739 | Validation Loss:  0.027111124247312546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  425 | Train Loss:  0.030940430238842964 | Validation Loss:  0.027049075812101364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  426 | Train Loss:  0.03087538294494152 | Validation Loss:  0.02698729932308197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  427 | Train Loss:  0.030810613185167313 | Validation Loss:  0.026925791054964066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  428 | Train Loss:  0.03074611909687519 | Validation Loss:  0.026864556595683098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  429 | Train Loss:  0.030681906268000603 | Validation Loss:  0.026803595945239067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  430 | Train Loss:  0.03061796724796295 | Validation Loss:  0.026742901653051376\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  431 | Train Loss:  0.030554307624697685 | Validation Loss:  0.026682471856474876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  432 | Train Loss:  0.030490918084979057 | Validation Loss:  0.026622310280799866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  433 | Train Loss:  0.030427806079387665 | Validation Loss:  0.026562418788671494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  434 | Train Loss:  0.03036496601998806 | Validation Loss:  0.026502784341573715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  435 | Train Loss:  0.030302390456199646 | Validation Loss:  0.026443423703312874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  436 | Train Loss:  0.03024008870124817 | Validation Loss:  0.026384318247437477\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  437 | Train Loss:  0.03017805516719818 | Validation Loss:  0.026325475424528122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  438 | Train Loss:  0.030116289854049683 | Validation Loss:  0.02626689150929451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  439 | Train Loss:  0.030054792761802673 | Validation Loss:  0.02620857022702694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  440 | Train Loss:  0.029993560165166855 | Validation Loss:  0.026150500401854515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  441 | Train Loss:  0.029932590201497078 | Validation Loss:  0.026092689484357834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  442 | Train Loss:  0.029871879145503044 | Validation Loss:  0.026035135611891747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  443 | Train Loss:  0.0298114325851202 | Validation Loss:  0.025977838784456253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  444 | Train Loss:  0.0297512486577034 | Validation Loss:  0.025920791551470757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  445 | Train Loss:  0.02969132363796234 | Validation Loss:  0.02586399018764496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  446 | Train Loss:  0.029631655663251877 | Validation Loss:  0.025807449594140053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  447 | Train Loss:  0.029572246596217155 | Validation Loss:  0.025751154869794846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  448 | Train Loss:  0.02951308898627758 | Validation Loss:  0.025695107877254486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  449 | Train Loss:  0.029454190284013748 | Validation Loss:  0.025639314204454422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  450 | Train Loss:  0.02939554676413536 | Validation Loss:  0.02558375895023346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  451 | Train Loss:  0.02933715470135212 | Validation Loss:  0.025528457015752792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  452 | Train Loss:  0.029279012233018875 | Validation Loss:  0.025473397225141525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  453 | Train Loss:  0.029221121221780777 | Validation Loss:  0.02541857771575451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  454 | Train Loss:  0.029163481667637825 | Validation Loss:  0.025363998487591743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  455 | Train Loss:  0.02910608984529972 | Validation Loss:  0.025309670716524124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  456 | Train Loss:  0.029048945754766464 | Validation Loss:  0.02525557205080986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  457 | Train Loss:  0.028992043808102608 | Validation Loss:  0.025201717391610146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  458 | Train Loss:  0.028935391455888748 | Validation Loss:  0.02514810673892498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  459 | Train Loss:  0.028878984972834587 | Validation Loss:  0.02509472705423832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  460 | Train Loss:  0.02882281504571438 | Validation Loss:  0.025041580200195312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  461 | Train Loss:  0.028766896575689316 | Validation Loss:  0.024988675490021706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  462 | Train Loss:  0.028711210936307907 | Validation Loss:  0.024935999885201454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  463 | Train Loss:  0.0286557674407959 | Validation Loss:  0.024883558973670006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  464 | Train Loss:  0.02860056422650814 | Validation Loss:  0.024831349030137062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  465 | Train Loss:  0.028545599430799484 | Validation Loss:  0.02477937564253807\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  466 | Train Loss:  0.02849087119102478 | Validation Loss:  0.024727623909711838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  467 | Train Loss:  0.02843637764453888 | Validation Loss:  0.02467610314488411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  468 | Train Loss:  0.028382116928696632 | Validation Loss:  0.024624817073345184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  469 | Train Loss:  0.028328094631433487 | Validation Loss:  0.02457374893128872\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  470 | Train Loss:  0.028274301439523697 | Validation Loss:  0.02452290989458561\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  471 | Train Loss:  0.02822074107825756 | Validation Loss:  0.024472299963235855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  472 | Train Loss:  0.02816741354763508 | Validation Loss:  0.02442191168665886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  473 | Train Loss:  0.028114311397075653 | Validation Loss:  0.024371741339564323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  474 | Train Loss:  0.02806144207715988 | Validation Loss:  0.024321798235177994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  475 | Train Loss:  0.028008801862597466 | Validation Loss:  0.024272074922919273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  476 | Train Loss:  0.027956388890743256 | Validation Loss:  0.02422257512807846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  477 | Train Loss:  0.027904197573661804 | Validation Loss:  0.02417328767478466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  478 | Train Loss:  0.02785222977399826 | Validation Loss:  0.02412422187626362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  479 | Train Loss:  0.027800491079688072 | Validation Loss:  0.024075374007225037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  480 | Train Loss:  0.027748972177505493 | Validation Loss:  0.024026744067668915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  481 | Train Loss:  0.02769767865538597 | Validation Loss:  0.023978326469659805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  482 | Train Loss:  0.027646608650684357 | Validation Loss:  0.023930123075842857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  483 | Train Loss:  0.027595750987529755 | Validation Loss:  0.02388213761150837\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  484 | Train Loss:  0.02754511684179306 | Validation Loss:  0.023834360763430595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  485 | Train Loss:  0.027494700625538826 | Validation Loss:  0.023786794394254684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  486 | Train Loss:  0.027444500476121902 | Validation Loss:  0.023739440366625786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  487 | Train Loss:  0.02739451825618744 | Validation Loss:  0.02369229681789875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  488 | Train Loss:  0.027344748377799988 | Validation Loss:  0.02364536188542843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  489 | Train Loss:  0.027295196428894997 | Validation Loss:  0.023598631843924522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  490 | Train Loss:  0.027245858684182167 | Validation Loss:  0.02355211414396763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  491 | Train Loss:  0.027196725830435753 | Validation Loss:  0.023505799472332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  492 | Train Loss:  0.027147812768816948 | Validation Loss:  0.02345968969166279\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  493 | Train Loss:  0.027099110186100006 | Validation Loss:  0.02341378480195999\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  494 | Train Loss:  0.02705061435699463 | Validation Loss:  0.02336808480322361\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  495 | Train Loss:  0.027002330869436264 | Validation Loss:  0.023322585970163345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  496 | Train Loss:  0.026954254135489464 | Validation Loss:  0.023277288302779198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  497 | Train Loss:  0.02690638229250908 | Validation Loss:  0.023232193663716316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  498 | Train Loss:  0.026858719065785408 | Validation Loss:  0.023187294602394104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  499 | Train Loss:  0.026811258867383003 | Validation Loss:  0.02314259670674801\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  500 | Train Loss:  0.026764005422592163 | Validation Loss:  0.023098096251487732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  501 | Train Loss:  0.026716960594058037 | Validation Loss:  0.023053796961903572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  502 | Train Loss:  0.02667010948061943 | Validation Loss:  0.023009691387414932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  503 | Train Loss:  0.026623468846082687 | Validation Loss:  0.02296578139066696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  504 | Train Loss:  0.026577023789286613 | Validation Loss:  0.02292206697165966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  505 | Train Loss:  0.026530776172876358 | Validation Loss:  0.022878548130393028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  506 | Train Loss:  0.026484733447432518 | Validation Loss:  0.02283521741628647\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  507 | Train Loss:  0.026438886299729347 | Validation Loss:  0.022792082279920578\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  508 | Train Loss:  0.026393240317702293 | Validation Loss:  0.02274913713335991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  509 | Train Loss:  0.02634778991341591 | Validation Loss:  0.02270638197660446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  510 | Train Loss:  0.026302531361579895 | Validation Loss:  0.022663820534944534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  511 | Train Loss:  0.0262574702501297 | Validation Loss:  0.02262144349515438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  512 | Train Loss:  0.026212606579065323 | Validation Loss:  0.02257925644516945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  513 | Train Loss:  0.02616792917251587 | Validation Loss:  0.02253725565969944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  514 | Train Loss:  0.026123449206352234 | Validation Loss:  0.022495439276099205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  515 | Train Loss:  0.02607916109263897 | Validation Loss:  0.022453811019659042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  516 | Train Loss:  0.026035062968730927 | Validation Loss:  0.022412369027733803\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  517 | Train Loss:  0.025991156697273254 | Validation Loss:  0.022371111437678337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  518 | Train Loss:  0.025947434827685356 | Validation Loss:  0.022330032661557198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  519 | Train Loss:  0.025903906673192978 | Validation Loss:  0.022289138287305832\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  520 | Train Loss:  0.025860562920570374 | Validation Loss:  0.022248422726988792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  521 | Train Loss:  0.02581740915775299 | Validation Loss:  0.022207891568541527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  522 | Train Loss:  0.025774436071515083 | Validation Loss:  0.022167539224028587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  523 | Train Loss:  0.025731651112437248 | Validation Loss:  0.022127367556095123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  524 | Train Loss:  0.025689054280519485 | Validation Loss:  0.022087369114160538\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  525 | Train Loss:  0.02564663253724575 | Validation Loss:  0.022047553211450577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  526 | Train Loss:  0.025604402646422386 | Validation Loss:  0.022007916122674942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  527 | Train Loss:  0.02556234784424305 | Validation Loss:  0.021968450397253036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  528 | Train Loss:  0.025520483031868935 | Validation Loss:  0.021929161623120308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  529 | Train Loss:  0.025478791445493698 | Validation Loss:  0.021890046074986458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  530 | Train Loss:  0.025437278673052788 | Validation Loss:  0.021851105615496635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  531 | Train Loss:  0.025395946577191353 | Validation Loss:  0.02181233838200569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  532 | Train Loss:  0.025354793295264244 | Validation Loss:  0.021773744374513626\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  533 | Train Loss:  0.02531381882727146 | Validation Loss:  0.02173532173037529\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  534 | Train Loss:  0.025273021310567856 | Validation Loss:  0.021697066724300385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  535 | Train Loss:  0.02523239515721798 | Validation Loss:  0.02165898308157921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  536 | Train Loss:  0.02519194968044758 | Validation Loss:  0.021621068939566612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  537 | Train Loss:  0.025151675567030907 | Validation Loss:  0.021583324298262596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  538 | Train Loss:  0.025111576542258263 | Validation Loss:  0.02154574915766716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  539 | Train Loss:  0.025071648880839348 | Validation Loss:  0.021508337929844856\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  540 | Train Loss:  0.025031890720129013 | Validation Loss:  0.021471092477440834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  541 | Train Loss:  0.024992309510707855 | Validation Loss:  0.021434014663100243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  542 | Train Loss:  0.02495289407670498 | Validation Loss:  0.021397102624177933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  543 | Train Loss:  0.02491365186870098 | Validation Loss:  0.021360352635383606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  544 | Train Loss:  0.024874577298760414 | Validation Loss:  0.021323764696717262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  545 | Train Loss:  0.024835674092173576 | Validation Loss:  0.02128734067082405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  546 | Train Loss:  0.02479693479835987 | Validation Loss:  0.021251078695058823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  547 | Train Loss:  0.024758361279964447 | Validation Loss:  0.021214980632066727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  548 | Train Loss:  0.0247199609875679 | Validation Loss:  0.021179039031267166\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  549 | Train Loss:  0.02468171715736389 | Validation Loss:  0.02114325948059559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  550 | Train Loss:  0.02464364655315876 | Validation Loss:  0.021107640117406845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  551 | Train Loss:  0.024605736136436462 | Validation Loss:  0.021072175353765488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  552 | Train Loss:  0.024567987769842148 | Validation Loss:  0.021036872640252113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  553 | Train Loss:  0.024530401453375816 | Validation Loss:  0.021001724526286125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  554 | Train Loss:  0.024492979049682617 | Validation Loss:  0.020966732874512672\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  555 | Train Loss:  0.02445572055876255 | Validation Loss:  0.020931897684931755\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  556 | Train Loss:  0.02441861853003502 | Validation Loss:  0.020897218957543373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  557 | Train Loss:  0.02438168041408062 | Validation Loss:  0.02086269110441208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  558 | Train Loss:  0.02434489317238331 | Validation Loss:  0.02082831785082817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  559 | Train Loss:  0.024308275431394577 | Validation Loss:  0.0207940973341465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  560 | Train Loss:  0.024271808564662933 | Validation Loss:  0.020760033279657364\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  561 | Train Loss:  0.024235503748059273 | Validation Loss:  0.020726118236780167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  562 | Train Loss:  0.0241993498057127 | Validation Loss:  0.020692355930805206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  563 | Train Loss:  0.02416335605084896 | Validation Loss:  0.020658740773797035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  564 | Train Loss:  0.02412751503288746 | Validation Loss:  0.0206252783536911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  565 | Train Loss:  0.024091830477118492 | Validation Loss:  0.020591963082551956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  566 | Train Loss:  0.024056296795606613 | Validation Loss:  0.0205587986856699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  567 | Train Loss:  0.02402091957628727 | Validation Loss:  0.020525775849819183\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  568 | Train Loss:  0.023985693231225014 | Validation Loss:  0.020492905750870705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  569 | Train Loss:  0.023950621485710144 | Validation Loss:  0.020460179075598717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  570 | Train Loss:  0.023915698751807213 | Validation Loss:  0.020427601411938667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  571 | Train Loss:  0.02388092502951622 | Validation Loss:  0.02039516530930996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  572 | Train Loss:  0.023846302181482315 | Validation Loss:  0.02036287635564804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  573 | Train Loss:  0.0238118264824152 | Validation Loss:  0.020330730825662613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  574 | Train Loss:  0.02377750352025032 | Validation Loss:  0.020298726856708527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  575 | Train Loss:  0.02374332584440708 | Validation Loss:  0.02026686631143093\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  576 | Train Loss:  0.02370929718017578 | Validation Loss:  0.020235147327184677\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  577 | Train Loss:  0.02367541380226612 | Validation Loss:  0.020203571766614914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  578 | Train Loss:  0.0236416757106781 | Validation Loss:  0.020172137767076492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  579 | Train Loss:  0.02360808476805687 | Validation Loss:  0.020140836015343666\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  580 | Train Loss:  0.02357463911175728 | Validation Loss:  0.02010968141257763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  581 | Train Loss:  0.02354133501648903 | Validation Loss:  0.020078660920262337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  582 | Train Loss:  0.02350817434489727 | Validation Loss:  0.020047783851623535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  583 | Train Loss:  0.02347516268491745 | Validation Loss:  0.020017044618725777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  584 | Train Loss:  0.023442285135388374 | Validation Loss:  0.019986437633633614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  585 | Train Loss:  0.023409554734826088 | Validation Loss:  0.019955972209572792\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  586 | Train Loss:  0.023376965895295143 | Validation Loss:  0.019925639033317566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  587 | Train Loss:  0.02334451489150524 | Validation Loss:  0.019895441830158234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  588 | Train Loss:  0.023312203586101532 | Validation Loss:  0.019865380600094795\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  589 | Train Loss:  0.023280031979084015 | Validation Loss:  0.019835451617836952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  590 | Train Loss:  0.02324799634516239 | Validation Loss:  0.019805656746029854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  591 | Train Loss:  0.02321610227227211 | Validation Loss:  0.01977599412202835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  592 | Train Loss:  0.02318434603512287 | Validation Loss:  0.019746465608477592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  593 | Train Loss:  0.023152723908424377 | Validation Loss:  0.01971707120537758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  594 | Train Loss:  0.023121241480112076 | Validation Loss:  0.019687803462147713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  595 | Train Loss:  0.02308989316225052 | Validation Loss:  0.019658667966723442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  596 | Train Loss:  0.023058680817484856 | Validation Loss:  0.019629664719104767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  597 | Train Loss:  0.023027600720524788 | Validation Loss:  0.01960078626871109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  598 | Train Loss:  0.022996656596660614 | Validation Loss:  0.01957203820347786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  599 | Train Loss:  0.022965844720602036 | Validation Loss:  0.019543420523405075\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  600 | Train Loss:  0.022935165092349052 | Validation Loss:  0.01951492950320244\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  601 | Train Loss:  0.022904621437191963 | Validation Loss:  0.0194865670055151\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  602 | Train Loss:  0.02287421002984047 | Validation Loss:  0.019458331167697906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  603 | Train Loss:  0.022843925282359123 | Validation Loss:  0.019430218264460564\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  604 | Train Loss:  0.022813772782683372 | Validation Loss:  0.019402233883738518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  605 | Train Loss:  0.022783750668168068 | Validation Loss:  0.01937437430024147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  606 | Train Loss:  0.02275385521352291 | Validation Loss:  0.01934664137661457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  607 | Train Loss:  0.02272409200668335 | Validation Loss:  0.01931902766227722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  608 | Train Loss:  0.022694459185004234 | Validation Loss:  0.01929154060781002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  609 | Train Loss:  0.02266494557261467 | Validation Loss:  0.01926417462527752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  610 | Train Loss:  0.02263556607067585 | Validation Loss:  0.019236931577324867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  611 | Train Loss:  0.02260630950331688 | Validation Loss:  0.019209809601306915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  612 | Train Loss:  0.022577183321118355 | Validation Loss:  0.019182810559868813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  613 | Train Loss:  0.02254818007349968 | Validation Loss:  0.01915592886507511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  614 | Train Loss:  0.022519301623106003 | Validation Loss:  0.01912916824221611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  615 | Train Loss:  0.022490547969937325 | Validation Loss:  0.01910253055393696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  616 | Train Loss:  0.022461917251348495 | Validation Loss:  0.01907600834965706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  617 | Train Loss:  0.022433409467339516 | Validation Loss:  0.01904960535466671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  618 | Train Loss:  0.022405028343200684 | Validation Loss:  0.01902332343161106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  619 | Train Loss:  0.02237676829099655 | Validation Loss:  0.018997156992554665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  620 | Train Loss:  0.02234862744808197 | Validation Loss:  0.01897110603749752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  621 | Train Loss:  0.022320609539747238 | Validation Loss:  0.018945172429084778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  622 | Train Loss:  0.022292712703347206 | Validation Loss:  0.018919358029961586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  623 | Train Loss:  0.022264935076236725 | Validation Loss:  0.018893655389547348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  624 | Train Loss:  0.022237276658415794 | Validation Loss:  0.018868068233132362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  625 | Train Loss:  0.022209743037819862 | Validation Loss:  0.01884259469807148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  626 | Train Loss:  0.022182321175932884 | Validation Loss:  0.018817238509655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  627 | Train Loss:  0.022155022248625755 | Validation Loss:  0.01879199407994747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  628 | Train Loss:  0.02212783694267273 | Validation Loss:  0.0187668614089489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  629 | Train Loss:  0.022100772708654404 | Validation Loss:  0.018741846084594727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  630 | Train Loss:  0.022073820233345032 | Validation Loss:  0.01871693879365921\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  631 | Train Loss:  0.02204698696732521 | Validation Loss:  0.018692143261432648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  632 | Train Loss:  0.02202026918530464 | Validation Loss:  0.01866745762526989\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  633 | Train Loss:  0.021993668749928474 | Validation Loss:  0.018642883747816086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  634 | Train Loss:  0.02196718193590641 | Validation Loss:  0.018618419766426086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  635 | Train Loss:  0.02194080874323845 | Validation Loss:  0.01859406754374504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  636 | Train Loss:  0.021914545446634293 | Validation Loss:  0.018569819629192352\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  637 | Train Loss:  0.021888399496674538 | Validation Loss:  0.018545683473348618\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  638 | Train Loss:  0.021862363442778587 | Validation Loss:  0.01852165348827839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  639 | Train Loss:  0.02183644101023674 | Validation Loss:  0.018497731536626816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  640 | Train Loss:  0.021810630336403847 | Validation Loss:  0.018473921343684196\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  641 | Train Loss:  0.021784933283925056 | Validation Loss:  0.018450211733579636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  642 | Train Loss:  0.02175934426486492 | Validation Loss:  0.01842661201953888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  643 | Train Loss:  0.02173386700451374 | Validation Loss:  0.01840311661362648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  644 | Train Loss:  0.021708497777581215 | Validation Loss:  0.018379727378487587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  645 | Train Loss:  0.021683242172002792 | Validation Loss:  0.0183564405888319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  646 | Train Loss:  0.021658090874552727 | Validation Loss:  0.01833326369524002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  647 | Train Loss:  0.021633049473166466 | Validation Loss:  0.0183101836591959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  648 | Train Loss:  0.02160811610519886 | Validation Loss:  0.018287211656570435\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  649 | Train Loss:  0.02158329077064991 | Validation Loss:  0.018264340236783028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  650 | Train Loss:  0.021558571606874466 | Validation Loss:  0.018241574987769127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  651 | Train Loss:  0.021533958613872528 | Validation Loss:  0.018218908458948135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  652 | Train Loss:  0.021509451791644096 | Validation Loss:  0.01819634437561035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  653 | Train Loss:  0.02148505114018917 | Validation Loss:  0.018173882737755775\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  654 | Train Loss:  0.02146075665950775 | Validation Loss:  0.018151521682739258\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  655 | Train Loss:  0.02143656462430954 | Validation Loss:  0.01812925934791565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  656 | Train Loss:  0.021412480622529984 | Validation Loss:  0.0181070975959301\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  657 | Train Loss:  0.021388497203588486 | Validation Loss:  0.018085038289427757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  658 | Train Loss:  0.021364616230130196 | Validation Loss:  0.018063075840473175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  659 | Train Loss:  0.021340839564800262 | Validation Loss:  0.018041210249066353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  660 | Train Loss:  0.021317167207598686 | Validation Loss:  0.01801944524049759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  661 | Train Loss:  0.02129359543323517 | Validation Loss:  0.017997777089476585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  662 | Train Loss:  0.02127012610435486 | Validation Loss:  0.01797620579600334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  663 | Train Loss:  0.021246757358312607 | Validation Loss:  0.01795472949743271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  664 | Train Loss:  0.021223489195108414 | Validation Loss:  0.017933351919054985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  665 | Train Loss:  0.02120032161474228 | Validation Loss:  0.01791207119822502\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  666 | Train Loss:  0.021177250891923904 | Validation Loss:  0.01789088547229767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  667 | Train Loss:  0.021154284477233887 | Validation Loss:  0.017869792878627777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  668 | Train Loss:  0.02113141492009163 | Validation Loss:  0.017848797142505646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  669 | Train Loss:  0.02110864222049713 | Validation Loss:  0.017827894538640976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  670 | Train Loss:  0.02108597196638584 | Validation Loss:  0.017807086929678917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  671 | Train Loss:  0.021063394844532013 | Validation Loss:  0.01778637245297432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  672 | Train Loss:  0.021040918305516243 | Validation Loss:  0.017765749245882034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  673 | Train Loss:  0.021018534898757935 | Validation Loss:  0.01774522103369236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  674 | Train Loss:  0.020996253937482834 | Validation Loss:  0.01772478222846985\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  675 | Train Loss:  0.020974062383174896 | Validation Loss:  0.017704438418149948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  676 | Train Loss:  0.020951971411705017 | Validation Loss:  0.01768418401479721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  677 | Train Loss:  0.0209299735724926 | Validation Loss:  0.017664020881056786\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  678 | Train Loss:  0.020908070728182793 | Validation Loss:  0.017643950879573822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  679 | Train Loss:  0.020886262878775597 | Validation Loss:  0.01762397214770317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  680 | Train Loss:  0.020864546298980713 | Validation Loss:  0.017604079097509384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  681 | Train Loss:  0.02084292843937874 | Validation Loss:  0.01758427545428276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  682 | Train Loss:  0.020821399986743927 | Validation Loss:  0.01756456308066845\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  683 | Train Loss:  0.020799962803721428 | Validation Loss:  0.017544938251376152\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  684 | Train Loss:  0.02077862247824669 | Validation Loss:  0.017525402829051018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  685 | Train Loss:  0.020757371559739113 | Validation Loss:  0.017505956813693047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  686 | Train Loss:  0.020736213773489 | Validation Loss:  0.01748659648001194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  687 | Train Loss:  0.020715145394206047 | Validation Loss:  0.01746731996536255\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  688 | Train Loss:  0.020694168284535408 | Validation Loss:  0.01744813472032547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  689 | Train Loss:  0.02067328430712223 | Validation Loss:  0.017429035156965256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  690 | Train Loss:  0.020652489736676216 | Validation Loss:  0.017410023137927055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  691 | Train Loss:  0.020631780847907066 | Validation Loss:  0.01739109307527542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  692 | Train Loss:  0.020611165091395378 | Validation Loss:  0.01737224869430065\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  693 | Train Loss:  0.020590635016560555 | Validation Loss:  0.017353489995002747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  694 | Train Loss:  0.020570194348692894 | Validation Loss:  0.017334815114736557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  695 | Train Loss:  0.020549839362502098 | Validation Loss:  0.017316224053502083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  696 | Train Loss:  0.020529579371213913 | Validation Loss:  0.017297718673944473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  697 | Train Loss:  0.020509401336312294 | Validation Loss:  0.01727929338812828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  698 | Train Loss:  0.020489314571022987 | Validation Loss:  0.017260955646634102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  699 | Train Loss:  0.02046930603682995 | Validation Loss:  0.01724269613623619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  700 | Train Loss:  0.02044939063489437 | Validation Loss:  0.017224522307515144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  701 | Train Loss:  0.020429560914635658 | Validation Loss:  0.017206430435180664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  702 | Train Loss:  0.02040981501340866 | Validation Loss:  0.01718841679394245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  703 | Train Loss:  0.020390154793858528 | Validation Loss:  0.017170488834381104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  704 | Train Loss:  0.02037057839334011 | Validation Loss:  0.017152637243270874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  705 | Train Loss:  0.020351087674498558 | Validation Loss:  0.01713486760854721\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  706 | Train Loss:  0.02033167891204357 | Validation Loss:  0.017117179930210114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  707 | Train Loss:  0.02031235210597515 | Validation Loss:  0.017099572345614433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  708 | Train Loss:  0.020293114706873894 | Validation Loss:  0.01708204112946987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  709 | Train Loss:  0.020273955538868904 | Validation Loss:  0.017064590007066727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  710 | Train Loss:  0.02025488018989563 | Validation Loss:  0.017047218978405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  711 | Train Loss:  0.02023588865995407 | Validation Loss:  0.01702992431819439\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  712 | Train Loss:  0.02021697722375393 | Validation Loss:  0.017012709751725197\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  713 | Train Loss:  0.020198149606585503 | Validation Loss:  0.016995571553707123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  714 | Train Loss:  0.020179402083158493 | Validation Loss:  0.016978511586785316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  715 | Train Loss:  0.020160730928182602 | Validation Loss:  0.01696152612566948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  716 | Train Loss:  0.020142147317528725 | Validation Loss:  0.01694462075829506\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  717 | Train Loss:  0.020123638212680817 | Validation Loss:  0.01692778989672661\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  718 | Train Loss:  0.020105211064219475 | Validation Loss:  0.016911035403609276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  719 | Train Loss:  0.0200868658721447 | Validation Loss:  0.016894357278943062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  720 | Train Loss:  0.020068597048521042 | Validation Loss:  0.016877753660082817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  721 | Train Loss:  0.020050406455993652 | Validation Loss:  0.016861222684383392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  722 | Train Loss:  0.02003229409456253 | Validation Loss:  0.016844769939780235\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  723 | Train Loss:  0.020014261826872826 | Validation Loss:  0.016828391700983047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  724 | Train Loss:  0.01999630779027939 | Validation Loss:  0.01681208424270153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  725 | Train Loss:  0.01997842825949192 | Validation Loss:  0.016795853152871132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  726 | Train Loss:  0.01996062695980072 | Validation Loss:  0.016779694706201553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  727 | Train Loss:  0.019942903891205788 | Validation Loss:  0.016763607040047646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  728 | Train Loss:  0.019925255328416824 | Validation Loss:  0.016747593879699707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  729 | Train Loss:  0.01990768499672413 | Validation Loss:  0.016731655225157738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  730 | Train Loss:  0.019890187308192253 | Validation Loss:  0.01671578548848629\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  731 | Train Loss:  0.019872765988111496 | Validation Loss:  0.01669999212026596\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  732 | Train Loss:  0.019855419173836708 | Validation Loss:  0.016684263944625854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  733 | Train Loss:  0.019838152453303337 | Validation Loss:  0.016668610274791718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  734 | Train Loss:  0.019820954650640488 | Validation Loss:  0.01665302738547325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  735 | Train Loss:  0.019803833216428757 | Validation Loss:  0.016637511551380157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  736 | Train Loss:  0.019786782562732697 | Validation Loss:  0.016622070223093033\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  737 | Train Loss:  0.019769808277487755 | Validation Loss:  0.01660669781267643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  738 | Train Loss:  0.019752906635403633 | Validation Loss:  0.016591394320130348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  739 | Train Loss:  0.019736075773835182 | Validation Loss:  0.016576161608099937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  740 | Train Loss:  0.01971932128071785 | Validation Loss:  0.016560997813940048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  741 | Train Loss:  0.019702637568116188 | Validation Loss:  0.01654590107500553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  742 | Train Loss:  0.019686026498675346 | Validation Loss:  0.016530869528651237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  743 | Train Loss:  0.019669484347105026 | Validation Loss:  0.016515912488102913\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  744 | Train Loss:  0.019653016701340675 | Validation Loss:  0.01650102064013481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  745 | Train Loss:  0.019636617973446846 | Validation Loss:  0.016486195847392082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  746 | Train Loss:  0.019620288163423538 | Validation Loss:  0.016471436247229576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  747 | Train Loss:  0.0196040291339159 | Validation Loss:  0.01645674556493759\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  748 | Train Loss:  0.019587844610214233 | Validation Loss:  0.01644212007522583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  749 | Train Loss:  0.019571727141737938 | Validation Loss:  0.01642756164073944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  750 | Train Loss:  0.019555678591132164 | Validation Loss:  0.016413072124123573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  751 | Train Loss:  0.01953970268368721 | Validation Loss:  0.01639864407479763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  752 | Train Loss:  0.01952379010617733 | Validation Loss:  0.01638428494334221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  753 | Train Loss:  0.01950795017182827 | Validation Loss:  0.016369987279176712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  754 | Train Loss:  0.019492173567414284 | Validation Loss:  0.016355756670236588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  755 | Train Loss:  0.019476471468806267 | Validation Loss:  0.016341589391231537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  756 | Train Loss:  0.019460834562778473 | Validation Loss:  0.01632748730480671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  757 | Train Loss:  0.01944526471197605 | Validation Loss:  0.016313448548316956\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  758 | Train Loss:  0.019429760053753853 | Validation Loss:  0.016299473121762276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  759 | Train Loss:  0.019414326176047325 | Validation Loss:  0.01628556288778782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  760 | Train Loss:  0.01939895562827587 | Validation Loss:  0.016271714121103287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  761 | Train Loss:  0.01938365399837494 | Validation Loss:  0.01625792868435383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  762 | Train Loss:  0.01936841569840908 | Validation Loss:  0.016244204714894295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  763 | Train Loss:  0.019353246316313744 | Validation Loss:  0.016230544075369835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  764 | Train Loss:  0.01933813840150833 | Validation Loss:  0.01621694676578045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  765 | Train Loss:  0.01932309754192829 | Validation Loss:  0.016203410923480988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  766 | Train Loss:  0.019308123737573624 | Validation Loss:  0.0161899346858263\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  767 | Train Loss:  0.01929321140050888 | Validation Loss:  0.01617651991546154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  768 | Train Loss:  0.01927836239337921 | Validation Loss:  0.016163168475031853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  769 | Train Loss:  0.019263578578829765 | Validation Loss:  0.016149872913956642\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  770 | Train Loss:  0.019248859956860542 | Validation Loss:  0.016136644408106804\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  771 | Train Loss:  0.019234204664826393 | Validation Loss:  0.016123469918966293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  772 | Train Loss:  0.01921961084008217 | Validation Loss:  0.016110356897115707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  773 | Train Loss:  0.019205080345273018 | Validation Loss:  0.016097303479909897\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  774 | Train Loss:  0.01919061318039894 | Validation Loss:  0.01608430966734886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  775 | Train Loss:  0.01917620748281479 | Validation Loss:  0.016071375459432602\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  776 | Train Loss:  0.01916186511516571 | Validation Loss:  0.016058500856161118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  777 | Train Loss:  0.019147586077451706 | Validation Loss:  0.01604568213224411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  778 | Train Loss:  0.019133364781737328 | Validation Loss:  0.016032924875617027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  779 | Train Loss:  0.019119206815958023 | Validation Loss:  0.01602022349834442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  780 | Train Loss:  0.019105108454823494 | Validation Loss:  0.01600757986307144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  781 | Train Loss:  0.01909107342362404 | Validation Loss:  0.015994995832443237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  782 | Train Loss:  0.01907709613442421 | Validation Loss:  0.01598246768116951\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  783 | Train Loss:  0.019063180312514305 | Validation Loss:  0.01596999354660511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  784 | Train Loss:  0.019049325957894325 | Validation Loss:  0.015957579016685486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  785 | Train Loss:  0.019035527482628822 | Validation Loss:  0.015945222228765488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  786 | Train Loss:  0.019021792337298393 | Validation Loss:  0.015932919457554817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  787 | Train Loss:  0.01900811493396759 | Validation Loss:  0.015920674428343773\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  788 | Train Loss:  0.018994498997926712 | Validation Loss:  0.015908485278487206\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  789 | Train Loss:  0.01898093707859516 | Validation Loss:  0.015896350145339966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  790 | Train Loss:  0.018967436626553535 | Validation Loss:  0.015884269028902054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  791 | Train Loss:  0.018953992053866386 | Validation Loss:  0.015872245654463768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  792 | Train Loss:  0.018940607085824013 | Validation Loss:  0.01586027629673481\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  793 | Train Loss:  0.018927279859781265 | Validation Loss:  0.01584836095571518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  794 | Train Loss:  0.018914010375738144 | Validation Loss:  0.015836497768759727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  795 | Train Loss:  0.0189007967710495 | Validation Loss:  0.015824690461158752\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  796 | Train Loss:  0.018887639045715332 | Validation Loss:  0.015812939032912254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  797 | Train Loss:  0.01887454278767109 | Validation Loss:  0.015801239758729935\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  798 | Train Loss:  0.018861500546336174 | Validation Loss:  0.015789594501256943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  799 | Train Loss:  0.018848514184355736 | Validation Loss:  0.01577800139784813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  800 | Train Loss:  0.018835581839084625 | Validation Loss:  0.015766460448503494\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  801 | Train Loss:  0.01882270723581314 | Validation Loss:  0.015754973515868187\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  802 | Train Loss:  0.018809888511896133 | Validation Loss:  0.01574353687465191\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  803 | Train Loss:  0.018797125667333603 | Validation Loss:  0.01573215425014496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  804 | Train Loss:  0.01878441497683525 | Validation Loss:  0.015720821917057037\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  805 | Train Loss:  0.018771762028336525 | Validation Loss:  0.015709543600678444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  806 | Train Loss:  0.018759163096547127 | Validation Loss:  0.01569831557571888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  807 | Train Loss:  0.018746616318821907 | Validation Loss:  0.015687137842178345\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  808 | Train Loss:  0.018734127283096313 | Validation Loss:  0.015676014125347137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  809 | Train Loss:  0.0187216904014349 | Validation Loss:  0.01566493883728981\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  810 | Train Loss:  0.01870930753648281 | Validation Loss:  0.015653911978006363\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  811 | Train Loss:  0.018696974962949753 | Validation Loss:  0.015642939135432243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  812 | Train Loss:  0.018684696406126022 | Validation Loss:  0.015632012858986855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  813 | Train Loss:  0.01867247372865677 | Validation Loss:  0.015621141530573368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  814 | Train Loss:  0.018660301342606544 | Validation Loss:  0.015610316768288612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  815 | Train Loss:  0.018648186698555946 | Validation Loss:  0.015599542297422886\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  816 | Train Loss:  0.01863611489534378 | Validation Loss:  0.01558881625533104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  817 | Train Loss:  0.01862410083413124 | Validation Loss:  0.015578140504658222\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  818 | Train Loss:  0.01861213892698288 | Validation Loss:  0.015567513182759285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  819 | Train Loss:  0.018600227311253548 | Validation Loss:  0.015556933358311653\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  820 | Train Loss:  0.018588367849588394 | Validation Loss:  0.015546401962637901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  821 | Train Loss:  0.01857656054198742 | Validation Loss:  0.015535918064415455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  822 | Train Loss:  0.018564799800515175 | Validation Loss:  0.015525482594966888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  823 | Train Loss:  0.01855309307575226 | Validation Loss:  0.015515096485614777\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  824 | Train Loss:  0.01854143664240837 | Validation Loss:  0.015504758805036545\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  825 | Train Loss:  0.018529832363128662 | Validation Loss:  0.01549446489661932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  826 | Train Loss:  0.018518276512622833 | Validation Loss:  0.01548422034829855\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  827 | Train Loss:  0.018506765365600586 | Validation Loss:  0.01547402236610651\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  828 | Train Loss:  0.018495310097932816 | Validation Loss:  0.015463870018720627\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  829 | Train Loss:  0.018483903259038925 | Validation Loss:  0.015453764237463474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  830 | Train Loss:  0.018472542986273766 | Validation Loss:  0.015443706884980202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  831 | Train Loss:  0.018461234867572784 | Validation Loss:  0.015433693304657936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  832 | Train Loss:  0.018449971452355385 | Validation Loss:  0.01542372815310955\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  833 | Train Loss:  0.018438760191202164 | Validation Loss:  0.015413803979754448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  834 | Train Loss:  0.018427599221467972 | Validation Loss:  0.0154039291664958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  835 | Train Loss:  0.018416481092572212 | Validation Loss:  0.015394099056720734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  836 | Train Loss:  0.018405413255095482 | Validation Loss:  0.015384312719106674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  837 | Train Loss:  0.01839439384639263 | Validation Loss:  0.015374572947621346\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  838 | Train Loss:  0.018383419141173363 | Validation Loss:  0.015364878810942173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  839 | Train Loss:  0.018372492864727974 | Validation Loss:  0.015355225652456284\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  840 | Train Loss:  0.018361616879701614 | Validation Loss:  0.015345617197453976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  841 | Train Loss:  0.018350785598158836 | Validation Loss:  0.0153360515832901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  842 | Train Loss:  0.01833999902009964 | Validation Loss:  0.015326534397900105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  843 | Train Loss:  0.018329260870814323 | Validation Loss:  0.015317058190703392\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  844 | Train Loss:  0.018318569287657738 | Validation Loss:  0.015307625755667686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  845 | Train Loss:  0.018307922407984734 | Validation Loss:  0.015298236161470413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  846 | Train Loss:  0.01829732395708561 | Validation Loss:  0.015288890339434147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  847 | Train Loss:  0.018286770209670067 | Validation Loss:  0.015279588289558887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  848 | Train Loss:  0.018276261165738106 | Validation Loss:  0.015270326286554337\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  849 | Train Loss:  0.018265798687934875 | Validation Loss:  0.015261108987033367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  850 | Train Loss:  0.018255380913615227 | Validation Loss:  0.01525193452835083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  851 | Train Loss:  0.01824500598013401 | Validation Loss:  0.015242801047861576\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  852 | Train Loss:  0.018234675750136375 | Validation Loss:  0.01523370947688818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  853 | Train Loss:  0.01822439208626747 | Validation Loss:  0.015224658884108067\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  854 | Train Loss:  0.018214151263237 | Validation Loss:  0.015215651132166386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  855 | Train Loss:  0.018203958868980408 | Validation Loss:  0.015206683427095413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  856 | Train Loss:  0.01819380559027195 | Validation Loss:  0.015197756700217724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  857 | Train Loss:  0.018183698877692223 | Validation Loss:  0.015188873745501041\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  858 | Train Loss:  0.01817363128066063 | Validation Loss:  0.015180028043687344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  859 | Train Loss:  0.018163613975048065 | Validation Loss:  0.015171225182712078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  860 | Train Loss:  0.018153633922338486 | Validation Loss:  0.015162462368607521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  861 | Train Loss:  0.018143698573112488 | Validation Loss:  0.015153742395341396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  862 | Train Loss:  0.018133806064724922 | Validation Loss:  0.015145059674978256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  863 | Train Loss:  0.01812395639717579 | Validation Loss:  0.01513641607016325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  864 | Train Loss:  0.018114149570465088 | Validation Loss:  0.015127812512218952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  865 | Train Loss:  0.01810438558459282 | Validation Loss:  0.015119251795113087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  866 | Train Loss:  0.018094662576913834 | Validation Loss:  0.015110727399587631\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  867 | Train Loss:  0.01808498241007328 | Validation Loss:  0.015102243982255459\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  868 | Train Loss:  0.01807534322142601 | Validation Loss:  0.015093797817826271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  869 | Train Loss:  0.018065746873617172 | Validation Loss:  0.015085393562912941\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  870 | Train Loss:  0.018056191504001617 | Validation Loss:  0.015077024698257446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  871 | Train Loss:  0.018046677112579346 | Validation Loss:  0.01506869588047266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  872 | Train Loss:  0.018037201836705208 | Validation Loss:  0.015060405246913433\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  873 | Train Loss:  0.018027769401669502 | Validation Loss:  0.01505215372890234\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  874 | Train Loss:  0.01801837608218193 | Validation Loss:  0.015043938532471657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  875 | Train Loss:  0.01800902560353279 | Validation Loss:  0.015035761520266533\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  876 | Train Loss:  0.017999714240431786 | Validation Loss:  0.015027621760964394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  877 | Train Loss:  0.017990441992878914 | Validation Loss:  0.015019522979855537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  878 | Train Loss:  0.017981210723519325 | Validation Loss:  0.015011458657681942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  879 | Train Loss:  0.01797202043235302 | Validation Loss:  0.015003429725766182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  880 | Train Loss:  0.0179628673940897 | Validation Loss:  0.014995441772043705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  881 | Train Loss:  0.01795375533401966 | Validation Loss:  0.014987488277256489\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  882 | Train Loss:  0.017944684252142906 | Validation Loss:  0.014979571104049683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  883 | Train Loss:  0.017935648560523987 | Validation Loss:  0.014971692115068436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  884 | Train Loss:  0.01792665384709835 | Validation Loss:  0.014963848516345024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  885 | Train Loss:  0.0179176963865757 | Validation Loss:  0.014956041239202023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  886 | Train Loss:  0.01790877990424633 | Validation Loss:  0.014948270283639431\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  887 | Train Loss:  0.017899898812174797 | Validation Loss:  0.014940536580979824\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  888 | Train Loss:  0.017891060560941696 | Validation Loss:  0.014932835474610329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  889 | Train Loss:  0.01788225583732128 | Validation Loss:  0.014925171621143818\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  890 | Train Loss:  0.017873490229249 | Validation Loss:  0.014917544089257717\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  891 | Train Loss:  0.017864763736724854 | Validation Loss:  0.014909951947629452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  892 | Train Loss:  0.01785607635974884 | Validation Loss:  0.014902392402291298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  893 | Train Loss:  0.017847422510385513 | Validation Loss:  0.014894870109856129\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  894 | Train Loss:  0.01783880777657032 | Validation Loss:  0.014887381345033646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  895 | Train Loss:  0.01783023029565811 | Validation Loss:  0.014879927970468998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  896 | Train Loss:  0.017821690067648888 | Validation Loss:  0.014872507192194462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  897 | Train Loss:  0.01781318709254265 | Validation Loss:  0.01486512366682291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  898 | Train Loss:  0.017804717645049095 | Validation Loss:  0.01485777273774147\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  899 | Train Loss:  0.017796289175748825 | Validation Loss:  0.014850457198917866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  900 | Train Loss:  0.01778789423406124 | Validation Loss:  0.014843174256384373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  901 | Train Loss:  0.017779536545276642 | Validation Loss:  0.014835923910140991\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  902 | Train Loss:  0.017771214246749878 | Validation Loss:  0.014828707091510296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  903 | Train Loss:  0.01776292733848095 | Validation Loss:  0.014821523800492287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  904 | Train Loss:  0.017754673957824707 | Validation Loss:  0.014814376831054688\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  905 | Train Loss:  0.017746463418006897 | Validation Loss:  0.014807259663939476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  906 | Train Loss:  0.017738282680511475 | Validation Loss:  0.01480017602443695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  907 | Train Loss:  0.017730137333273888 | Validation Loss:  0.014793126843869686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  908 | Train Loss:  0.017722025513648987 | Validation Loss:  0.014786108396947384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  909 | Train Loss:  0.01771395467221737 | Validation Loss:  0.014779123477637768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  910 | Train Loss:  0.01770591363310814 | Validation Loss:  0.014772170223295689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  911 | Train Loss:  0.017697907984256744 | Validation Loss:  0.014765250496566296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  912 | Train Loss:  0.017689939588308334 | Validation Loss:  0.01475836243480444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  913 | Train Loss:  0.01768200471997261 | Validation Loss:  0.014751508831977844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  914 | Train Loss:  0.017674103379249573 | Validation Loss:  0.014744683168828487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  915 | Train Loss:  0.01766623556613922 | Validation Loss:  0.014737890101969242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  916 | Train Loss:  0.017658401280641556 | Validation Loss:  0.014731127768754959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  917 | Train Loss:  0.017650600522756577 | Validation Loss:  0.014724398963153362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  918 | Train Loss:  0.017642835155129433 | Validation Loss:  0.014717700891196728\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  919 | Train Loss:  0.017635099589824677 | Validation Loss:  0.014711033552885056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  920 | Train Loss:  0.017627405002713203 | Validation Loss:  0.014704396016895771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  921 | Train Loss:  0.01761973649263382 | Validation Loss:  0.014697792008519173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  922 | Train Loss:  0.017612101510167122 | Validation Loss:  0.014691215008497238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  923 | Train Loss:  0.01760450191795826 | Validation Loss:  0.014684670604765415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  924 | Train Loss:  0.017596930265426636 | Validation Loss:  0.01467815786600113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  925 | Train Loss:  0.017589401453733444 | Validation Loss:  0.014671673066914082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  926 | Train Loss:  0.017581898719072342 | Validation Loss:  0.014665219932794571\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  927 | Train Loss:  0.017574427649378777 | Validation Loss:  0.014658795669674873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  928 | Train Loss:  0.017566990107297897 | Validation Loss:  0.014652403071522713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  929 | Train Loss:  0.017559584230184555 | Validation Loss:  0.014646041207015514\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  930 | Train Loss:  0.01755221001803875 | Validation Loss:  0.01463970448821783\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  931 | Train Loss:  0.01754486747086048 | Validation Loss:  0.014633401297032833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  932 | Train Loss:  0.017537560313940048 | Validation Loss:  0.0146271251142025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  933 | Train Loss:  0.017530279234051704 | Validation Loss:  0.014620879665017128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  934 | Train Loss:  0.017523031681776047 | Validation Loss:  0.014614662155508995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  935 | Train Loss:  0.017515817657113075 | Validation Loss:  0.0146084725856781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  936 | Train Loss:  0.017508629709482193 | Validation Loss:  0.014602314680814743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  937 | Train Loss:  0.017501477152109146 | Validation Loss:  0.014596182852983475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  938 | Train Loss:  0.017494356259703636 | Validation Loss:  0.014590080827474594\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  939 | Train Loss:  0.017487259581685066 | Validation Loss:  0.014584006741642952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  940 | Train Loss:  0.01748019829392433 | Validation Loss:  0.014577960595488548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  941 | Train Loss:  0.017473164945840836 | Validation Loss:  0.014571943320333958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  942 | Train Loss:  0.017466165125370026 | Validation Loss:  0.014565953984856606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  943 | Train Loss:  0.017459193244576454 | Validation Loss:  0.014559994451701641\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  944 | Train Loss:  0.01745225302875042 | Validation Loss:  0.014554059132933617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  945 | Train Loss:  0.017445340752601624 | Validation Loss:  0.01454815361648798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  946 | Train Loss:  0.017438460141420364 | Validation Loss:  0.014542274177074432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  947 | Train Loss:  0.017431607469916344 | Validation Loss:  0.014536424539983273\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  948 | Train Loss:  0.01742478646337986 | Validation Loss:  0.014530600979924202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  949 | Train Loss:  0.017417991533875465 | Validation Loss:  0.014524806290864944\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  950 | Train Loss:  0.017411230131983757 | Validation Loss:  0.014519037678837776\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  951 | Train Loss:  0.017404496669769287 | Validation Loss:  0.014513293281197548\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  952 | Train Loss:  0.017397789284586906 | Validation Loss:  0.014507579617202282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  953 | Train Loss:  0.017391115427017212 | Validation Loss:  0.014501889236271381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  954 | Train Loss:  0.017384463921189308 | Validation Loss:  0.014496227726340294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  955 | Train Loss:  0.01737784780561924 | Validation Loss:  0.014490592293441296\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  956 | Train Loss:  0.01737125590443611 | Validation Loss:  0.014484982937574387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  957 | Train Loss:  0.01736469380557537 | Validation Loss:  0.014479399658739567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  958 | Train Loss:  0.017358163371682167 | Validation Loss:  0.014473842456936836\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  959 | Train Loss:  0.017351655289530754 | Validation Loss:  0.014468311332166195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  960 | Train Loss:  0.01734517700970173 | Validation Loss:  0.014462805353105068\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  961 | Train Loss:  0.01733872853219509 | Validation Loss:  0.014457326382398605\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  962 | Train Loss:  0.017332307994365692 | Validation Loss:  0.014451871626079082\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  963 | Train Loss:  0.017325911670923233 | Validation Loss:  0.014446445740759373\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  964 | Train Loss:  0.01731954701244831 | Validation Loss:  0.014441042207181454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  965 | Train Loss:  0.01731320656836033 | Validation Loss:  0.01443566381931305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  966 | Train Loss:  0.017306894063949585 | Validation Loss:  0.014430311508476734\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  967 | Train Loss:  0.01730060949921608 | Validation Loss:  0.014424986205995083\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  968 | Train Loss:  0.017294352874159813 | Validation Loss:  0.014419682323932648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  969 | Train Loss:  0.017288120463490486 | Validation Loss:  0.014414405450224876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  970 | Train Loss:  0.017281917855143547 | Validation Loss:  0.014409152790904045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  971 | Train Loss:  0.017275739461183548 | Validation Loss:  0.01440392341464758\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  972 | Train Loss:  0.017269590869545937 | Validation Loss:  0.014398720115423203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  973 | Train Loss:  0.017263466492295265 | Validation Loss:  0.014393541030585766\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  974 | Train Loss:  0.017257366329431534 | Validation Loss:  0.014388385228812695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  975 | Train Loss:  0.01725129596889019 | Validation Loss:  0.014383257366716862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  976 | Train Loss:  0.017245251685380936 | Validation Loss:  0.014378150925040245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  977 | Train Loss:  0.01723923347890377 | Validation Loss:  0.014373067766427994\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  978 | Train Loss:  0.017233241349458694 | Validation Loss:  0.014368007890880108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  979 | Train Loss:  0.01722727157175541 | Validation Loss:  0.014362974092364311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  980 | Train Loss:  0.01722133159637451 | Validation Loss:  0.014357962645590305\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  981 | Train Loss:  0.017215415835380554 | Validation Loss:  0.014352974481880665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  982 | Train Loss:  0.017209526151418686 | Validation Loss:  0.01434800960123539\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  983 | Train Loss:  0.017203662544488907 | Validation Loss:  0.014343071728944778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  984 | Train Loss:  0.017197825014591217 | Validation Loss:  0.01433815248310566\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  985 | Train Loss:  0.01719200611114502 | Validation Loss:  0.01433325745165348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  986 | Train Loss:  0.01718621701002121 | Validation Loss:  0.014328387565910816\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  987 | Train Loss:  0.01718045398592949 | Validation Loss:  0.014323538169264793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  988 | Train Loss:  0.01717471517622471 | Validation Loss:  0.014318713918328285\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  989 | Train Loss:  0.017169000580906868 | Validation Loss:  0.014313909225165844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  990 | Train Loss:  0.017163308337330818 | Validation Loss:  0.014309129677712917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  991 | Train Loss:  0.017157644033432007 | Validation Loss:  0.014304372482001781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  992 | Train Loss:  0.017152003943920135 | Validation Loss:  0.014299637638032436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  993 | Train Loss:  0.017146386206150055 | Validation Loss:  0.014294924214482307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  994 | Train Loss:  0.017140794545412064 | Validation Loss:  0.014290232211351395\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  995 | Train Loss:  0.017135225236415863 | Validation Loss:  0.014285564422607422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  996 | Train Loss:  0.01712968200445175 | Validation Loss:  0.014280918054282665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  997 | Train Loss:  0.01712416112422943 | Validation Loss:  0.014276293106377125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  998 | Train Loss:  0.01711866445839405 | Validation Loss:  0.014271690510213375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Epoch:  999 | Train Loss:  0.01711319014430046 | Validation Loss:  0.014267109334468842\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'SGD'} Test Loss:  0.014721584506332874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  0 | Train Loss:  0.09485693275928497 | Validation Loss:  0.06007534638047218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  1 | Train Loss:  0.06500546634197235 | Validation Loss:  0.039976272732019424\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  2 | Train Loss:  0.04424969479441643 | Validation Loss:  0.02296549268066883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  3 | Train Loss:  0.026399031281471252 | Validation Loss:  0.011563479900360107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  4 | Train Loss:  0.013892685063183308 | Validation Loss:  0.009515133686363697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  5 | Train Loss:  0.010868576355278492 | Validation Loss:  0.008537995629012585\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  6 | Train Loss:  0.009975221939384937 | Validation Loss:  0.008016623556613922\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  7 | Train Loss:  0.009084762074053288 | Validation Loss:  0.0070854853838682175\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  8 | Train Loss:  0.008173090405762196 | Validation Loss:  0.006563521921634674\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  9 | Train Loss:  0.007274379022419453 | Validation Loss:  0.005605640355497599\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  10 | Train Loss:  0.006410763133317232 | Validation Loss:  0.005412505008280277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  11 | Train Loss:  0.005706272553652525 | Validation Loss:  0.004758275113999844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  12 | Train Loss:  0.005495856516063213 | Validation Loss:  0.005810895469039679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  13 | Train Loss:  0.005683375988155603 | Validation Loss:  0.006526486482471228\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  14 | Train Loss:  0.0075430599972605705 | Validation Loss:  0.004744937177747488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  15 | Train Loss:  0.0046057505533099174 | Validation Loss:  0.0042855096980929375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  16 | Train Loss:  0.00486598489806056 | Validation Loss:  0.0037927275989204645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  17 | Train Loss:  0.0036447292659431696 | Validation Loss:  0.0033123239409178495\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  18 | Train Loss:  0.003649498103186488 | Validation Loss:  0.0031954047735780478\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  19 | Train Loss:  0.003030042862519622 | Validation Loss:  0.0028454528655856848\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  20 | Train Loss:  0.0030491999350488186 | Validation Loss:  0.002841900335624814\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  21 | Train Loss:  0.0026625054888427258 | Validation Loss:  0.002610832452774048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  22 | Train Loss:  0.002738452749326825 | Validation Loss:  0.002638160716742277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  23 | Train Loss:  0.0024526347406208515 | Validation Loss:  0.0024935263209044933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  24 | Train Loss:  0.002574742306023836 | Validation Loss:  0.0025103879161179066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  25 | Train Loss:  0.002325883135199547 | Validation Loss:  0.0024161050096154213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  26 | Train Loss:  0.0024638932663947344 | Validation Loss:  0.0024065799079835415\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  27 | Train Loss:  0.002227249089628458 | Validation Loss:  0.002332079689949751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  28 | Train Loss:  0.0023507277946919203 | Validation Loss:  0.0023044918198138475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  29 | Train Loss:  0.0021316453348845243 | Validation Loss:  0.0022337196860462427\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  30 | Train Loss:  0.0022254963405430317 | Validation Loss:  0.0022053129505366087\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  31 | Train Loss:  0.002038651844486594 | Validation Loss:  0.002135399030521512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  32 | Train Loss:  0.0021037033293396235 | Validation Loss:  0.0021170962136238813\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  33 | Train Loss:  0.001955742249265313 | Validation Loss:  0.002050498966127634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  34 | Train Loss:  0.001999777043238282 | Validation Loss:  0.002044642111286521\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  35 | Train Loss:  0.0018876325339078903 | Validation Loss:  0.001983728026971221\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  36 | Train Loss:  0.0019184384727850556 | Validation Loss:  0.001988279866054654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  37 | Train Loss:  0.001834740280173719 | Validation Loss:  0.0019338931888341904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  38 | Train Loss:  0.001857853028923273 | Validation Loss:  0.0019458986353129148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  39 | Train Loss:  0.0017950850306078792 | Validation Loss:  0.0018977324943989515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  40 | Train Loss:  0.0018139432650059462 | Validation Loss:  0.001914734486490488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  41 | Train Loss:  0.0017660324228927493 | Validation Loss:  0.0018719506915658712\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  42 | Train Loss:  0.001782683189958334 | Validation Loss:  0.0018922631861642003\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  43 | Train Loss:  0.001745171844959259 | Validation Loss:  0.0018539050361141562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  44 | Train Loss:  0.0017608581110835075 | Validation Loss:  0.0018764821579679847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  45 | Train Loss:  0.0017305976944044232 | Validation Loss:  0.0018416704842820764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  46 | Train Loss:  0.0017461255192756653 | Validation Loss:  0.001865908969193697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  47 | Train Loss:  0.0017209039069712162 | Validation Loss:  0.0018339125672355294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  48 | Train Loss:  0.0017368533881381154 | Validation Loss:  0.0018594813300296664\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  49 | Train Loss:  0.0017150916391983628 | Validation Loss:  0.0018297206843271852\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  50 | Train Loss:  0.0017319227335974574 | Validation Loss:  0.0018564382335171103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  51 | Train Loss:  0.001712451339699328 | Validation Loss:  0.001828469568863511\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  52 | Train Loss:  0.0017305576475337148 | Validation Loss:  0.0018562154145911336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  53 | Train Loss:  0.0017124594887718558 | Validation Loss:  0.0018296961206942797\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  54 | Train Loss:  0.001732180709950626 | Validation Loss:  0.0018583439523354173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  55 | Train Loss:  0.0017146841855719686 | Validation Loss:  0.0018330015009269118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  56 | Train Loss:  0.0017362961079925299 | Validation Loss:  0.0018623728537932038\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  57 | Train Loss:  0.0017187041230499744 | Validation Loss:  0.001837963005527854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  58 | Train Loss:  0.0017423862591385841 | Validation Loss:  0.001867797807790339\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  59 | Train Loss:  0.0017240428132936358 | Validation Loss:  0.0018440682906657457\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  60 | Train Loss:  0.0017498353263363242 | Validation Loss:  0.0018740202067419887\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  61 | Train Loss:  0.001730126328766346 | Validation Loss:  0.0018506754422560334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  62 | Train Loss:  0.001757885911501944 | Validation Loss:  0.0018803420243784785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  63 | Train Loss:  0.0017362775979563594 | Validation Loss:  0.001857027062214911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  64 | Train Loss:  0.0017656526761129498 | Validation Loss:  0.0018860105192288756\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  65 | Train Loss:  0.0017417635535821319 | Validation Loss:  0.001862323610112071\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  66 | Train Loss:  0.0017722074408084154 | Validation Loss:  0.0018903169548138976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  67 | Train Loss:  0.001745893037877977 | Validation Loss:  0.0018658543704077601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  68 | Train Loss:  0.0017767270328477025 | Validation Loss:  0.0018927159253507853\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  69 | Train Loss:  0.0017481347313150764 | Validation Loss:  0.0018671375000849366\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  70 | Train Loss:  0.0017786521930247545 | Validation Loss:  0.0018929308280348778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  71 | Train Loss:  0.0017482253024354577 | Validation Loss:  0.0018660256173461676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  72 | Train Loss:  0.00177780631929636 | Validation Loss:  0.001890998799353838\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  73 | Train Loss:  0.0017462095711380243 | Validation Loss:  0.0018627195386216044\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  74 | Train Loss:  0.001774413394741714 | Validation Loss:  0.0018872387008741498\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  75 | Train Loss:  0.0017424119869247079 | Validation Loss:  0.0018577001756057143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  76 | Train Loss:  0.0017690189415588975 | Validation Loss:  0.0018821625271812081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  77 | Train Loss:  0.0017373424489051104 | Validation Loss:  0.0018515984993427992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  78 | Train Loss:  0.0017623447347432375 | Validation Loss:  0.0018763607367873192\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  79 | Train Loss:  0.0017315837321802974 | Validation Loss:  0.0018450652714818716\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  80 | Train Loss:  0.001755141420289874 | Validation Loss:  0.001870403066277504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  81 | Train Loss:  0.00172569474671036 | Validation Loss:  0.0018386754672974348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  82 | Train Loss:  0.0017480762908235192 | Validation Loss:  0.001864772872067988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  83 | Train Loss:  0.0017201442969962955 | Validation Loss:  0.0018328778678551316\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  84 | Train Loss:  0.0017416748451068997 | Validation Loss:  0.001859840122051537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  85 | Train Loss:  0.0017152877990156412 | Validation Loss:  0.0018279856303706765\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  86 | Train Loss:  0.0017363090300932527 | Validation Loss:  0.0018558570882305503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  87 | Train Loss:  0.0017113648355007172 | Validation Loss:  0.0018241900252178311\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  88 | Train Loss:  0.0017322087660431862 | Validation Loss:  0.0018529726658016443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  89 | Train Loss:  0.0017085139406844974 | Validation Loss:  0.001821582904085517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  90 | Train Loss:  0.0017294876743108034 | Validation Loss:  0.001851249486207962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  91 | Train Loss:  0.0017067906446754932 | Validation Loss:  0.0018201773054897785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  92 | Train Loss:  0.0017281670588999987 | Validation Loss:  0.0018506754422560334\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  93 | Train Loss:  0.0017061791149899364 | Validation Loss:  0.001819921308197081\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  94 | Train Loss:  0.0017281895270571113 | Validation Loss:  0.00185117544606328\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  95 | Train Loss:  0.0017066041473299265 | Validation Loss:  0.0018207080429419875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  96 | Train Loss:  0.0017294310964643955 | Validation Loss:  0.001852616434916854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  97 | Train Loss:  0.0017079358221963048 | Validation Loss:  0.0018223758088424802\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  98 | Train Loss:  0.0017317032907158136 | Validation Loss:  0.0018548095831647515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  99 | Train Loss:  0.0017099900869652629 | Validation Loss:  0.0018247091211378574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  100 | Train Loss:  0.0017347540706396103 | Validation Loss:  0.001857515424489975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  101 | Train Loss:  0.0017125350423157215 | Validation Loss:  0.0018274448812007904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  102 | Train Loss:  0.0017382754012942314 | Validation Loss:  0.0018604542128741741\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  103 | Train Loss:  0.0017152997897937894 | Validation Loss:  0.0018302808748558164\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  104 | Train Loss:  0.0017419143114238977 | Validation Loss:  0.001863322569988668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  105 | Train Loss:  0.0017179902642965317 | Validation Loss:  0.0018329017329961061\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  106 | Train Loss:  0.0017453033942729235 | Validation Loss:  0.0018658229382708669\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  107 | Train Loss:  0.0017203183379024267 | Validation Loss:  0.001835005939938128\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  108 | Train Loss:  0.0017480907263234258 | Validation Loss:  0.0018676923355087638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  109 | Train Loss:  0.0017220297595486045 | Validation Loss:  0.001836346578784287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  110 | Train Loss:  0.0017499887617304921 | Validation Loss:  0.0018687390256673098\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  111 | Train Loss:  0.0017229393124580383 | Validation Loss:  0.0018367615994066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  112 | Train Loss:  0.0017508083255961537 | Validation Loss:  0.0018688620766624808\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  113 | Train Loss:  0.0017229518853127956 | Validation Loss:  0.0018361948896199465\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  114 | Train Loss:  0.0017504809657111764 | Validation Loss:  0.0018680640496313572\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  115 | Train Loss:  0.0017220715526491404 | Validation Loss:  0.0018347017467021942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  116 | Train Loss:  0.0017490695463493466 | Validation Loss:  0.0018664440140128136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  117 | Train Loss:  0.0017203984316438437 | Validation Loss:  0.001832431647926569\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  118 | Train Loss:  0.0017467443831264973 | Validation Loss:  0.0018641776405274868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  119 | Train Loss:  0.0017181041184812784 | Validation Loss:  0.0018296013586223125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  120 | Train Loss:  0.0017437549540773034 | Validation Loss:  0.0018614860018715262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  121 | Train Loss:  0.0017154046799987555 | Validation Loss:  0.0018264572136104107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  122 | Train Loss:  0.0017403875244781375 | Validation Loss:  0.0018586054211482406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  123 | Train Loss:  0.0017125301528722048 | Validation Loss:  0.0018232468282803893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  124 | Train Loss:  0.0017369294073432684 | Validation Loss:  0.0018557616276666522\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  125 | Train Loss:  0.001709698117338121 | Validation Loss:  0.0018201903440058231\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  126 | Train Loss:  0.001733639044687152 | Validation Loss:  0.0018531496170908213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  127 | Train Loss:  0.0017070956528186798 | Validation Loss:  0.0018174673896282911\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  128 | Train Loss:  0.0017307274974882603 | Validation Loss:  0.0018509235233068466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  129 | Train Loss:  0.0017048701411113143 | Validation Loss:  0.0018152098637074232\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  130 | Train Loss:  0.001728351111523807 | Validation Loss:  0.0018491890514269471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  131 | Train Loss:  0.0017031215829774737 | Validation Loss:  0.001813498791307211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  132 | Train Loss:  0.0017266076756641269 | Validation Loss:  0.00184800592251122\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  133 | Train Loss:  0.0017019048100337386 | Validation Loss:  0.0018123677000403404\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  134 | Train Loss:  0.0017255381681025028 | Validation Loss:  0.0018473867094144225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  135 | Train Loss:  0.0017012311145663261 | Validation Loss:  0.001811806345358491\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  136 | Train Loss:  0.001725133042782545 | Validation Loss:  0.0018473039381206036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  137 | Train Loss:  0.0017010723240673542 | Validation Loss:  0.0018117665313184261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  138 | Train Loss:  0.0017253366531804204 | Validation Loss:  0.0018476927652955055\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  139 | Train Loss:  0.0017013652250170708 | Validation Loss:  0.0018121643224731088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  140 | Train Loss:  0.001726052025333047 | Validation Loss:  0.0018484563333913684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  141 | Train Loss:  0.0017020157538354397 | Validation Loss:  0.001812889240682125\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  142 | Train Loss:  0.0017271499382331967 | Validation Loss:  0.001849472988396883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  143 | Train Loss:  0.0017029069131240249 | Validation Loss:  0.0018138086888939142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  144 | Train Loss:  0.0017284760251641273 | Validation Loss:  0.0018506079213693738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  145 | Train Loss:  0.001703908434137702 | Validation Loss:  0.0018147807568311691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  146 | Train Loss:  0.0017298635793849826 | Validation Loss:  0.0018517187563702464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  147 | Train Loss:  0.0017048838781192899 | Validation Loss:  0.0018156619044020772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  148 | Train Loss:  0.001731144730001688 | Validation Loss:  0.0018526717321947217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  149 | Train Loss:  0.0017057046061381698 | Validation Loss:  0.0018163223285228014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  150 | Train Loss:  0.0017321673221886158 | Validation Loss:  0.0018533521797508001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  151 | Train Loss:  0.0017062608385458589 | Validation Loss:  0.0018166574882343411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  152 | Train Loss:  0.0017328085377812386 | Validation Loss:  0.0018536759307608008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  153 | Train Loss:  0.001706472015939653 | Validation Loss:  0.0018165982328355312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  154 | Train Loss:  0.0017329874681308866 | Validation Loss:  0.001853596419095993\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  155 | Train Loss:  0.0017062949482351542 | Validation Loss:  0.001816117437556386\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  156 | Train Loss:  0.00173267035279423 | Validation Loss:  0.0018531077075749636\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  157 | Train Loss:  0.0017057242803275585 | Validation Loss:  0.001815228839404881\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  158 | Train Loss:  0.0017318724421784282 | Validation Loss:  0.0018522428581491113\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  159 | Train Loss:  0.0017047928413376212 | Validation Loss:  0.0018139847088605165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  160 | Train Loss:  0.0017306542722508311 | Validation Loss:  0.0018510676454752684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  161 | Train Loss:  0.0017035647761076689 | Validation Loss:  0.001812467467971146\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  162 | Train Loss:  0.0017291104886680841 | Validation Loss:  0.001849670778028667\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  163 | Train Loss:  0.001702125882729888 | Validation Loss:  0.001810775720514357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  164 | Train Loss:  0.0017273563425987959 | Validation Loss:  0.0018481524894014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  165 | Train Loss:  0.0017005730187520385 | Validation Loss:  0.001809016102924943\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  166 | Train Loss:  0.0017255155835300684 | Validation Loss:  0.001846615574322641\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  167 | Train Loss:  0.0016990044387057424 | Validation Loss:  0.0018072902457788587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  168 | Train Loss:  0.0017237082356587052 | Validation Loss:  0.0018451543292030692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  169 | Train Loss:  0.001697510713711381 | Validation Loss:  0.0018056880217045546\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  170 | Train Loss:  0.0017220391891896725 | Validation Loss:  0.0018438475672155619\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  171 | Train Loss:  0.0016961670480668545 | Validation Loss:  0.0018042785814031959\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  172 | Train Loss:  0.0017205916810780764 | Validation Loss:  0.0018427555914968252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  173 | Train Loss:  0.0016950295539572835 | Validation Loss:  0.00180311209987849\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  174 | Train Loss:  0.0017194239189848304 | Validation Loss:  0.0018419157713651657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  175 | Train Loss:  0.0016941344365477562 | Validation Loss:  0.0018022136064246297\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  176 | Train Loss:  0.0017185669858008623 | Validation Loss:  0.001841342425905168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  177 | Train Loss:  0.0016934938030317426 | Validation Loss:  0.001801585080102086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  178 | Train Loss:  0.0017180242575705051 | Validation Loss:  0.0018410264747217298\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  179 | Train Loss:  0.001693099387921393 | Validation Loss:  0.0018012078944593668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  180 | Train Loss:  0.0017177743138745427 | Validation Loss:  0.0018409411422908306\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  181 | Train Loss:  0.001692924415692687 | Validation Loss:  0.001801044913008809\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  182 | Train Loss:  0.0017177744302898645 | Validation Loss:  0.0018410421907901764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  183 | Train Loss:  0.0016929262783378363 | Validation Loss:  0.0018010453786700964\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  184 | Train Loss:  0.0017179646529257298 | Validation Loss:  0.0018412728095427155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  185 | Train Loss:  0.0016930504934862256 | Validation Loss:  0.001801146543584764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  186 | Train Loss:  0.0017182730371132493 | Validation Loss:  0.0018415686208754778\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  187 | Train Loss:  0.001693235943093896 | Validation Loss:  0.0018012820510193706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  188 | Train Loss:  0.0017186207696795464 | Validation Loss:  0.0018418639665469527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  189 | Train Loss:  0.001693418831564486 | Validation Loss:  0.001801387406885624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  190 | Train Loss:  0.0017189319478347898 | Validation Loss:  0.0018420963315293193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  191 | Train Loss:  0.0016935404855757952 | Validation Loss:  0.0018014023080468178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  192 | Train Loss:  0.001719136256724596 | Validation Loss:  0.0018422129796817899\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  193 | Train Loss:  0.0016935502644628286 | Validation Loss:  0.0018012791406363249\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  194 | Train Loss:  0.001719177234917879 | Validation Loss:  0.0018421733984723687\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  195 | Train Loss:  0.0016934103332459927 | Validation Loss:  0.001800985774025321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  196 | Train Loss:  0.0017190163489431143 | Validation Loss:  0.001841953955590725\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  197 | Train Loss:  0.0016930977581068873 | Validation Loss:  0.001800505560822785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  198 | Train Loss:  0.001718634506687522 | Validation Loss:  0.0018415485974401236\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  199 | Train Loss:  0.0016926074167713523 | Validation Loss:  0.0017998423427343369\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  200 | Train Loss:  0.0017180348513647914 | Validation Loss:  0.0018409679178148508\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  201 | Train Loss:  0.0016919500194489956 | Validation Loss:  0.00179901416413486\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  202 | Train Loss:  0.0017172389198094606 | Validation Loss:  0.0018402382265776396\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  203 | Train Loss:  0.0016911507118493319 | Validation Loss:  0.0017980542033910751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  204 | Train Loss:  0.001716284197755158 | Validation Loss:  0.0018393956124782562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  205 | Train Loss:  0.0016902441857382655 | Validation Loss:  0.0017970032058656216\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  206 | Train Loss:  0.0017152188811451197 | Validation Loss:  0.001838484313338995\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  207 | Train Loss:  0.001689273165538907 | Validation Loss:  0.0017959082033485174\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  208 | Train Loss:  0.0017140979180112481 | Validation Loss:  0.0018375505460426211\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  209 | Train Loss:  0.0016882814234122634 | Validation Loss:  0.001794815412722528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  210 | Train Loss:  0.0017129757907241583 | Validation Loss:  0.0018366394797340035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  211 | Train Loss:  0.0016873119166120887 | Validation Loss:  0.0017937675584107637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  212 | Train Loss:  0.0017119027907028794 | Validation Loss:  0.0018357898807153106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  213 | Train Loss:  0.0016864016652107239 | Validation Loss:  0.0017928004963323474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  214 | Train Loss:  0.0017109212931245565 | Validation Loss:  0.0018350332975387573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  215 | Train Loss:  0.0016855805879458785 | Validation Loss:  0.0017919394886121154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  216 | Train Loss:  0.0017100614495575428 | Validation Loss:  0.001834389753639698\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  217 | Train Loss:  0.0016848670784384012 | Validation Loss:  0.0017912002513185143\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  218 | Train Loss:  0.0017093421192839742 | Validation Loss:  0.001833869144320488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  219 | Train Loss:  0.00168427056632936 | Validation Loss:  0.0017905866261571646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  220 | Train Loss:  0.0017087685409933329 | Validation Loss:  0.0018334711203351617\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  221 | Train Loss:  0.001683790236711502 | Validation Loss:  0.0017900930251926184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  222 | Train Loss:  0.001708333962596953 | Validation Loss:  0.0018331846222281456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  223 | Train Loss:  0.0016834159614518285 | Validation Loss:  0.0017897034995257854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  224 | Train Loss:  0.0017080202233046293 | Validation Loss:  0.0018329898593947291\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  225 | Train Loss:  0.0016831280663609505 | Validation Loss:  0.0017893954645842314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  226 | Train Loss:  0.0017078013624995947 | Validation Loss:  0.0018328620353713632\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  227 | Train Loss:  0.0016829032683745027 | Validation Loss:  0.001789141446352005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  228 | Train Loss:  0.001707644434645772 | Validation Loss:  0.0018327712314203382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  229 | Train Loss:  0.0016827128129079938 | Validation Loss:  0.0017889097798615694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  230 | Train Loss:  0.0017075130017474294 | Validation Loss:  0.0018326860154047608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  231 | Train Loss:  0.0016825267812237144 | Validation Loss:  0.0017886703135445714\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  232 | Train Loss:  0.0017073712078854442 | Validation Loss:  0.0018325772834941745\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  233 | Train Loss:  0.0016823181649670005 | Validation Loss:  0.0017883962718769908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  234 | Train Loss:  0.001707187038846314 | Validation Loss:  0.0018324197735637426\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  235 | Train Loss:  0.0016820624005049467 | Validation Loss:  0.001788063091225922\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  236 | Train Loss:  0.0017069325549528003 | Validation Loss:  0.0018321939278393984\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  237 | Train Loss:  0.0016817408613860607 | Validation Loss:  0.0017876563360914588\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  238 | Train Loss:  0.0017065894789993763 | Validation Loss:  0.001831887406297028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  239 | Train Loss:  0.001681343070231378 | Validation Loss:  0.001787167158909142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  240 | Train Loss:  0.0017061473336070776 | Validation Loss:  0.0018314957851544023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  241 | Train Loss:  0.0016808637883514166 | Validation Loss:  0.0017865942791104317\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  242 | Train Loss:  0.0017056051874533296 | Validation Loss:  0.0018310215091332793\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  243 | Train Loss:  0.001680306508205831 | Validation Loss:  0.001785944914445281\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  244 | Train Loss:  0.0017049704911187291 | Validation Loss:  0.0018304752884432673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  245 | Train Loss:  0.0016796806594356894 | Validation Loss:  0.0017852319870144129\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  246 | Train Loss:  0.0017042586114257574 | Validation Loss:  0.0018298729555681348\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  247 | Train Loss:  0.0016790017252787948 | Validation Loss:  0.0017844735411927104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  248 | Train Loss:  0.0017034908523783088 | Validation Loss:  0.0018292336026206613\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  249 | Train Loss:  0.0016782877501100302 | Validation Loss:  0.0017836898332461715\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  250 | Train Loss:  0.0017026904970407486 | Validation Loss:  0.001828577951528132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  251 | Train Loss:  0.0016775588737800717 | Validation Loss:  0.0017829020507633686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  252 | Train Loss:  0.0017018829239532351 | Validation Loss:  0.001827927422709763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  253 | Train Loss:  0.0016768352361395955 | Validation Loss:  0.0017821304500102997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  254 | Train Loss:  0.001701091998256743 | Validation Loss:  0.0018273011082783341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  255 | Train Loss:  0.0016761347651481628 | Validation Loss:  0.001781392376869917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  256 | Train Loss:  0.0017003383254632354 | Validation Loss:  0.0018267148407176137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  257 | Train Loss:  0.0016754716634750366 | Validation Loss:  0.0017807009862735868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  258 | Train Loss:  0.0016996375052258372 | Validation Loss:  0.0018261789809912443\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  259 | Train Loss:  0.0016748568741604686 | Validation Loss:  0.0017800650093704462\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  260 | Train Loss:  0.001699000014923513 | Validation Loss:  0.001825700281187892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  261 | Train Loss:  0.00167429621797055 | Validation Loss:  0.00177948793862015\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  262 | Train Loss:  0.001698430278338492 | Validation Loss:  0.0018252794397994876\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  263 | Train Loss:  0.0016737902769818902 | Validation Loss:  0.0017789681442081928\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  264 | Train Loss:  0.0016979266656562686 | Validation Loss:  0.001824912615120411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  265 | Train Loss:  0.001673334976658225 | Validation Loss:  0.001778499223291874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  266 | Train Loss:  0.001697481027804315 | Validation Loss:  0.001824590959586203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  267 | Train Loss:  0.0016729221679270267 | Validation Loss:  0.0017780705820769072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  268 | Train Loss:  0.0016970826545730233 | Validation Loss:  0.0018243038794025779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  269 | Train Loss:  0.0016725418390706182 | Validation Loss:  0.0017776712775230408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  270 | Train Loss:  0.001696716994047165 | Validation Loss:  0.0018240371719002724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  271 | Train Loss:  0.0016721804859116673 | Validation Loss:  0.0017772858263924718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  272 | Train Loss:  0.0016963668167591095 | Validation Loss:  0.0018237765179947019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  273 | Train Loss:  0.0016718244878575206 | Validation Loss:  0.0017769008409231901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  274 | Train Loss:  0.0016960158245638013 | Validation Loss:  0.0018235082970932126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  275 | Train Loss:  0.0016714608063921332 | Validation Loss:  0.001776502700522542\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  276 | Train Loss:  0.0016956484178081155 | Validation Loss:  0.00182321947067976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  277 | Train Loss:  0.0016710777999833226 | Validation Loss:  0.0017760806949809194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  278 | Train Loss:  0.0016952510923147202 | Validation Loss:  0.0018229011911898851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  279 | Train Loss:  0.0016706660389900208 | Validation Loss:  0.0017756270244717598\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  280 | Train Loss:  0.001694815349765122 | Validation Loss:  0.0018225464737042785\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  281 | Train Loss:  0.0016702204011380672 | Validation Loss:  0.001775136450305581\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  282 | Train Loss:  0.0016943346709012985 | Validation Loss:  0.0018221527570858598\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  283 | Train Loss:  0.0016697375103831291 | Validation Loss:  0.0017746079247444868\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  284 | Train Loss:  0.0016938080079853535 | Validation Loss:  0.0018217202741652727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  285 | Train Loss:  0.0016692178323864937 | Validation Loss:  0.0017740444745868444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  286 | Train Loss:  0.0016932378057390451 | Validation Loss:  0.001821253215894103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  287 | Train Loss:  0.0016686654416844249 | Validation Loss:  0.0017734505236148834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  288 | Train Loss:  0.0016926301177591085 | Validation Loss:  0.0018207583343610168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  289 | Train Loss:  0.0016680870903655887 | Validation Loss:  0.0017728345701470971\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  290 | Train Loss:  0.001691994839347899 | Validation Loss:  0.0018202444771304727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  291 | Train Loss:  0.0016674903454259038 | Validation Loss:  0.0017722051125019789\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  292 | Train Loss:  0.0016913419822230935 | Validation Loss:  0.0018197211902588606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  293 | Train Loss:  0.0016668855678290129 | Validation Loss:  0.0017715729773044586\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  294 | Train Loss:  0.0016906838864088058 | Validation Loss:  0.0018191987182945013\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  295 | Train Loss:  0.0016662810230627656 | Validation Loss:  0.0017709465464577079\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  296 | Train Loss:  0.0016900311456993222 | Validation Loss:  0.0018186854431405663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  297 | Train Loss:  0.0016656853258609772 | Validation Loss:  0.0017703345511108637\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  298 | Train Loss:  0.001689393655396998 | Validation Loss:  0.0018181890482082963\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  299 | Train Loss:  0.0016651058103889227 | Validation Loss:  0.0017697431612759829\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  300 | Train Loss:  0.0016887788660824299 | Validation Loss:  0.0018177155870944262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  301 | Train Loss:  0.0016645477153360844 | Validation Loss:  0.0017691770335659385\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  302 | Train Loss:  0.001688192831352353 | Validation Loss:  0.001817269017919898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  303 | Train Loss:  0.0016640146495774388 | Validation Loss:  0.0017686393111944199\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  304 | Train Loss:  0.0016876389272511005 | Validation Loss:  0.0018168495735153556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  305 | Train Loss:  0.0016635076608508825 | Validation Loss:  0.0017681288300082088\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  306 | Train Loss:  0.0016871161060407758 | Validation Loss:  0.0018164565553888679\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  307 | Train Loss:  0.0016630254685878754 | Validation Loss:  0.0017676432617008686\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  308 | Train Loss:  0.0016866214573383331 | Validation Loss:  0.0018160855397582054\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  309 | Train Loss:  0.001662563649006188 | Validation Loss:  0.0017671787645667791\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  310 | Train Loss:  0.0016861503245308995 | Validation Loss:  0.0018157325685024261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  311 | Train Loss:  0.0016621185932308435 | Validation Loss:  0.0017667294014245272\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  312 | Train Loss:  0.0016856963047757745 | Validation Loss:  0.001815391005948186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  313 | Train Loss:  0.0016616842476651073 | Validation Loss:  0.0017662886530160904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  314 | Train Loss:  0.0016852517146617174 | Validation Loss:  0.0018150538671761751\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  315 | Train Loss:  0.0016612542094662786 | Validation Loss:  0.0017658504657447338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  316 | Train Loss:  0.0016848092200234532 | Validation Loss:  0.0018147150985896587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  317 | Train Loss:  0.0016608219593763351 | Validation Loss:  0.0017654082039371133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  318 | Train Loss:  0.001684360671788454 | Validation Loss:  0.00181436853017658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  319 | Train Loss:  0.0016603827243670821 | Validation Loss:  0.0017649572109803557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  320 | Train Loss:  0.0016839004820212722 | Validation Loss:  0.0018140096217393875\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  321 | Train Loss:  0.001659931382164359 | Validation Loss:  0.00176449294667691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  322 | Train Loss:  0.0016834235284477472 | Validation Loss:  0.0018136349972337484\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  323 | Train Loss:  0.0016594654880464077 | Validation Loss:  0.0017640134319663048\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  324 | Train Loss:  0.001682927249930799 | Validation Loss:  0.0018132426775991917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  325 | Train Loss:  0.0016589824808761477 | Validation Loss:  0.0017635164549574256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  326 | Train Loss:  0.001682410016655922 | Validation Loss:  0.0018128325464203954\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  327 | Train Loss:  0.0016584828263148665 | Validation Loss:  0.001763004227541387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  328 | Train Loss:  0.0016818724106997252 | Validation Loss:  0.0018124060006812215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  329 | Train Loss:  0.0016579680377617478 | Validation Loss:  0.0017624774482101202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  330 | Train Loss:  0.0016813173424452543 | Validation Loss:  0.0018119659507647157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  331 | Train Loss:  0.0016574403271079063 | Validation Loss:  0.0017619404243305326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  332 | Train Loss:  0.0016807483043521643 | Validation Loss:  0.0018115162383764982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  333 | Train Loss:  0.0016569035360589623 | Validation Loss:  0.0017613971140235662\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  334 | Train Loss:  0.001680170651525259 | Validation Loss:  0.0018110618693754077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  335 | Train Loss:  0.0016563620883971453 | Validation Loss:  0.0017608519410714507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  336 | Train Loss:  0.0016795898554846644 | Validation Loss:  0.0018106066854670644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  337 | Train Loss:  0.0016558204079046845 | Validation Loss:  0.0017603093292564154\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  338 | Train Loss:  0.00167901034001261 | Validation Loss:  0.0018101552268490195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  339 | Train Loss:  0.0016552823362872005 | Validation Loss:  0.0017597732366994023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  340 | Train Loss:  0.0016784380422905087 | Validation Loss:  0.0018097124993801117\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  341 | Train Loss:  0.0016547520644962788 | Validation Loss:  0.0017592470394447446\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  342 | Train Loss:  0.0016778761055320501 | Validation Loss:  0.001809279783628881\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  343 | Train Loss:  0.0016542313387617469 | Validation Loss:  0.0017587329493835568\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  344 | Train Loss:  0.0016773274401202798 | Validation Loss:  0.001808859291486442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  345 | Train Loss:  0.0016537222545593977 | Validation Loss:  0.0017582315485924482\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  346 | Train Loss:  0.0016767929773777723 | Validation Loss:  0.0018084512557834387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  347 | Train Loss:  0.0016532248118892312 | Validation Loss:  0.0017577437683939934\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  348 | Train Loss:  0.001676273182965815 | Validation Loss:  0.0018080566078424454\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  349 | Train Loss:  0.0016527395928278565 | Validation Loss:  0.0017572685610502958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  350 | Train Loss:  0.0016757675912231207 | Validation Loss:  0.0018076731357723475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  351 | Train Loss:  0.0016522654332220554 | Validation Loss:  0.00175680429674685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  352 | Train Loss:  0.00167527433950454 | Validation Loss:  0.0018072988605126739\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  353 | Train Loss:  0.001651799539104104 | Validation Loss:  0.0017563484143465757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  354 | Train Loss:  0.0016747902845963836 | Validation Loss:  0.001806931453756988\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  355 | Train Loss:  0.0016513399314135313 | Validation Loss:  0.0017558985855430365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  356 | Train Loss:  0.0016743120504543185 | Validation Loss:  0.0018065677722916007\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  357 | Train Loss:  0.0016508838161826134 | Validation Loss:  0.0017554513178765774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  358 | Train Loss:  0.0016738360282033682 | Validation Loss:  0.0018062039744108915\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  359 | Train Loss:  0.001650427933782339 | Validation Loss:  0.001755003584548831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  360 | Train Loss:  0.0016733590746298432 | Validation Loss:  0.0018058383138850331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  361 | Train Loss:  0.0016499697230756283 | Validation Loss:  0.001754552940838039\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  362 | Train Loss:  0.0016728774644434452 | Validation Loss:  0.0018054667161777616\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  363 | Train Loss:  0.0016495061572641134 | Validation Loss:  0.0017540967091917992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  364 | Train Loss:  0.0016723887529224157 | Validation Loss:  0.0018050884827971458\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  365 | Train Loss:  0.00164903630502522 | Validation Loss:  0.0017536343075335026\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  366 | Train Loss:  0.001671891426667571 | Validation Loss:  0.0018047032644972205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  367 | Train Loss:  0.001648559933528304 | Validation Loss:  0.0017531650373712182\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  368 | Train Loss:  0.0016713854856789112 | Validation Loss:  0.0018043097807094455\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  369 | Train Loss:  0.0016480757622048259 | Validation Loss:  0.0017526887822896242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  370 | Train Loss:  0.0016708702314645052 | Validation Loss:  0.0018039086135104299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  371 | Train Loss:  0.0016475844895467162 | Validation Loss:  0.0017522067064419389\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  372 | Train Loss:  0.0016703472938388586 | Validation Loss:  0.001803501509130001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  373 | Train Loss:  0.0016470877453684807 | Validation Loss:  0.0017517200903967023\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  374 | Train Loss:  0.0016698177205398679 | Validation Loss:  0.0018030896317213774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  375 | Train Loss:  0.001646586344577372 | Validation Loss:  0.0017512303311377764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  376 | Train Loss:  0.001669283607043326 | Validation Loss:  0.0018026753095909953\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  377 | Train Loss:  0.0016460828483104706 | Validation Loss:  0.0017507398733869195\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  378 | Train Loss:  0.0016687482129782438 | Validation Loss:  0.0018022602889686823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  379 | Train Loss:  0.0016455785371363163 | Validation Loss:  0.0017502506962046027\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  380 | Train Loss:  0.0016682128189131618 | Validation Loss:  0.0018018477130681276\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  381 | Train Loss:  0.0016450767870992422 | Validation Loss:  0.0017497652443125844\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  382 | Train Loss:  0.0016676812665537 | Validation Loss:  0.0018014381639659405\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  383 | Train Loss:  0.0016445780638605356 | Validation Loss:  0.00174928386695683\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  384 | Train Loss:  0.0016671536723151803 | Validation Loss:  0.00180103350430727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  385 | Train Loss:  0.0016440835315734148 | Validation Loss:  0.001748808310367167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  386 | Train Loss:  0.001666632597334683 | Validation Loss:  0.001800634665414691\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  387 | Train Loss:  0.001643595052883029 | Validation Loss:  0.0017483397386968136\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  388 | Train Loss:  0.0016661188565194607 | Validation Loss:  0.0018002426950260997\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  389 | Train Loss:  0.0016431122785434127 | Validation Loss:  0.0017478779191151261\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  390 | Train Loss:  0.0016656123334541917 | Validation Loss:  0.0017998572438955307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  391 | Train Loss:  0.0016426362562924623 | Validation Loss:  0.0017474228516221046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  392 | Train Loss:  0.0016651132609695196 | Validation Loss:  0.001799477613531053\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  393 | Train Loss:  0.001642165589146316 | Validation Loss:  0.00174697395414114\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  394 | Train Loss:  0.0016646208241581917 | Validation Loss:  0.0017991038039326668\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  395 | Train Loss:  0.0016416998114436865 | Validation Loss:  0.0017465301789343357\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  396 | Train Loss:  0.0016641335096210241 | Validation Loss:  0.001798733021132648\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  397 | Train Loss:  0.0016412375262007117 | Validation Loss:  0.0017460897797718644\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  398 | Train Loss:  0.0016636494547128677 | Validation Loss:  0.0017983649158850312\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  399 | Train Loss:  0.0016407775692641735 | Validation Loss:  0.0017456506611779332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  400 | Train Loss:  0.0016631671460345387 | Validation Loss:  0.00179799715988338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  401 | Train Loss:  0.001640317845158279 | Validation Loss:  0.0017452128231525421\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  402 | Train Loss:  0.0016626850701868534 | Validation Loss:  0.0017976290546357632\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  403 | Train Loss:  0.0016398578882217407 | Validation Loss:  0.0017447741702198982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  404 | Train Loss:  0.0016622014809399843 | Validation Loss:  0.0017972593195736408\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  405 | Train Loss:  0.0016393966507166624 | Validation Loss:  0.001744334353134036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  406 | Train Loss:  0.0016617156798020005 | Validation Loss:  0.0017968863248825073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  407 | Train Loss:  0.001638932153582573 | Validation Loss:  0.0017438913928344846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  408 | Train Loss:  0.0016612260369583964 | Validation Loss:  0.0017965107690542936\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  409 | Train Loss:  0.001638465328142047 | Validation Loss:  0.0017434463370591402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  410 | Train Loss:  0.0016607329016551375 | Validation Loss:  0.001796131837181747\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  411 | Train Loss:  0.0016379955923184752 | Validation Loss:  0.001742998487316072\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  412 | Train Loss:  0.0016602359246462584 | Validation Loss:  0.0017957495292648673\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  413 | Train Loss:  0.0016375224804505706 | Validation Loss:  0.0017425485420972109\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  414 | Train Loss:  0.0016597352223470807 | Validation Loss:  0.0017953644273802638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  415 | Train Loss:  0.0016370469238609076 | Validation Loss:  0.0017420962685719132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  416 | Train Loss:  0.0016592319589108229 | Validation Loss:  0.001794976880773902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  417 | Train Loss:  0.0016365695046260953 | Validation Loss:  0.0017416425980627537\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  418 | Train Loss:  0.0016587261343374848 | Validation Loss:  0.0017945879371836782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  419 | Train Loss:  0.0016360904555767775 | Validation Loss:  0.001741189043968916\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  420 | Train Loss:  0.0016582192620262504 | Validation Loss:  0.0017941994592547417\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  421 | Train Loss:  0.001635611755773425 | Validation Loss:  0.001740736304782331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  422 | Train Loss:  0.001657713670283556 | Validation Loss:  0.0017938119126483798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  423 | Train Loss:  0.0016351344529539347 | Validation Loss:  0.0017402858939021826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  424 | Train Loss:  0.001657209824770689 | Validation Loss:  0.0017934264615178108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  425 | Train Loss:  0.001634658663533628 | Validation Loss:  0.0017398379277437925\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  426 | Train Loss:  0.0016567081911489367 | Validation Loss:  0.0017930434551090002\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  427 | Train Loss:  0.001634185784496367 | Validation Loss:  0.0017393931047990918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  428 | Train Loss:  0.0016562101664021611 | Validation Loss:  0.0017926632426679134\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  429 | Train Loss:  0.0016337150009348989 | Validation Loss:  0.001738951774314046\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  430 | Train Loss:  0.001655715168453753 | Validation Loss:  0.0017922867555171251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  431 | Train Loss:  0.001633248059079051 | Validation Loss:  0.0017385142855346203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  432 | Train Loss:  0.0016552244778722525 | Validation Loss:  0.0017919138772413135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  433 | Train Loss:  0.001632784609682858 | Validation Loss:  0.0017380804056301713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  434 | Train Loss:  0.0016547376289963722 | Validation Loss:  0.0017915437929332256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  435 | Train Loss:  0.0016323237214237452 | Validation Loss:  0.0017376499017700553\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  436 | Train Loss:  0.0016542542725801468 | Validation Loss:  0.0017911766190081835\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  437 | Train Loss:  0.0016318653943017125 | Validation Loss:  0.0017372218426316977\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  438 | Train Loss:  0.0016537734773010015 | Validation Loss:  0.0017908115405589342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  439 | Train Loss:  0.0016314095119014382 | Validation Loss:  0.0017367963446304202\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  440 | Train Loss:  0.0016532951267436147 | Validation Loss:  0.0017904483247548342\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  441 | Train Loss:  0.0016309550264850259 | Validation Loss:  0.0017363728256896138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  442 | Train Loss:  0.0016528184060007334 | Validation Loss:  0.0017900854581966996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  443 | Train Loss:  0.00163050158880651 | Validation Loss:  0.001735949539579451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  444 | Train Loss:  0.0016523420345038176 | Validation Loss:  0.0017897234065458179\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  445 | Train Loss:  0.001630048151127994 | Validation Loss:  0.0017355269519612193\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  446 | Train Loss:  0.00165186554659158 | Validation Loss:  0.0017893610056489706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  447 | Train Loss:  0.0016295949462801218 | Validation Loss:  0.0017351048300042748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  448 | Train Loss:  0.001651389291509986 | Validation Loss:  0.0017889980226755142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  449 | Train Loss:  0.0016291412757709622 | Validation Loss:  0.0017346813110634685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  450 | Train Loss:  0.0016509115230292082 | Validation Loss:  0.0017886334098875523\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  451 | Train Loss:  0.0016286858590319753 | Validation Loss:  0.001734257093630731\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  452 | Train Loss:  0.0016504318919032812 | Validation Loss:  0.0017882678657770157\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  453 | Train Loss:  0.001628229976631701 | Validation Loss:  0.0017338322941213846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  454 | Train Loss:  0.0016499512130394578 | Validation Loss:  0.0017879002261906862\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  455 | Train Loss:  0.0016277722315862775 | Validation Loss:  0.0017334059812128544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  456 | Train Loss:  0.0016494685551151633 | Validation Loss:  0.0017875320045277476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  457 | Train Loss:  0.001627313788048923 | Validation Loss:  0.0017329796683043242\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  458 | Train Loss:  0.0016489849658682942 | Validation Loss:  0.0017871626187115908\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  459 | Train Loss:  0.0016268546460196376 | Validation Loss:  0.001732553238980472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  460 | Train Loss:  0.0016485013766214252 | Validation Loss:  0.0017867936985567212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  461 | Train Loss:  0.0016263957368209958 | Validation Loss:  0.0017321272753179073\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  462 | Train Loss:  0.0016480173217132688 | Validation Loss:  0.001786424545571208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  463 | Train Loss:  0.001625936827622354 | Validation Loss:  0.0017317017773166299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  464 | Train Loss:  0.001647534198127687 | Validation Loss:  0.0017860564403235912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  465 | Train Loss:  0.0016254791989922523 | Validation Loss:  0.0017312780255451798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  466 | Train Loss:  0.0016470520058646798 | Validation Loss:  0.0017856894992291927\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  467 | Train Loss:  0.0016250225016847253 | Validation Loss:  0.0017308556707575917\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  468 | Train Loss:  0.0016465714434161782 | Validation Loss:  0.0017853239551186562\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  469 | Train Loss:  0.0016245672013610601 | Validation Loss:  0.0017304348293691874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  470 | Train Loss:  0.0016460922779515386 | Validation Loss:  0.0017849593423306942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  471 | Train Loss:  0.0016241129487752914 | Validation Loss:  0.0017300157342106104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  472 | Train Loss:  0.0016456149751320481 | Validation Loss:  0.001784596941433847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  473 | Train Loss:  0.0016236609080806375 | Validation Loss:  0.00172959896735847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  474 | Train Loss:  0.001645140117034316 | Validation Loss:  0.001784236403182149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  475 | Train Loss:  0.001623210497200489 | Validation Loss:  0.0017291841795668006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  476 | Train Loss:  0.0016446671215817332 | Validation Loss:  0.0017838777275756001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  477 | Train Loss:  0.0016227620653808117 | Validation Loss:  0.0017287712544202805\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  478 | Train Loss:  0.0016441959887742996 | Validation Loss:  0.0017835204489529133\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  479 | Train Loss:  0.0016223150305449963 | Validation Loss:  0.001728359959088266\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  480 | Train Loss:  0.0016437264857813716 | Validation Loss:  0.0017831643344834447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  481 | Train Loss:  0.001621869159862399 | Validation Loss:  0.0017279505264014006\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  482 | Train Loss:  0.0016432588454335928 | Validation Loss:  0.0017828099662438035\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  483 | Train Loss:  0.0016214250354096293 | Validation Loss:  0.0017275422578677535\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  484 | Train Loss:  0.001642792485654354 | Validation Loss:  0.0017824559472501278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  485 | Train Loss:  0.001620981376618147 | Validation Loss:  0.0017271348042413592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  486 | Train Loss:  0.0016423272900283337 | Validation Loss:  0.0017821030924096704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  487 | Train Loss:  0.0016205388819798827 | Validation Loss:  0.0017267281655222178\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  488 | Train Loss:  0.0016418623272329569 | Validation Loss:  0.0017817500047385693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  489 | Train Loss:  0.0016200962709262967 | Validation Loss:  0.0017263221088796854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  490 | Train Loss:  0.0016413972480222583 | Validation Loss:  0.0017813972663134336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  491 | Train Loss:  0.0016196538927033544 | Validation Loss:  0.0017259158194065094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  492 | Train Loss:  0.001640932634472847 | Validation Loss:  0.001781043829396367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  493 | Train Loss:  0.0016192112816497684 | Validation Loss:  0.0017255094135180116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  494 | Train Loss:  0.0016404667403548956 | Validation Loss:  0.0017806898104026914\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  495 | Train Loss:  0.0016187680885195732 | Validation Loss:  0.0017251026583835483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  496 | Train Loss:  0.0016400007298216224 | Validation Loss:  0.001780335558578372\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  497 | Train Loss:  0.0016183251282200217 | Validation Loss:  0.0017246962524950504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  498 | Train Loss:  0.0016395350685343146 | Validation Loss:  0.0017799811903387308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  499 | Train Loss:  0.0016178819350898266 | Validation Loss:  0.001724290195852518\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  500 | Train Loss:  0.0016390688251703978 | Validation Loss:  0.0017796270549297333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  501 | Train Loss:  0.001617438974790275 | Validation Loss:  0.0017238841392099857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  502 | Train Loss:  0.0016386029310524464 | Validation Loss:  0.001779272686690092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  503 | Train Loss:  0.0016169960144907236 | Validation Loss:  0.0017234787810593843\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  504 | Train Loss:  0.0016381371533498168 | Validation Loss:  0.0017789187841117382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  505 | Train Loss:  0.0016165536362677813 | Validation Loss:  0.0017230731900781393\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  506 | Train Loss:  0.0016376713756471872 | Validation Loss:  0.001778564415872097\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  507 | Train Loss:  0.0016161107923835516 | Validation Loss:  0.0017226685304194689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  508 | Train Loss:  0.0016372061800211668 | Validation Loss:  0.001778211328200996\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  509 | Train Loss:  0.0016156693454831839 | Validation Loss:  0.0017222650349140167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  510 | Train Loss:  0.0016367423813790083 | Validation Loss:  0.0017778591718524694\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  511 | Train Loss:  0.0016152288299053907 | Validation Loss:  0.0017218629363924265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  512 | Train Loss:  0.001636279746890068 | Validation Loss:  0.0017775079468265176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  513 | Train Loss:  0.0016147894784808159 | Validation Loss:  0.001721461652778089\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  514 | Train Loss:  0.0016358179273083806 | Validation Loss:  0.0017771576531231403\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  515 | Train Loss:  0.0016143511747941375 | Validation Loss:  0.0017210617661476135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  516 | Train Loss:  0.0016353580867871642 | Validation Loss:  0.001776808756403625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  517 | Train Loss:  0.001613914268091321 | Validation Loss:  0.0017206629272550344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  518 | Train Loss:  0.0016348989447578788 | Validation Loss:  0.0017764604417607188\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  519 | Train Loss:  0.0016134779434651136 | Validation Loss:  0.0017202652525156736\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  520 | Train Loss:  0.0016344411997124553 | Validation Loss:  0.001776113291271031\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  521 | Train Loss:  0.0016130426665768027 | Validation Loss:  0.0017198685090988874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  522 | Train Loss:  0.0016339843859896064 | Validation Loss:  0.001775766839273274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  523 | Train Loss:  0.001612608670257032 | Validation Loss:  0.0017194730462506413\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  524 | Train Loss:  0.0016335290856659412 | Validation Loss:  0.0017754215514287353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  525 | Train Loss:  0.001612175488844514 | Validation Loss:  0.0017190785147249699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  526 | Train Loss:  0.0016330743674188852 | Validation Loss:  0.0017750764964148402\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  527 | Train Loss:  0.001611742889508605 | Validation Loss:  0.0017186846816912293\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  528 | Train Loss:  0.0016326206969097257 | Validation Loss:  0.0017747319070622325\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  529 | Train Loss:  0.0016113105230033398 | Validation Loss:  0.0017182906158268452\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  530 | Train Loss:  0.0016321667935699224 | Validation Loss:  0.0017743875505402684\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  531 | Train Loss:  0.0016108786221593618 | Validation Loss:  0.0017178973648697138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  532 | Train Loss:  0.0016317133558914065 | Validation Loss:  0.0017740433104336262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  533 | Train Loss:  0.0016104471869766712 | Validation Loss:  0.0017175045795738697\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  534 | Train Loss:  0.001631260383874178 | Validation Loss:  0.0017736991867423058\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  535 | Train Loss:  0.0016100159846246243 | Validation Loss:  0.0017171119106933475\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  536 | Train Loss:  0.001630807644687593 | Validation Loss:  0.0017733549466356635\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  537 | Train Loss:  0.001609585015103221 | Validation Loss:  0.0017167195910587907\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  538 | Train Loss:  0.0016303551383316517 | Validation Loss:  0.0017730110557749867\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  539 | Train Loss:  0.0016091540455818176 | Validation Loss:  0.0017163270385935903\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  540 | Train Loss:  0.0016299025155603886 | Validation Loss:  0.0017726670484989882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  541 | Train Loss:  0.001608723308891058 | Validation Loss:  0.0017159349517896771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  542 | Train Loss:  0.0016294498927891254 | Validation Loss:  0.001772322691977024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  543 | Train Loss:  0.0016082922229543328 | Validation Loss:  0.0017155427485704422\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  544 | Train Loss:  0.0016289976192638278 | Validation Loss:  0.0017719788011163473\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  545 | Train Loss:  0.001607862301170826 | Validation Loss:  0.001715150778181851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  546 | Train Loss:  0.0016285452293232083 | Validation Loss:  0.0017716343281790614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  547 | Train Loss:  0.0016074315644800663 | Validation Loss:  0.0017147595062851906\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  548 | Train Loss:  0.0016280936542898417 | Validation Loss:  0.0017712910193949938\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  549 | Train Loss:  0.001607001991942525 | Validation Loss:  0.0017143687000498176\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  550 | Train Loss:  0.001627642777748406 | Validation Loss:  0.0017709481762722135\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  551 | Train Loss:  0.0016065731178969145 | Validation Loss:  0.0017139781266450882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  552 | Train Loss:  0.0016271921340376139 | Validation Loss:  0.0017706051003187895\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  553 | Train Loss:  0.0016061443602666259 | Validation Loss:  0.001713588717393577\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  554 | Train Loss:  0.0016267423052340746 | Validation Loss:  0.0017702626064419746\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  555 | Train Loss:  0.0016057161847129464 | Validation Loss:  0.0017131990753114223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  556 | Train Loss:  0.0016262925928458571 | Validation Loss:  0.0017699204618111253\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  557 | Train Loss:  0.0016052883584052324 | Validation Loss:  0.0017128102481365204\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  558 | Train Loss:  0.0016258438117802143 | Validation Loss:  0.001769578899256885\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  559 | Train Loss:  0.0016048615798354149 | Validation Loss:  0.0017124225851148367\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  560 | Train Loss:  0.0016253964276984334 | Validation Loss:  0.0017692382680252194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  561 | Train Loss:  0.0016044356161728501 | Validation Loss:  0.0017120350385084748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  562 | Train Loss:  0.0016249496256932616 | Validation Loss:  0.0017688978696241975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  563 | Train Loss:  0.0016040100017562509 | Validation Loss:  0.0017116485396400094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  564 | Train Loss:  0.001624503405764699 | Validation Loss:  0.0017685584025457501\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  565 | Train Loss:  0.0016035856679081917 | Validation Loss:  0.0017112623900175095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  566 | Train Loss:  0.0016240580007433891 | Validation Loss:  0.0017682190518826246\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  567 | Train Loss:  0.0016031613340601325 | Validation Loss:  0.001710876589640975\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  568 | Train Loss:  0.0016236129449680448 | Validation Loss:  0.0017678799340501428\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  569 | Train Loss:  0.0016027373494580388 | Validation Loss:  0.001710491138510406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  570 | Train Loss:  0.0016231687040999532 | Validation Loss:  0.0017675411654636264\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  571 | Train Loss:  0.0016023142961785197 | Validation Loss:  0.001710106385871768\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  572 | Train Loss:  0.0016227250453084707 | Validation Loss:  0.0017672025132924318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  573 | Train Loss:  0.0016018912428990006 | Validation Loss:  0.0017097212839871645\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  574 | Train Loss:  0.001622280920855701 | Validation Loss:  0.001766863977536559\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  575 | Train Loss:  0.0016014681896194816 | Validation Loss:  0.0017093365313485265\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  576 | Train Loss:  0.001621837611310184 | Validation Loss:  0.0017665253253653646\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  577 | Train Loss:  0.001601045485585928 | Validation Loss:  0.0017089518951252103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  578 | Train Loss:  0.0016213941853493452 | Validation Loss:  0.0017661867896094918\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  579 | Train Loss:  0.0016006232472136617 | Validation Loss:  0.0017085678409785032\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  580 | Train Loss:  0.0016209515742957592 | Validation Loss:  0.0017658487195149064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  581 | Train Loss:  0.0016002013580873609 | Validation Loss:  0.0017081837868317962\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  582 | Train Loss:  0.001620509079657495 | Validation Loss:  0.0017655098345130682\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  583 | Train Loss:  0.0015997793525457382 | Validation Loss:  0.0017077992670238018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  584 | Train Loss:  0.0016200662357732654 | Validation Loss:  0.0017651714151725173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  585 | Train Loss:  0.0015993574634194374 | Validation Loss:  0.0017074152128770947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  586 | Train Loss:  0.0016196239739656448 | Validation Loss:  0.0017648331122472882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  587 | Train Loss:  0.001598936039954424 | Validation Loss:  0.0017070310423150659\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  588 | Train Loss:  0.0016191817121580243 | Validation Loss:  0.0017644943436607718\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  589 | Train Loss:  0.001598514267243445 | Validation Loss:  0.0017066471045836806\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  590 | Train Loss:  0.0016187395667657256 | Validation Loss:  0.0017641555750742555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  591 | Train Loss:  0.0015980929601937532 | Validation Loss:  0.00170626281760633\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  592 | Train Loss:  0.0016182977706193924 | Validation Loss:  0.0017638166900724173\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  593 | Train Loss:  0.0015976717695593834 | Validation Loss:  0.0017058784142136574\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  594 | Train Loss:  0.0016178557416424155 | Validation Loss:  0.001763477805070579\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  595 | Train Loss:  0.0015972504625096917 | Validation Loss:  0.0017054947093129158\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  596 | Train Loss:  0.0016174144111573696 | Validation Loss:  0.0017631393857300282\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  597 | Train Loss:  0.0015968300867825747 | Validation Loss:  0.0017051108879968524\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  598 | Train Loss:  0.0016169733135029674 | Validation Loss:  0.0017628006171435118\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  599 | Train Loss:  0.0015964097110554576 | Validation Loss:  0.0017047271830961108\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  600 | Train Loss:  0.0016165324486792088 | Validation Loss:  0.0017624616157263517\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  601 | Train Loss:  0.0015959891024976969 | Validation Loss:  0.0017043432453647256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  602 | Train Loss:  0.0016160914674401283 | Validation Loss:  0.0017621231963858008\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  603 | Train Loss:  0.001595569308847189 | Validation Loss:  0.0017039603553712368\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  604 | Train Loss:  0.0016156519996002316 | Validation Loss:  0.001761785359121859\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  605 | Train Loss:  0.0015951500972732902 | Validation Loss:  0.0017035769997164607\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  606 | Train Loss:  0.0016152122989296913 | Validation Loss:  0.00176144705619663\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  607 | Train Loss:  0.001594731118530035 | Validation Loss:  0.0017031943425536156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  608 | Train Loss:  0.0016147735295817256 | Validation Loss:  0.0017611094517633319\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  609 | Train Loss:  0.0015943123726174235 | Validation Loss:  0.0017028112197294831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  610 | Train Loss:  0.0016143345274031162 | Validation Loss:  0.001760771032422781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  611 | Train Loss:  0.0015938938595354557 | Validation Loss:  0.001702428562566638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  612 | Train Loss:  0.0016138958744704723 | Validation Loss:  0.00176043261308223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  613 | Train Loss:  0.0015934753464534879 | Validation Loss:  0.001702045788988471\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  614 | Train Loss:  0.001613457570783794 | Validation Loss:  0.0017600948922336102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  615 | Train Loss:  0.001593057531863451 | Validation Loss:  0.0017016633646562696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  616 | Train Loss:  0.0016130199655890465 | Validation Loss:  0.0017597571713849902\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  617 | Train Loss:  0.0015926400665193796 | Validation Loss:  0.001701280940324068\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  618 | Train Loss:  0.0016125827096402645 | Validation Loss:  0.001759419566951692\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  619 | Train Loss:  0.0015922229504212737 | Validation Loss:  0.0017008983995765448\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  620 | Train Loss:  0.001612145802937448 | Validation Loss:  0.0017590818461030722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  621 | Train Loss:  0.0015918058343231678 | Validation Loss:  0.001700516091659665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  622 | Train Loss:  0.0016117088962346315 | Validation Loss:  0.0017587434267625213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  623 | Train Loss:  0.00159138860180974 | Validation Loss:  0.0017001332016661763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  624 | Train Loss:  0.0016112715238705277 | Validation Loss:  0.0017584050074219704\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  625 | Train Loss:  0.001590971602126956 | Validation Loss:  0.001699749962426722\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  626 | Train Loss:  0.0016108345007523894 | Validation Loss:  0.0017580660060048103\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  627 | Train Loss:  0.0015905540203675628 | Validation Loss:  0.001699366606771946\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  628 | Train Loss:  0.0016103974776342511 | Validation Loss:  0.0017577270045876503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  629 | Train Loss:  0.0015901371371001005 | Validation Loss:  0.001698983833193779\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  630 | Train Loss:  0.001609961036592722 | Validation Loss:  0.001757388119585812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  631 | Train Loss:  0.0015897207194939256 | Validation Loss:  0.0016986002447083592\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  632 | Train Loss:  0.001609524479135871 | Validation Loss:  0.0017570487689226866\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  633 | Train Loss:  0.0015893037198111415 | Validation Loss:  0.0016982164233922958\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  634 | Train Loss:  0.0016090876888483763 | Validation Loss:  0.0017567089525982738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  635 | Train Loss:  0.0015888870693743229 | Validation Loss:  0.0016978324856609106\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  636 | Train Loss:  0.0016086507821455598 | Validation Loss:  0.0017563692526891828\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  637 | Train Loss:  0.0015884703025221825 | Validation Loss:  0.0016974485479295254\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  638 | Train Loss:  0.0016082145739346743 | Validation Loss:  0.001756028737872839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  639 | Train Loss:  0.0015880536520853639 | Validation Loss:  0.0016970639117062092\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  640 | Train Loss:  0.0016077780164778233 | Validation Loss:  0.0017556879902258515\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  641 | Train Loss:  0.0015876367688179016 | Validation Loss:  0.0016966793918982148\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  642 | Train Loss:  0.001607341575436294 | Validation Loss:  0.0017553474754095078\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  643 | Train Loss:  0.001587220118381083 | Validation Loss:  0.0016962947556748986\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  644 | Train Loss:  0.0016069054836407304 | Validation Loss:  0.0017550067277625203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  645 | Train Loss:  0.0015868040500208735 | Validation Loss:  0.0016959100030362606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  646 | Train Loss:  0.001606469857506454 | Validation Loss:  0.0017546659801155329\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  647 | Train Loss:  0.001586388098075986 | Validation Loss:  0.0016955253668129444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  648 | Train Loss:  0.0016060342313721776 | Validation Loss:  0.0017543249996379018\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  649 | Train Loss:  0.00158597226254642 | Validation Loss:  0.001695140264928341\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  650 | Train Loss:  0.0016055986052379012 | Validation Loss:  0.001753983087837696\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  651 | Train Loss:  0.0015855560777708888 | Validation Loss:  0.0016947544645518064\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  652 | Train Loss:  0.001605162862688303 | Validation Loss:  0.0017536415252834558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  653 | Train Loss:  0.001585140242241323 | Validation Loss:  0.0016943690134212375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  654 | Train Loss:  0.0016047279350459576 | Validation Loss:  0.0017532992642372847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  655 | Train Loss:  0.001584724523127079 | Validation Loss:  0.001693983213044703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  656 | Train Loss:  0.0016042925417423248 | Validation Loss:  0.0017529571196064353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  657 | Train Loss:  0.0015843092696741223 | Validation Loss:  0.0016935972962528467\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  658 | Train Loss:  0.0016038577305153012 | Validation Loss:  0.0017526146257296205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  659 | Train Loss:  0.001583893783390522 | Validation Loss:  0.001693211612291634\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  660 | Train Loss:  0.0016034235013648868 | Validation Loss:  0.001752272481098771\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  661 | Train Loss:  0.0015834791120141745 | Validation Loss:  0.0016928253462538123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  662 | Train Loss:  0.0016029893886297941 | Validation Loss:  0.0017519299872219563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  663 | Train Loss:  0.0015830642078071833 | Validation Loss:  0.0016924391966313124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  664 | Train Loss:  0.0016025552758947015 | Validation Loss:  0.0017515867948532104\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  665 | Train Loss:  0.0015826496528461576 | Validation Loss:  0.001692052697762847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  666 | Train Loss:  0.0016021212795749307 | Validation Loss:  0.0017512432532384992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  667 | Train Loss:  0.0015822346322238445 | Validation Loss:  0.0016916653839871287\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  668 | Train Loss:  0.0016016868175938725 | Validation Loss:  0.001750899013131857\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  669 | Train Loss:  0.0015818196116015315 | Validation Loss:  0.0016912780702114105\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  670 | Train Loss:  0.0016012528212741017 | Validation Loss:  0.0017505544237792492\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  671 | Train Loss:  0.001581404940225184 | Validation Loss:  0.0016908905236050487\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  672 | Train Loss:  0.0016008192906156182 | Validation Loss:  0.0017502105329185724\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  673 | Train Loss:  0.0015809909673407674 | Validation Loss:  0.001690502860583365\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  674 | Train Loss:  0.0016003858763724566 | Validation Loss:  0.0017498655943199992\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  675 | Train Loss:  0.0015805764123797417 | Validation Loss:  0.001690114731900394\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  676 | Train Loss:  0.0015999524621292949 | Validation Loss:  0.0017495201900601387\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  677 | Train Loss:  0.0015801620902493596 | Validation Loss:  0.0016897260211408138\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  678 | Train Loss:  0.0015995185822248459 | Validation Loss:  0.0017491737380623817\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  679 | Train Loss:  0.0015797471860423684 | Validation Loss:  0.0016893368447199464\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  680 | Train Loss:  0.001599084585905075 | Validation Loss:  0.0017488274024799466\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  681 | Train Loss:  0.0015793327474966645 | Validation Loss:  0.0016889474354684353\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  682 | Train Loss:  0.0015986512880772352 | Validation Loss:  0.0017484808340668678\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  683 | Train Loss:  0.0015789184253662825 | Validation Loss:  0.0016885576769709587\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  684 | Train Loss:  0.0015982177574187517 | Validation Loss:  0.0017481333343312144\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  685 | Train Loss:  0.0015785039868205786 | Validation Loss:  0.0016881674528121948\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  686 | Train Loss:  0.0015977842267602682 | Validation Loss:  0.001747785252518952\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  687 | Train Loss:  0.001578089315444231 | Validation Loss:  0.0016877766465768218\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  688 | Train Loss:  0.001597350463271141 | Validation Loss:  0.0017474370542913675\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  689 | Train Loss:  0.001577674993313849 | Validation Loss:  0.0016873854910954833\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  690 | Train Loss:  0.0015969171654433012 | Validation Loss:  0.0017470880411565304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  691 | Train Loss:  0.0015772607875987887 | Validation Loss:  0.001686994219198823\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  692 | Train Loss:  0.0015964835183694959 | Validation Loss:  0.0017467387951910496\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  693 | Train Loss:  0.001576846232637763 | Validation Loss:  0.001686602714471519\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  694 | Train Loss:  0.0015960505697876215 | Validation Loss:  0.0017463893163949251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  695 | Train Loss:  0.0015764323761686683 | Validation Loss:  0.0016862110933288932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  696 | Train Loss:  0.0015956178540363908 | Validation Loss:  0.0017460399540141225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  697 | Train Loss:  0.0015760189853608608 | Validation Loss:  0.0016858192393556237\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  698 | Train Loss:  0.0015951857203617692 | Validation Loss:  0.0017456902423873544\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  699 | Train Loss:  0.0015756055945530534 | Validation Loss:  0.001685427618212998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  700 | Train Loss:  0.001594753935933113 | Validation Loss:  0.001745340065099299\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  701 | Train Loss:  0.0015751926694065332 | Validation Loss:  0.0016850352985784411\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  702 | Train Loss:  0.0015943220350891352 | Validation Loss:  0.0017449893057346344\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  703 | Train Loss:  0.0015747793950140476 | Validation Loss:  0.0016846428625285625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  704 | Train Loss:  0.001593890367075801 | Validation Loss:  0.0017446383135393262\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  705 | Train Loss:  0.0015743667026981711 | Validation Loss:  0.0016842499608173966\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  706 | Train Loss:  0.0015934589318931103 | Validation Loss:  0.0017442868556827307\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  707 | Train Loss:  0.0015739540103822947 | Validation Loss:  0.0016838565934449434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  708 | Train Loss:  0.0015930276131257415 | Validation Loss:  0.0017439346993342042\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  709 | Train Loss:  0.0015735412016510963 | Validation Loss:  0.001683462760411203\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  710 | Train Loss:  0.001592596061527729 | Validation Loss:  0.0017435820773243904\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  711 | Train Loss:  0.001573128392919898 | Validation Loss:  0.0016830689273774624\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  712 | Train Loss:  0.001592164975591004 | Validation Loss:  0.0017432289896532893\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  713 | Train Loss:  0.001572716049849987 | Validation Loss:  0.0016826745122671127\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  714 | Train Loss:  0.0015917338896542788 | Validation Loss:  0.0017428752034902573\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  715 | Train Loss:  0.0015723035903647542 | Validation Loss:  0.0016822797479107976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  716 | Train Loss:  0.0015913030365481973 | Validation Loss:  0.0017425216501578689\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  717 | Train Loss:  0.0015718915965408087 | Validation Loss:  0.0016818850999698043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  718 | Train Loss:  0.001590872649103403 | Validation Loss:  0.0017421675147488713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  719 | Train Loss:  0.0015714794863015413 | Validation Loss:  0.0016814894042909145\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  720 | Train Loss:  0.0015904423780739307 | Validation Loss:  0.001741812564432621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  721 | Train Loss:  0.0015710677253082395 | Validation Loss:  0.0016810944071039557\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  722 | Train Loss:  0.001590012339875102 | Validation Loss:  0.001741457381285727\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  723 | Train Loss:  0.001570656313560903 | Validation Loss:  0.0016806984785944223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  724 | Train Loss:  0.0015895824180915952 | Validation Loss:  0.0017411019653081894\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  725 | Train Loss:  0.0015702447853982449 | Validation Loss:  0.0016803019680082798\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  726 | Train Loss:  0.0015891526127234101 | Validation Loss:  0.001740745734423399\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  727 | Train Loss:  0.001569833722896874 | Validation Loss:  0.001679905690252781\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  728 | Train Loss:  0.001588722923770547 | Validation Loss:  0.0017403890378773212\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  729 | Train Loss:  0.0015694223111495376 | Validation Loss:  0.0016795090632513165\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  730 | Train Loss:  0.0015882938168942928 | Validation Loss:  0.0017400322249159217\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  731 | Train Loss:  0.0015690118307247758 | Validation Loss:  0.0016791123198345304\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  732 | Train Loss:  0.0015878650592640042 | Validation Loss:  0.0017396747134625912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  733 | Train Loss:  0.0015686014667153358 | Validation Loss:  0.0016787153435871005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  734 | Train Loss:  0.001587436767295003 | Validation Loss:  0.0017393174348399043\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  735 | Train Loss:  0.0015681913355365396 | Validation Loss:  0.0016783176688477397\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  736 | Train Loss:  0.0015870083589106798 | Validation Loss:  0.001738959108479321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  737 | Train Loss:  0.0015677812043577433 | Validation Loss:  0.0016779203433543444\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  738 | Train Loss:  0.0015865805326029658 | Validation Loss:  0.0017386008985340595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  739 | Train Loss:  0.001567371655255556 | Validation Loss:  0.00167752243578434\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  740 | Train Loss:  0.0015861529391258955 | Validation Loss:  0.0017382416408509016\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  741 | Train Loss:  0.001566962106153369 | Validation Loss:  0.0016771245282143354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  742 | Train Loss:  0.0015857256948947906 | Validation Loss:  0.0017378829652443528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  743 | Train Loss:  0.0015665534883737564 | Validation Loss:  0.001676726620644331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  744 | Train Loss:  0.0015852992655709386 | Validation Loss:  0.0017375234747305512\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  745 | Train Loss:  0.0015661448705941439 | Validation Loss:  0.0016763285966590047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  746 | Train Loss:  0.0015848729526624084 | Validation Loss:  0.0017371639842167497\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  747 | Train Loss:  0.0015657370677217841 | Validation Loss:  0.0016759298741817474\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  748 | Train Loss:  0.0015844469889998436 | Validation Loss:  0.0017368028638884425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  749 | Train Loss:  0.0015653283335268497 | Validation Loss:  0.0016755313845351338\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  750 | Train Loss:  0.0015840207925066352 | Validation Loss:  0.00173644267488271\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  751 | Train Loss:  0.0015649209963157773 | Validation Loss:  0.0016751307994127274\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  752 | Train Loss:  0.0015835945960134268 | Validation Loss:  0.0017360791098326445\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  753 | Train Loss:  0.0015645116800442338 | Validation Loss:  0.0016747333575040102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  754 | Train Loss:  0.0015831694472581148 | Validation Loss:  0.001735721598379314\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  755 | Train Loss:  0.0015641074860468507 | Validation Loss:  0.0016743313753977418\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  756 | Train Loss:  0.0015827449969947338 | Validation Loss:  0.0017353527946397662\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  757 | Train Loss:  0.001563695608638227 | Validation Loss:  0.0016739368438720703\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  758 | Train Loss:  0.0015823205467313528 | Validation Loss:  0.0017350020352751017\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  759 | Train Loss:  0.0015632978174835443 | Validation Loss:  0.0016735283425077796\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  760 | Train Loss:  0.0015818970277905464 | Validation Loss:  0.0017346198437735438\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  761 | Train Loss:  0.0015628775581717491 | Validation Loss:  0.001673146733082831\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  762 | Train Loss:  0.001581475604325533 | Validation Loss:  0.0017342949286103249\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  763 | Train Loss:  0.0015625004889443517 | Validation Loss:  0.001672720187343657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  764 | Train Loss:  0.0015810560435056686 | Validation Loss:  0.0017338716425001621\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  765 | Train Loss:  0.0015620524063706398 | Validation Loss:  0.0016723903827369213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  766 | Train Loss:  0.0015806527808308601 | Validation Loss:  0.0017336494056507945\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  767 | Train Loss:  0.0015617561293765903 | Validation Loss:  0.001671930542215705\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  768 | Train Loss:  0.0015802778070792556 | Validation Loss:  0.0017331393901258707\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  769 | Train Loss:  0.0015612548450008035 | Validation Loss:  0.001671832986176014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  770 | Train Loss:  0.0015800094697624445 | Validation Loss:  0.0017333453288301826\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  771 | Train Loss:  0.0015613154973834753 | Validation Loss:  0.0016714136581867933\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  772 | Train Loss:  0.0015798984095454216 | Validation Loss:  0.0017328131943941116\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  773 | Train Loss:  0.0015608317917212844 | Validation Loss:  0.001671584788709879\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  774 | Train Loss:  0.0015797462547197938 | Validation Loss:  0.0017332792049273849\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  775 | Train Loss:  0.0015611108392477036 | Validation Loss:  0.001671027741394937\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  776 | Train Loss:  0.0015795757062733173 | Validation Loss:  0.001732379081659019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  777 | Train Loss:  0.0015602883649989963 | Validation Loss:  0.0016707489266991615\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  778 | Train Loss:  0.0015789506724104285 | Validation Loss:  0.0017322188941761851\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  779 | Train Loss:  0.0015600130427628756 | Validation Loss:  0.001669971621595323\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  780 | Train Loss:  0.0015783276176080108 | Validation Loss:  0.0017312599811702967\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  781 | Train Loss:  0.0015591195551678538 | Validation Loss:  0.0016694068908691406\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  782 | Train Loss:  0.0015775327337905765 | Validation Loss:  0.001730801071971655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  783 | Train Loss:  0.0015585923101752996 | Validation Loss:  0.001668635755777359\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  784 | Train Loss:  0.0015767791774123907 | Validation Loss:  0.0017299468163400888\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  785 | Train Loss:  0.0015577800804749131 | Validation Loss:  0.0016680259723216295\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  786 | Train Loss:  0.0015760203823447227 | Validation Loss:  0.0017294366843998432\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  787 | Train Loss:  0.001557221869006753 | Validation Loss:  0.0016673553036525846\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  788 | Train Loss:  0.0015753296902403235 | Validation Loss:  0.0017287484370172024\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  789 | Train Loss:  0.0015565515495836735 | Validation Loss:  0.0016668251482769847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  790 | Train Loss:  0.0015747087309136987 | Validation Loss:  0.0017283207271248102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  791 | Train Loss:  0.001556079601868987 | Validation Loss:  0.0016663039568811655\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  792 | Train Loss:  0.001574175781570375 | Validation Loss:  0.001727826427668333\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  793 | Train Loss:  0.0015555804129689932 | Validation Loss:  0.0016659065149724483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  794 | Train Loss:  0.0015737366629764438 | Validation Loss:  0.0017275337595492601\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  795 | Train Loss:  0.0015552385011687875 | Validation Loss:  0.0016655393410474062\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  796 | Train Loss:  0.0015733817126601934 | Validation Loss:  0.0017272166442126036\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  797 | Train Loss:  0.0015548981027677655 | Validation Loss:  0.0016652689082548022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  798 | Train Loss:  0.001573108835145831 | Validation Loss:  0.0017270452808588743\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  799 | Train Loss:  0.0015546715585514903 | Validation Loss:  0.0016650178004056215\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  800 | Train Loss:  0.0015728921862319112 | Validation Loss:  0.0017268469091504812\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  801 | Train Loss:  0.0015544377965852618 | Validation Loss:  0.0016648259479552507\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  802 | Train Loss:  0.001572721404954791 | Validation Loss:  0.0017267378279939294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  803 | Train Loss:  0.0015542714390903711 | Validation Loss:  0.0016646240837872028\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  804 | Train Loss:  0.0015725642442703247 | Validation Loss:  0.0017265757778659463\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  805 | Train Loss:  0.0015540712047368288 | Validation Loss:  0.0016644421266391873\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  806 | Train Loss:  0.0015724094118922949 | Validation Loss:  0.0017264545895159245\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  807 | Train Loss:  0.0015538956504315138 | Validation Loss:  0.001664216979406774\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  808 | Train Loss:  0.0015722244279459119 | Validation Loss:  0.001726248417980969\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  809 | Train Loss:  0.0015536563005298376 | Validation Loss:  0.0016639827517792583\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  810 | Train Loss:  0.0015720059163868427 | Validation Loss:  0.0017260536551475525\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  811 | Train Loss:  0.0015534148551523685 | Validation Loss:  0.0016636791406199336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  812 | Train Loss:  0.00157172791659832 | Validation Loss:  0.001725747250020504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  813 | Train Loss:  0.0015530871460214257 | Validation Loss:  0.0016633601626381278\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  814 | Train Loss:  0.0015714027686044574 | Validation Loss:  0.001725458656437695\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  815 | Train Loss:  0.001552760018967092 | Validation Loss:  0.0016629616729915142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  816 | Train Loss:  0.0015710151055827737 | Validation Loss:  0.0017250441014766693\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  817 | Train Loss:  0.0015523390611633658 | Validation Loss:  0.0016625783173367381\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  818 | Train Loss:  0.0015706026460975409 | Validation Loss:  0.0017247067298740149\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  819 | Train Loss:  0.0015519645530730486 | Validation Loss:  0.00166211964096874\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  820 | Train Loss:  0.0015701547963544726 | Validation Loss:  0.0017242331523448229\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  821 | Train Loss:  0.001551497378386557 | Validation Loss:  0.0016617324436083436\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  822 | Train Loss:  0.0015697212656959891 | Validation Loss:  0.001723916851915419\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  823 | Train Loss:  0.0015511374222114682 | Validation Loss:  0.0016612340696156025\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  824 | Train Loss:  0.0015692440792918205 | Validation Loss:  0.0017233719117939472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  825 | Train Loss:  0.0015506123891100287 | Validation Loss:  0.0016607999568805099\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  826 | Train Loss:  0.0015687346458435059 | Validation Loss:  0.0017229682998731732\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  827 | Train Loss:  0.0015501719899475574 | Validation Loss:  0.0016601827228441834\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  828 | Train Loss:  0.0015681307995691895 | Validation Loss:  0.0017222834285348654\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  829 | Train Loss:  0.0015495221596211195 | Validation Loss:  0.001659665023908019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  830 | Train Loss:  0.0015675019240006804 | Validation Loss:  0.0017217978602275252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  831 | Train Loss:  0.0015490083023905754 | Validation Loss:  0.0016589785227552056\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  832 | Train Loss:  0.0015668306732550263 | Validation Loss:  0.0017210791120305657\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  833 | Train Loss:  0.0015483313472941518 | Validation Loss:  0.0016584884142503142\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  834 | Train Loss:  0.0015662148362025619 | Validation Loss:  0.0017206498887389898\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  835 | Train Loss:  0.001547867781482637 | Validation Loss:  0.0016578302020207047\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  836 | Train Loss:  0.001565605285577476 | Validation Loss:  0.0017199877183884382\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  837 | Train Loss:  0.0015472485683858395 | Validation Loss:  0.0016574615146964788\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  838 | Train Loss:  0.0015651063295081258 | Validation Loss:  0.0017197109991684556\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  839 | Train Loss:  0.0015469191130250692 | Validation Loss:  0.0016568736173212528\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  840 | Train Loss:  0.0015646270476281643 | Validation Loss:  0.0017191284568980336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  841 | Train Loss:  0.0015463829040527344 | Validation Loss:  0.0016566774575039744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  842 | Train Loss:  0.001564284902997315 | Validation Loss:  0.001719052903354168\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  843 | Train Loss:  0.001546226441860199 | Validation Loss:  0.0016561413649469614\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  844 | Train Loss:  0.0015639429911971092 | Validation Loss:  0.001718513434752822\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  845 | Train Loss:  0.001545744831673801 | Validation Loss:  0.0016561392694711685\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  846 | Train Loss:  0.0015637478791177273 | Validation Loss:  0.0017186572076752782\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  847 | Train Loss:  0.0015457726549357176 | Validation Loss:  0.001655588042922318\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  848 | Train Loss:  0.0015635066665709019 | Validation Loss:  0.0017180755967274308\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  849 | Train Loss:  0.0015452731167897582 | Validation Loss:  0.0016557883936911821\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  850 | Train Loss:  0.0015634143492206931 | Validation Loss:  0.0017184456810355186\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  851 | Train Loss:  0.0015454862732440233 | Validation Loss:  0.0016551336739212275\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  852 | Train Loss:  0.0015632212162017822 | Validation Loss:  0.0017177184345200658\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  853 | Train Loss:  0.0015448782360181212 | Validation Loss:  0.0016555165639147162\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  854 | Train Loss:  0.0015631598653271794 | Validation Loss:  0.0017182874726131558\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  855 | Train Loss:  0.0015452506486326456 | Validation Loss:  0.0016546993283554912\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  856 | Train Loss:  0.0015629578847438097 | Validation Loss:  0.0017173468368127942\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  857 | Train Loss:  0.0015444670571014285 | Validation Loss:  0.0016551421722397208\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  858 | Train Loss:  0.001562814344651997 | Validation Loss:  0.0017179608112201095\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  859 | Train Loss:  0.0015448692720383406 | Validation Loss:  0.0016541784862056375\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  860 | Train Loss:  0.0015625192318111658 | Validation Loss:  0.0017168287886306643\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  861 | Train Loss:  0.0015439152484759688 | Validation Loss:  0.001654463936574757\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  862 | Train Loss:  0.0015621839556843042 | Validation Loss:  0.0017172495136037469\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  863 | Train Loss:  0.0015441463328897953 | Validation Loss:  0.001653430750593543\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  864 | Train Loss:  0.0015617171302437782 | Validation Loss:  0.0017160400748252869\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  865 | Train Loss:  0.001543113379739225 | Validation Loss:  0.0016534548485651612\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  866 | Train Loss:  0.001561203389428556 | Validation Loss:  0.0017161770956590772\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  867 | Train Loss:  0.001543100574053824 | Validation Loss:  0.001652441220358014\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  868 | Train Loss:  0.0015605883672833443 | Validation Loss:  0.0017150192288681865\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  869 | Train Loss:  0.0015421020798385143 | Validation Loss:  0.0016522689256817102\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  870 | Train Loss:  0.0015600047772750258 | Validation Loss:  0.0017149528721347451\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  871 | Train Loss:  0.0015419208211824298 | Validation Loss:  0.001651345519348979\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  872 | Train Loss:  0.0015593401622027159 | Validation Loss:  0.0017139276023954153\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  873 | Train Loss:  0.0015410305932164192 | Validation Loss:  0.0016510769492015243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  874 | Train Loss:  0.0015587778761982918 | Validation Loss:  0.0017137541435658932\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  875 | Train Loss:  0.0015407676110044122 | Validation Loss:  0.0016502694925293326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  876 | Train Loss:  0.0015581368934363127 | Validation Loss:  0.0017128799809142947\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  877 | Train Loss:  0.0015400033444166183 | Validation Loss:  0.001649965182878077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  878 | Train Loss:  0.0015576310688629746 | Validation Loss:  0.0017126597231253982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  879 | Train Loss:  0.0015397119568660855 | Validation Loss:  0.0016492821741849184\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  880 | Train Loss:  0.001557059120386839 | Validation Loss:  0.001711948774755001\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  881 | Train Loss:  0.0015390857588499784 | Validation Loss:  0.0016489943955093622\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  882 | Train Loss:  0.001556647359393537 | Validation Loss:  0.0017117428360506892\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  883 | Train Loss:  0.0015388199826702476 | Validation Loss:  0.0016484538791701198\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  884 | Train Loss:  0.0015561876352876425 | Validation Loss:  0.001711210235953331\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  885 | Train Loss:  0.0015383473364636302 | Validation Loss:  0.001648220233619213\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  886 | Train Loss:  0.0015559002058580518 | Validation Loss:  0.0017110565677285194\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  887 | Train Loss:  0.0015381391858682036 | Validation Loss:  0.0016478223260492086\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  888 | Train Loss:  0.0015555652789771557 | Validation Loss:  0.0017106876475736499\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  889 | Train Loss:  0.0015378101961687207 | Validation Loss:  0.0016476400196552277\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  890 | Train Loss:  0.0015553925186395645 | Validation Loss:  0.0017105764709413052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  891 | Train Loss:  0.0015376468654721975 | Validation Loss:  0.0016473553841933608\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  892 | Train Loss:  0.0015551471151411533 | Validation Loss:  0.001710326294414699\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  893 | Train Loss:  0.001537422533147037 | Validation Loss:  0.001647182391025126\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  894 | Train Loss:  0.0015550411771982908 | Validation Loss:  0.001710205338895321\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  895 | Train Loss:  0.0015372559428215027 | Validation Loss:  0.0016469694674015045\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  896 | Train Loss:  0.0015548233641311526 | Validation Loss:  0.0017100217519327998\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  897 | Train Loss:  0.0015370901674032211 | Validation Loss:  0.0016467393143102527\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  898 | Train Loss:  0.0015547210350632668 | Validation Loss:  0.0017098188400268555\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  899 | Train Loss:  0.0015368538442999125 | Validation Loss:  0.0016465700464323163\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  900 | Train Loss:  0.0015544674824923277 | Validation Loss:  0.0017096700612455606\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  901 | Train Loss:  0.0015367177547886968 | Validation Loss:  0.0016462052008137107\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  902 | Train Loss:  0.0015543136978521943 | Validation Loss:  0.0017093055648729205\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  903 | Train Loss:  0.0015363415004685521 | Validation Loss:  0.0016460918122902513\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  904 | Train Loss:  0.0015539827290922403 | Validation Loss:  0.0017092111520469189\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  905 | Train Loss:  0.0015362492995336652 | Validation Loss:  0.0016455091536045074\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  906 | Train Loss:  0.001553749549202621 | Validation Loss:  0.001708602299913764\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  907 | Train Loss:  0.0015356632648035884 | Validation Loss:  0.001645531039685011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  908 | Train Loss:  0.0015533429104834795 | Validation Loss:  0.001708661438897252\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  909 | Train Loss:  0.001535696443170309 | Validation Loss:  0.001644655829295516\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  910 | Train Loss:  0.0015530462842434645 | Validation Loss:  0.0017077344236895442\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  911 | Train Loss:  0.0015348461456596851 | Validation Loss:  0.001644934294745326\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  912 | Train Loss:  0.0015526139177381992 | Validation Loss:  0.0017081095138564706\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  913 | Train Loss:  0.001535132178105414 | Validation Loss:  0.0016437789890915155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  914 | Train Loss:  0.0015523121692240238 | Validation Loss:  0.0017068616580218077\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  915 | Train Loss:  0.001534037757664919 | Validation Loss:  0.001644316129386425\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  916 | Train Loss:  0.0015519229928031564 | Validation Loss:  0.001707591349259019\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  917 | Train Loss:  0.001534589217044413 | Validation Loss:  0.0016430695541203022\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  918 | Train Loss:  0.0015516415005549788 | Validation Loss:  0.0017061663093045354\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  919 | Train Loss:  0.0015333896735683084 | Validation Loss:  0.0016436103032901883\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  920 | Train Loss:  0.0015512965619564056 | Validation Loss:  0.0017069440800696611\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  921 | Train Loss:  0.001533936825580895 | Validation Loss:  0.0016424587229266763\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  922 | Train Loss:  0.0015509342774748802 | Validation Loss:  0.0017055525677278638\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  923 | Train Loss:  0.0015327980509027839 | Validation Loss:  0.0016427627997472882\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  924 | Train Loss:  0.00155059271492064 | Validation Loss:  0.0017060707323253155\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  925 | Train Loss:  0.0015330981696024537 | Validation Loss:  0.0016417609294876456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  926 | Train Loss:  0.0015500789741054177 | Validation Loss:  0.0017048321897163987\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  927 | Train Loss:  0.0015320905949920416 | Validation Loss:  0.0016417282167822123\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  928 | Train Loss:  0.0015496481209993362 | Validation Loss:  0.0017049016896635294\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  929 | Train Loss:  0.0015320144593715668 | Validation Loss:  0.001640819013118744\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  930 | Train Loss:  0.001548955449834466 | Validation Loss:  0.001703827758319676\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  931 | Train Loss:  0.0015311157330870628 | Validation Loss:  0.0016404751222580671\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  932 | Train Loss:  0.0015483855968341231 | Validation Loss:  0.0017035260098055005\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  933 | Train Loss:  0.0015307515859603882 | Validation Loss:  0.001639704336412251\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  934 | Train Loss:  0.0015476658008992672 | Validation Loss:  0.0017027049325406551\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  935 | Train Loss:  0.0015300435479730368 | Validation Loss:  0.001639261725358665\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  936 | Train Loss:  0.0015471286606043577 | Validation Loss:  0.0017023002728819847\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  937 | Train Loss:  0.001529620261862874 | Validation Loss:  0.001638705492950976\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  938 | Train Loss:  0.0015465536853298545 | Validation Loss:  0.0017017679056152701\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  939 | Train Loss:  0.0015291505260393023 | Validation Loss:  0.0016383142210543156\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  940 | Train Loss:  0.0015461744042113423 | Validation Loss:  0.0017014247132465243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  941 | Train Loss:  0.0015288038412109017 | Validation Loss:  0.001637989771552384\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  942 | Train Loss:  0.001545812003314495 | Validation Loss:  0.0017011634772643447\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  943 | Train Loss:  0.001528566237539053 | Validation Loss:  0.001637699780985713\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  944 | Train Loss:  0.0015456194523721933 | Validation Loss:  0.0017009269213303924\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  945 | Train Loss:  0.0015283288666978478 | Validation Loss:  0.0016375732375308871\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  946 | Train Loss:  0.0015454519307240844 | Validation Loss:  0.0017008759314194322\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  947 | Train Loss:  0.0015282727545127273 | Validation Loss:  0.0016373590333387256\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  948 | Train Loss:  0.0015453989617526531 | Validation Loss:  0.001700705848634243\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  949 | Train Loss:  0.0015281055821105838 | Validation Loss:  0.0016373633407056332\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  950 | Train Loss:  0.001545351929962635 | Validation Loss:  0.001700777793303132\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  951 | Train Loss:  0.001528153894469142 | Validation Loss:  0.001637154957279563\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  952 | Train Loss:  0.0015453473897650838 | Validation Loss:  0.0017005898989737034\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  953 | Train Loss:  0.0015279799699783325 | Validation Loss:  0.0016372103709727526\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  954 | Train Loss:  0.0015453234082087874 | Validation Loss:  0.0017006946727633476\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  955 | Train Loss:  0.0015280500520020723 | Validation Loss:  0.0016369264340028167\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  956 | Train Loss:  0.0015452663647010922 | Validation Loss:  0.0017003988614305854\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  957 | Train Loss:  0.0015277907950803638 | Validation Loss:  0.0016369677614420652\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  958 | Train Loss:  0.0015451847575604916 | Validation Loss:  0.00170047243591398\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  959 | Train Loss:  0.0015278210630640388 | Validation Loss:  0.00163654878269881\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  960 | Train Loss:  0.0015449980273842812 | Validation Loss:  0.0017000101506710052\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  961 | Train Loss:  0.0015274292090907693 | Validation Loss:  0.0016365419141948223\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  962 | Train Loss:  0.0015448270132765174 | Validation Loss:  0.0017000333173200488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  963 | Train Loss:  0.0015273947501555085 | Validation Loss:  0.0016359809087589383\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  964 | Train Loss:  0.0015444785822182894 | Validation Loss:  0.0016994050238281488\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  965 | Train Loss:  0.00152688124217093 | Validation Loss:  0.0016359194414690137\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  966 | Train Loss:  0.0015442540170624852 | Validation Loss:  0.0016993964090943336\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  967 | Train Loss:  0.0015267846174538136 | Validation Loss:  0.0016352885868400335\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  968 | Train Loss:  0.0015437559923157096 | Validation Loss:  0.0016986833652481437\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  969 | Train Loss:  0.0015262390952557325 | Validation Loss:  0.001635149703361094\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  970 | Train Loss:  0.0015435742679983377 | Validation Loss:  0.0016986381961032748\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  971 | Train Loss:  0.0015260610962286592 | Validation Loss:  0.0016346422489732504\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  972 | Train Loss:  0.0015429771738126874 | Validation Loss:  0.0016980316722765565\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  973 | Train Loss:  0.0015256637707352638 | Validation Loss:  0.0016342763556167483\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  974 | Train Loss:  0.0015429196646437049 | Validation Loss:  0.0016977534396573901\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  975 | Train Loss:  0.0015252373414114118 | Validation Loss:  0.001634204643778503\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  976 | Train Loss:  0.0015422678552567959 | Validation Loss:  0.0016975435428321362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  977 | Train Loss:  0.0015252099838107824 | Validation Loss:  0.0016331903170794249\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  978 | Train Loss:  0.0015421832213178277 | Validation Loss:  0.0016965639078989625\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  979 | Train Loss:  0.0015241849469020963 | Validation Loss:  0.001633761916309595\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  980 | Train Loss:  0.0015414762310683727 | Validation Loss:  0.0016970854485407472\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  981 | Train Loss:  0.0015247378032654524 | Validation Loss:  0.0016321196453645825\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  982 | Train Loss:  0.0015412764623761177 | Validation Loss:  0.0016954179154708982\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  983 | Train Loss:  0.0015232078731060028 | Validation Loss:  0.0016328635392710567\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  984 | Train Loss:  0.0015405695885419846 | Validation Loss:  0.0016962422523647547\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  985 | Train Loss:  0.0015239035710692406 | Validation Loss:  0.00163134909234941\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  986 | Train Loss:  0.00154022010974586 | Validation Loss:  0.0016945382812991738\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  987 | Train Loss:  0.0015224394155666232 | Validation Loss:  0.0016316595720127225\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  988 | Train Loss:  0.0015395944938063622 | Validation Loss:  0.001694993581622839\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  989 | Train Loss:  0.0015227339463308454 | Validation Loss:  0.0016305670142173767\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  990 | Train Loss:  0.0015390703920274973 | Validation Loss:  0.001693726982921362\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  991 | Train Loss:  0.001521700993180275 | Validation Loss:  0.0016305532772094011\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  992 | Train Loss:  0.0015386657323688269 | Validation Loss:  0.0016938725020736456\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  993 | Train Loss:  0.001521704951301217 | Validation Loss:  0.0016298737609758973\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  994 | Train Loss:  0.001538146287202835 | Validation Loss:  0.0016930815763771534\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  995 | Train Loss:  0.0015210978453978896 | Validation Loss:  0.0016297074034810066\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  996 | Train Loss:  0.0015379462856799364 | Validation Loss:  0.0016930445563048124\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  997 | Train Loss:  0.0015209570992738008 | Validation Loss:  0.0016293554799631238\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  998 | Train Loss:  0.0015375352231785655 | Validation Loss:  0.0016926530515775084\n",
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Epoch:  999 | Train Loss:  0.0015206863172352314 | Validation Loss:  0.0016291297506541014\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:  {'n_epochs': 1000, 'learning_rate': 0.001, 'optimizer': 'RMSprop'} Test Loss:  0.001331959618255496\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = \"model_weights_best_lstm.pth\"\n",
    "\n",
    "setups = [\n",
    "    {\"n_epochs\": 300, \"learning_rate\": 0.01, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.01, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.1, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"Adam\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"SGD\"},\n",
    "    {\"n_epochs\": 1000, \"learning_rate\": 0.001, \"optimizer\": \"RMSprop\"}\n",
    "]\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 3\n",
    "\n",
    "for setup in setups:\n",
    "    n_epochs = setup[\"n_epochs\"]\n",
    "    learning_rate = setup[\"learning_rate\"]\n",
    "    optimizer_name = setup[\"optimizer\"]\n",
    "\n",
    "    lstm = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(lstm.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer name: {optimizer_name}\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        lstm.train()\n",
    "        hidden_state = None\n",
    "        output = lstm(X_train)\n",
    "        loss = criterion(output.view(-1), y_train)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lstm.eval()\n",
    "        val_hidden_state = None\n",
    "        test_output = lstm(X_val)\n",
    "        val_loss = criterion(test_output.view(-1), y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "        print('Setup: ', setup, 'Epoch: ', epoch, '| Train Loss: ', loss.item(), '| Validation Loss: ', val_loss.item())\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(lstm.state_dict(), best_model_path)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"Loss Curves for setup: {setup}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    lstm.eval()\n",
    "    test_hidden_state = None\n",
    "    pred_test = lstm(X_test)\n",
    "    test_loss = criterion(pred_test.view(-1), y_test)\n",
    "    print('Setup: ', setup, 'Test Loss: ', test_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d16621",
   "metadata": {},
   "source": [
    "# ##REFERENCES\n",
    "\n",
    "- https://pytorch.org/\n",
    "- https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3506970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
